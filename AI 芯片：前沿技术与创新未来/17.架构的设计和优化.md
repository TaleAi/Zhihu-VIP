## 17.架构的设计和优化
架构级设计包括计算引擎设计和存储系统设计。设计专用 DNN 硬件的挑战包括设计一个灵活的体系结构，然后找到配置该体系结构的最佳方法，以便为不同的 DNN 层获得最佳的硬件性能和能效。找到最佳配置的机会取决于硬件的灵活性，但较高的灵活性通常意味着需要增加额外的硬件而降低效率。 


因此，通常需要经过反复的精炼过程才能得到最佳的架构设计。这是一个找到最佳映射的过程，包含确认 MAC 操作在时间（即同一 PE 上的串行顺序）和空间上（即跨许多并行 PE 的顺序）的执行顺序；如何在不同级别的存储层次结构上分区（Tiling）和移动数据，以按照该执行顺序进行计算。对于给定的 DNN 层，通常存在大量可能的映射方案。因此，找到所需指标的最佳映射方案非常关键。 


#### 把数据流用图表示的架构设计


一些新的架构把数据流用图（Graph）来表示，如 Graphcore 的智能处理单元（Intelligent Processing Unit，IPU）就是非常创新的，这种架构优化了网络内部的运行成本，已用于云端数据中心的服务器中。几年前刚创立的 Wave Computing 的数据流处理芯片也是采用数据流图进行高速处理，但 2020 年 4 月，这家在 AI 芯片领域备受关注的独角兽公司却走上了破产倒闭之路。下面以 Graphcore 的 IPU 为例，介绍其设计架构。 


Graphcore 是一家英国 AI 初创公司。这家公司认为，CPU 的专长是标量，GPU 的专长是矢量，而用于智能计算的图架构是 AI 专用芯片的最佳选择。该公司专门开发了一个处理器来处理这种图架构，并将其称为智能处理单元（IPU）。 


IPU 芯片有一些独特的功能。例如：这家公司认为训练和推理无须分开，而应该同时支持（见图 3.12）；采用同构多核架构，超过 1000 个独立的处理器；支持 all-to-all 的核间无拥塞（Non-Blocking）通信，采用 20 世纪 80 年代提出的整体同步并行计算（Bulk Synchronous Parallel，BSP）模型，该模型是用在并行硬件和软件之间的一种桥接模型，成为冯·诺依曼串行模型的一种替代。BSP 把操作分为 3 个时间段来进行：计算、同步屏蔽及通信阶段（见图 3.13 和图 3.14）。 


![img](https://pic1.zhimg.com/v2-fccd9b51ea2595ebfec012f44c30a4df.webp)

![img](https://pic4.zhimg.com/v2-9c3d58d1579a8cee7737aa234c2fe442.webp)

![img](https://pic2.zhimg.com/v2-4eab5838f23fe78380ff0357c1d19357.webp)

BSP 放弃了程序局部性原理，从而简化了程序设计与实现。这一点有利于并行计算，因为大规模计算往往需要大量处理器，但实际上很难提供那么多处理器，于是一个处理器可能会被映射到多个虚拟进程。这种情况下，处理器对存储器的访问会受到附带的程序局部性原理的约束。为了防止拥塞，选路器使用对等（Peer-to-Peer，P2P）网络的方式进行通信。 


另外，IPU 芯片上大量的面积被用于 SRAM，避免直接连接 DRAM，其目标是所有的模型都能够放在分布式的片上存储器中。IPU 中学习是模型结构和参数的推理过程；而所有的智能处理，也即推理，采用了概率优化。 


IPU 架构的一个优点是它适用于当今的许多机器学习方法（如 CNN），也针对不同的机器学习方法进行了高度优化，如强化学习等。该架构重新考虑了传统的微处理器软件堆栈，把开发人员定义的关于矢量和标量的事务转化成图和张量形式，把算法过程和编程过程变成「从数据中学习」这样一种知识模型。整个知识模型的表示被分解为巨大的并行工作负载，在 IPU 处理器上进行调度和执行。 


当 IPU 运行时，IPU 程序从一个或多个张量读取数据，并将其结果写回其他张量。张量可以由多个块处理，每个块都对本地存储的张量元素进行操作。 


该计算可以表示为一个图，其中节点表示由一个块执行的代码，边是由节点操作的数据（见图 3.15）。如果数据在执行节点代码的块的内存中，则这些边表示对本地内存的读取和写入。如果存在另一个块上存储的变量，则边表示通过信息交换结构进行的通信。 


由一个节点执行的功能可以是任何操作，包括从简单的算术运算到重塑或转置张量数据，或执行一个 N 维的卷积。 


![img](https://pic1.zhimg.com/v2-722f8c4b92260abd435d1c340bf4fb2b.webp)

IPU 集成了大量混合精度浮点单元，据称其推理和训练性能比其他公司的 AI 芯片高 10～1000 倍，但细节尚未披露。据说 IPU 具有如此高的性能，是为了在处理器中维持完整的机器学习模型。该芯片名为 Colossus-IPU，第一代采用 16nm 工艺制造，并于 2017 年底开始交付。 


第二代芯片 GC200 于 2020 年 7 月发布，采用台积电 7nm 工艺技术，在 823 mm  2  的裸片上包含了超过 594 亿个晶体管。这比 2020 年 5 月英伟达最新发布的 GPU A100 的 540 亿个晶体管增加了约 10%。GC200 集成了 1472 个独立的 IPU 内核，可以执行 8832 个独立的并行计算线程（1 个 IPU 运行 6 个线程）。该芯片包含了新开发的被称为 AIFloat 的浮点 MAC 来执行性能达 1 PFLOPS 的计算。GC200 的实际性能据称比第一代提高了 8 倍。 


Graphcore 在 2017 年底曾发布 IPU 的基准测试结果。根据这个结果，为了训练 ResNet50 的神经网络，单片 IPU 每秒可训练 2000 幅或更多图像（批量大小为 8）；使用配备 8 个 IPU 的加速卡，每秒可以学习 16,000 幅图像；性能可随着核的数量增加而线性提高。使用 LSTM 神经网络的推理要比英伟达的 P100 快 182～242 倍。 


装载 Graphcore GC200 板卡的戴尔服务器，据说现在已经销售一空，如果一台服务器就能达到 2 PFLOPS 的性能，那当然会有很多开发 AI 的公司去买，也就会十分受欢迎。该产品的应用重点是数据中心和一些需要大量计算的边缘侧应用（如自动驾驶汽车），目前不针对手机等消费类边缘侧设备。 


#### 架构设计及优化的其他考虑


实验证明，最高的能耗来自访问片外 DRAM 存储器以进行数据移动，而不是乘积累加计算本身。换句话说，由于大量操作导致的额外存储器访问和数据移动的能耗成本经常超过计算的能耗成本。因此，深度学习加速器在架构设计上需要仔细考虑这一点，以便在运行时间和功耗方面实现高效的架构。 


近年来，关于如何减少存储器数据来回移动的问题，已经有大量的研究成果被发表。例如，陈云霁等人创建了一个 64 芯片系统的架构，通过尽量靠近存储数据，最大限度地减少突触和神经元之间的数据移动。它减少了外部存储器带宽的负担，并且在能耗降低到 1/150 的情况下实现了 450 倍的加速  [42]  。王世豪等人建议将相邻的 PE 分组为称为 ChainNN 的双通道 PE，以减少大量的数据移动。他们在台积电 28nm 工艺下进行了模拟，并在 AlexNet 中实现了 806.4 GOPS 的峰值吞吐量  [43]  。Yann LeCun 的研究团队则把用于 32 位 CPU 上的 SIMD 处理器，用来设计针对 ASIC 综合的一个系统，以执行百万像素图像的实时检测、识别和分割。他们利用硬件中的可用并行性优化 CNN 中的操作。在处理帧率方面，这个 ASIC 实现优于 CPU 传统方法  [44]  。 


许多研究人员从数据并行性和流水线并行性两方面来提高神经网络架构的并行性，以提高整体吞吐量。除了上述数据流图架构之外，沈基永（Jaehyeong Sim）等人提出了一种相对复杂的神经元处理单元（Neuron Processing Element，NPE），它由 MAC 块、激活函数块和最大汇集块组成  [45]  ，这是数据并行和流水线并行的混合。在该设计中，在一个 NPE 中为不同的 MAC 计算共享相同的内核卷积核数据，并且将不同的内核应用于不同的 NPE 以利用输入图像数据。Hinton 等人则采用了基于数据流处理的简单 PE 的高度并行空间体系结构  [46]  。每个 PE 都具有灵活的自主本地控制和本地缓冲，部分求和由空间数组中的 PE 计算，并存储在本地存储器中以进行最终求和，求和结果用于下一层计算。 


在非常大的神经网络中，数据重用的方法尤其重要。需要合理地分解超大卷积并将其映射到有效电路，这也涉及存储系统的优化，主要技术是尽量减少代价较昂贵的存储器层级的访问。研究人员开发了诸如本地缓冲区、分区（Tiling）和数据重用之类的技术，以最大程度减少数据移动。分区和数据重用技术可以有效地减少存储器流量，但需要非常仔细地设计数据移动。PE 上的本地存储缓冲区是 PE 的专用缓冲区，旨在最大化数据重用，但在中间结果的存储量很大时变得不可行。片上存储器都有需要在容量和面积之间取得折中的问题，这需要通过仔细的数据移动和数据压缩技术来设计解决。孙广宇等人提出了一种分区的结构层，采用片上存储器并减少外部存储器访问  [47]  ；Yu-Hsin Chen 等人则采用了运行长度压缩技术来降低图像带宽要求  [48]  。 


尽管有很多方法可以应用这些数据重用的技术，但是我们可以总结出几种常用的数据流设计模式：行固定、权重固定、输出固定（Output Stationary）和输入固定（Input Stationary）。这些数据流设计模式可以在许多最新的深度学习 AI 芯片设计中看到。 


行固定方法可以在卷积计算中防止往返 PE 阵列的重复数据流，从而消除由于数据传输而造成的功率浪费。它可对所有类型数据（权重、输入激活和部分和）在寄存器文件级别上最大限度地重用和累积，从而降低总的能耗。Eyeriss v1、Eyeriss v2 芯片（见本书 4.3.1 节）采用了行固定方法。除行固定方法外，典型的数据流方法包括权重固定方法，该方法适用于如 TPU 中所使用的批量处理。权重固定数据流旨在通过最大限度地重用每个 PE 的寄存器中的权重来使读取权重的能量消耗达到最小值。英伟达和谷歌的设计都采用了这种方法。 


输出固定方法旨在最大限度地降低读写部分和的能量消耗，比较适用于一些使用稀疏压缩的 AI 芯片。另外，还有一种输入固定方法，其设计目的是为了使读取输入激活的能耗降到最低。 


尽管创建新的数据流和优化这些数据流的硬件架构有无限种可能性，但是如何把数据流和芯片的架构匹配到在性能和能效上达到最佳，仍然是一个有待继续深入研究的问题。 


备案号:YX01jbkWgwB9w7Lle

