## 6.AI 芯片的种类
过去，大部分 AI 模型的建立或算法的运算，都是在以 CPU 为核心的计算机里进行模拟的结果。 


近些年来，正如摩尔定律所揭示的，业界通过在芯片上放置越来越多的晶体管使 CPU 实现更高的性能。然而，由于严格的功耗限制，片上时钟频率无法跟随这种上升趋势。使用多核处理器可以在不提高时钟频率的情况下提高计算性能。因此，从 2005 年开始，主流厂商转而采用多核处理器作为克服该问题的替代解决方案。但是很可惜，这种解决方案从长期来看可扩展性并不好。通过在芯片内部添加更多处理器核所实现的性能提升，是以各种快速增长的复杂性为代价的，如核与核之间的通信、内存一致性，还有最重要的功耗问题。 


在早期的芯片工艺技术发展节点中，从一个节点到下一个节点允许的晶体管频率几乎加倍，并且可以通过降低电源电压，使功率密度几乎保持恒定。随着工艺技术进一步发展，虽然从一个节点到另一个节点的晶体管密度仍会增加，但它们的最大频率大致相同，并且电源电压不会相应地降低。结果，现在每个新技术节点的功率密度都在增加。因此，现在最大的挑战是降低单位平方毫米的功耗和热量耗散。 


这种趋势很快就会限制内核数量的扩展，就像十多年前单个 CPU 核时钟频率被限制的情况那样。很多技术论文将这种情形表述为「暗硅」（Dark Silicon）效应——芯片在工作的时候，其中一部分区域必须保持断电以符合热量耗散约束条件。用一个简单的比喻来说，就好像房间里装了很多几百瓦的大灯泡，不能同时打开，只能让其中一部分点亮，如果同时打开，该房间的线路就会因高温被烧毁。解决此问题的一种方法是使用硬件加速器。硬件加速器可帮助处理器降低工作负载，提高总吞吐量并降低能耗。 


现在的绝大多数 AI 芯片，就是这种硬件加速器。目前市场上第一批用于 AI 的芯片包括现成的 CPU、GPU、FPGA 和 DSP，以及它们的各种组合。虽然英特尔（Intel）、谷歌（Google）、英伟达（NVIDIA）、高通（Qualcomm）和 IBM 等公司已经推出或正在开发新的芯片设计，但目前还很难说哪家一定会胜出。一般来说，总是需要至少一个 CPU 来控制系统，但是当数据流需要并行处理时，将需要各种类型的协处理器（即硬件加速器），这就是专用集成电路（Application Specific Integrated Circuit，ASIC）芯片。 


CPU、GPU、FPGA 及 ASIC 这 4 种芯片有不同的架构（见图 1.8）。下面分别讨论这 4 种芯片。 


![img](https://pic3.zhimg.com/v2-a08abc4b7ec1d4ebcbea6e1c53301344.webp)

**1.CPU**


AI 算法（包括深度学习算法）可以在通用 CPU 上实现。由于 CPU 的普及和通用编程能力，神经网络技术在开始时期被大量推广和应用。一些关键的算法验证都是在 CPU 上完成的。 


但是，CPU 并不是实现神经网络的理想硬件。CPU 以提供复杂的控制流而闻名，这对于更常规的、基于规则的计算可能是有益的，但对像使用数据驱动方法那样的神经网络却不是那么重要。神经网络的运行过程几乎不需要控制，因此在神经网络计算中，数据流是计算的主要部分，而不是控制流。 


另外，对于 AI 算法的硬件实现和大规模应用来说，高吞吐量和低时延计算的需求在不断增长，而通用 CPU 是基于冯·诺依曼（Von Neumann）体系结构的处理器，存在不少结构限制；摩尔定律发展的放慢也意味着 CPU 的性能无法再得到快速改善。 


**2.GPU**


GPU 最初用于快速图形渲染，它适用于 SIMD 并行处理，能快速进行图形渲染相关的浮点密集型计算。GPU 架构的发展非常迅速，从一开始的可重构 GPU，发展到可编程的大规模并行协处理器，这使它非常适合 AI 这样需要高性能并行计算的场景。为了满足更快和更高精度 AI 的需求，人们正在继续推动越来越多并行 GPU 的开发。现在，具有超过 1000 个处理核和很大容量片上存储器的 GPU，其功耗仅为几瓦。尽管如此，对一些终端应用来说，这样的能效还不够高。 


另外，利用 GPU 来模拟类脑架构，如脉冲神经网络（Spiking Neural Network，SNN），也是一种选择。其中一种被称为 GPU 增强型神经网络（GPU-enhanced Neural Network，GeNN）的类脑网络，利用神经网络算法的并行特性提供仿真环境，可以通过基于代码生成的方法在英伟达通用 GPU 上执行。 


为了帮助软件工程师更方便地开发并行软件，像 CUDA 这样的 GPU 专用编程系统也一直在不断完善，它可以运行数万个并发线程和数百个处理器核。但是从本身的架构来说，GPU 的设计还存在一些缺陷。例如，很难快速地为 GPU 加载数据以使它们保持忙碌状态等。因此，很多人在继续研究新的 GPU 架构。无论如何，在模拟大型人工神经网络时，GPU 还是发挥了非常大的作用。 


**3.FPGA**


现场可编程门阵列（Field Programmable Gate Array，FPGA）是一种「可重构」芯片，具有模块化和规则化的架构，主要包含可编程逻辑模块、片上存储器及用于连接逻辑模块的可重构互连层次结构。此外，它还可能包含数字信号处理模块和嵌入式处理器核。从电路级设计来看，FPGA 可以通过使用触发器来实现时序逻辑，通过使用查询表来实现组合逻辑。通过执行时序分析，可以插入流水线级以提高时钟频率。从系统级设计来看，FPGA 可进行高级综合，可以将 C 语言转换为可综合的硬件描述语言（Hardware Description Language，HDL），以加速开发过程。 


FPGA 的优点是非常明显的。即使在被制造出来以后，FPGA 都可以在运行之前和运行期间对硬件进行重构，这给硬件应用带来了极大的灵活性。因此在 20 世纪 80 年代末，赛灵思（Xilinx）公司在最初推出 FPGA 时就宣称，它是芯片业界的一场革命。 


近年新发布的一些 FPGA 采用了嵌入式 ARM 内核的片上系统（System-on-a-Chip，SoC）设计方法。如今，具有约 10 亿个逻辑门复杂度和几兆字节（MB）内部静态随机存取存储器（Static Random-Access Memory，SRAM）的 SoC 设计可以用最先进的 FPGA 实现。它的时钟频率接近吉赫兹（GHz）范围，因此算力可以在几瓦的功耗下达到 GFLOPS 的数量级。因此，FPGA 为并行实现人工神经网络提供了一个有吸引力的替代方案，具有灵活性高和上市时间短的优势。 


开发 FPGA 和 ASIC 设计的时间基本相当，但 FPGA 的一大优势是不需要制造时间，在用有效的 EDA 工具电路综合之后，即可直接测试新设计。而且 FPGA 的并行化程度可以做得很高。但是，与 ASIC 相比，FPGA 的缺点是速度更慢、面积更大、功耗更高。 


未来，并行标准芯片（如多核 CPU、GPU 或 FPGA）在市场驱动的开发和改进中具有成本效益高、容易获得等优势。它们具有最高的灵活性，不但可以用于深度学习加速器，而且可以用于本书第 5 章将介绍的类脑架构及其他类型的智能计算和仿生计算。 


**4.ASIC**


ASIC 是定制的专用 AI 芯片，可以在芯片架构和电路上进行优化，以满足特定的应用需求。无论从性能、能效、成本角度，还是算法的最佳实现方面，ASIC 都是标准芯片无法比拟的。随着 AI 算法和应用技术的发展，ASIC 逐渐展现出自己的优势，非常适合各种 AI 应用场景。现在大部分 AI 芯片的初创公司，都是以开发一种独特、新颖的 ASIC 为目标。 


把 AI 算法「硬件化」，即用 ASIC 来实现，带来了高性能、低功耗等突出优点，但缺点也是十分明显的。ASIC 芯片的开发需要很高的成本，一旦设计完毕，交由芯片代工厂流片（即制造芯片），首先需要支付一大笔一次性工程费用。这种费用一般不会低于 1000 万美元。这对中小型企业，尤其是初创公司来说，是一道很高的门槛。而 AI 又属于一个技术迭代速度很快的领域，现在几乎每隔几个月就会有新的算法和模型出现，这对于开发芯片的公司来说，意味着很大的商业风险。 


ASIC 芯片一旦开始批量生产，就无法再改动里面的硬件架构。万一市场对 AI 芯片功能的需求出现重大变化，或者研发成功了新的 AI 算法，那这款芯片就只能被淘汰而由芯片设计者继续开发新的芯片。有的公司为了及时使用新的算法，甚至需要中途从代工厂召回正在生产的 ASIC 芯片，以更新芯片设计。 


为了避免这种风险，除了选用灵活性很高的 FPGA 来实现之外，还可以采用模块化设计方法，即形成一个 IP 核（知识产权核，Intellectual Property Core）的库，可以根据设计需要来选取。 


目前比较前沿的研究是设计「可进化」芯片，它基本上接近通过芯片的「自学习」来提升芯片自身的性能。AI 芯片的最终目标是能够「自学习」，即芯片能够自己学习「如何学习」；另外一个重要目标是做到智能机器之间（相当于 AI 芯片之间）的相互学习和协调，从而使智能机器自己得到更多的知识。这种「自学习」的性能，很可能随时间呈指数级提升，并将最终导致智能机器的智能水平超越人类。这条路虽然还很漫长，但是一些研究人员已经开始起步。 


图 1.9 为不同种类的 AI 芯片及其计算范式。 


![img](https://pic1.zhimg.com/v2-e5a81e68b1439083e2bb896c04db351f.webp)

#### 深度学习加速器


FPGA、GPU 和多核 CPU 处理器等并行标准芯片种类的日益丰富，为人工神经网络的实时应用提供了更大的选择范围。由于这些标准芯片价格低廉且可从市场上直接获得，采用它们实现神经网络计算比较容易起步。 


深度神经网络本质上是并行的，因此很明显，使用多核处理器来实现深度神经网络是有吸引力的。「并行计算」这个概念让很多研究人员至少着迷了 30 年。过去一段时间，人们在并行计算领域付出的努力已经让市场看到了希望并聚集了不少投资。但截至目前，单处理器计算仍占上风。然而，通用计算正在向并行架构迈出不可逆转的步伐，因为单线程单处理器的性能无法再以过去那种速度提高。 


具有高度并行性的神经网络的兴起，使人们对高性能并行计算的需求又一次被提上日程。真实世界的应用是并行的，硬件也可以做到并行，而缺少的是编程模型和支持这些不断发展的大规模并行计算体系结构的系统软件。此外，如何在多核架构的计算能力、存储器容量及内部和外部通信带宽之间取得平衡，目前尚无明确共识。 


在并行超级计算机或计算机网络上模拟大型神经网络，把神经网络映射到多核架构方面，已经有了很多研究成果，各种技术被开发出来。多核处理器也可以嵌入机器人或智能手机这样的移动设备里。 


如前文所述，深度学习算法中的大部分处理涉及矩阵乘法。GPU 擅长矩阵和矩阵的乘法，以及矩阵和矢量的乘法。在 GPU 中，这些乘法被大量用于三维坐标的转换。GPU 具有数百到数千个乘积累加的核，适用于张量处理。正是由于这个原因，英伟达的 GPU 以作为 AI 芯片而闻名。当然，使用配备 DSP 或 SIMD 指令的处理器也可以完成类似的处理，但很少有处理器能在核的数量及并行处理能力方面与 GPU 相当。 


但是，由于 GPU 主要用于图形处理，因此它还包含张量处理之外的其他功能，这导致了高功耗。因此，业界已经开发出具有专用于张量处理功能的结构的 ASIC 芯片。基于几十年的技术积累，深度学习算法正在投入实际应用。为了达到更好的应用效果，需要有这种专用的 AI 芯片来加速处理深度学习，帮助多核 CPU 处理器加速运算。今后的趋势是把这类加速器和多核 CPU 处理器集成到同一块芯片中。 


总之，虽然 AI 芯片的开发主流都是基于深度学习算法，但是深度学习本身也在不断改进和更新。从 2009 年开始，深度学习相关的论文数量每隔两年都会翻一番；2018 年之后，每天约有 100 篇关于这方面新的算法和思路的技术论文发表。这说明深度学习仍然是一个重要的研究领域，有许多有前景的应用和各种芯片设计的创新机会。 


#### 类脑芯片


要让 AI 芯片达到更高的智能水平，非常重要的一点是要使神经网络的运作模式更像人类大脑。除了大学研究所之外，很多开发 AI 芯片的公司专门成立了「脑神经科学」部门（如被谷歌收购的 DeepMind），以进一步研究人脑的思考过程，建立更科学、更符合生物特性、更细致的神经网络模型。 


本书 1.3.1 节中讲到的深度学习加速器，就是在如何提高乘法计算和累加计算的性能上下功夫。为了贴近大脑的生物特性，AI 芯片应该具有在模拟而非数字方面更准确地模拟人类大脑运作的功能。用这种方法开发的 AI 芯片被称为类脑芯片或者神经形态芯片。这种芯片基于新的芯片架构，关键组成部分包含脉冲神经元、低精度突触和可扩展的通信网络等。脉冲神经元的概念直接来源于哺乳动物大脑的生物模型，其基本思想是神经元不会在每个传播周期都被激活，只有在膜电位达到特定值时才会被激活。膜电位是与细胞膜上的电荷相关的神经元的内在参数。 


这类芯片的特点是基本上没有时钟，它采用事件驱动型操作（如模仿人类大脑电脉冲的脉冲神经网络），因此它的功耗远低于那些基于 DSP、SIMD 和张量处理等处理方式的芯片。 


在过去 10 年中，美国和欧洲的大型政府资助计划都聚焦于神经形态芯片的开发。这些芯片按照以生物学为基础的原理运行，以提高性能和能效。例如，有些项目直接把许多输入「硬」连接到各个电子神经元，而有些项目使用类似生物神经元之间发生的短异步尖脉冲电压进行通信。 


每个神经元、突触和树突均可以采用数字或模拟电路来实现。但是实际上，新的类脑芯片通常仍以传统的数字电路实现。对于数字芯片，我们可以使用有效的 EDA 软件工具来实现快速、可靠和复杂的设计，也可以使用最先进的生产线来生产器件密度最高的芯片。 


而模拟电路的设计需要更多的设计时间，且设计人员需要具备良好的晶体管物理理论知识和布局布线设计的熟练经验。另外，只有少数工艺线（如 TI 等拥有的）以模拟电路为主要产品，对模拟电路的工艺流程有足够的经验。 


此外，也有一些研究人员已开始将一些新型半导体器件，如阻变存储器（Resistive Random Access Memory，RRAM）技术应用于脉冲神经网络，来实现实时分类。 


大约自 2018 年起，深度学习加速器在性能上有了飞速发展，占领了主流市场（这导致目前大部分人讲到 AI 芯片就只指深度学习加速器），而类脑芯片在性能上与这些深度学习加速器拉大了距离。类脑芯片在截至本书成稿时还无法真正商用。 


因此，有家初创公司想走一条捷径：试图仍然使用深度学习的基本框架来保持较高的计算性能，而把类脑芯片的一些优点结合进去，如使用脉冲来激活深度学习中的输入，这样也可以比一般的深度学习模型大大降低功耗。这种思路仍然是深度学习的思路，而并非是类脑芯片。 


个别研究深度学习的权威专家甚至认定基于神经形态计算的类脑芯片方向走不通，不如继续沿着深度学习这个思路深入研究下去。然而，类脑芯片不光是学术界实验室的产物，在产业界也有很多团队仍然在积极研究。更重要的是，深度学习更多地被视为一种数学模型，而类脑芯片才更接近模拟大脑功能的实际特征，因此有着达到高能效的巨大潜力。英特尔等公司的研究结果表明，当正确准备好数据时，这种神经形态计算可能非常强大，这可能会为新型电子产品提供机会。 


#### 仿生芯片及其他智能芯片


自然计算和仿生计算都是随着计算机硬件和计算机科学的进步而发展起来的。这些计算范式和算法涵盖了非常广泛的领域，其中发展最快的要数现在用得最多、最广的人工神经网络，这也是芯片实现最成功的领域。因为神经网络已在前文单独叙述了，本节介绍自然计算与仿生计算时就不再赘述。 


自然计算包含的范围很广，它不但包括对生物机制的模拟，还包括对大自然的物理（包括量子物理）、化学现象，及社会、文化、语言、情感等复杂自适应系统等的模拟，具有一定的智能机制。它被用来解决传统算法解决不了的问题，主要是在解决最优化问题上显示出强大的生命力和进一步发展的潜力。由于这类计算把大自然中有益的信息处理机制作为研究和模仿对象，也有人把这类算法称为「智能算法」。 


自然计算的算法有退火算法、遗传算法、文化算法、蚁群算法、细胞神经网络、模糊算法、情感计算的算法、烟花算法等，也包含量子算法。这些算法可以用数字芯片或模拟芯片实现，很多是用 FPGA 实现，或者仅利用多核 CPU 进行模拟。而最近几年备受瞩目的量子计算，也有不少公司和大学研究所采用普通互补金属氧化物半导体（Complementary Metal Oxide Semiconductor，CMOS）硅基芯片，或硅光芯片来实现。现在，研究人员已经看到了用这样的芯片实现量子机器学习、量子 AI 计算的曙光。 


仿生计算是对大自然生物机制的模拟，包含遗传计算、细胞自动机/细胞神经网络、免疫计算、DNA 计算（又称分子计算）等。生物体的自适应优化现象不断给人以启示，尤其是生物体和生态系统自身的演进和进化可以使很多相当复杂的优化问题得到解决。仿生算法主要利用 FPGA 实现，因为仿生芯片需要支持运行期间的硬件动态重构，还要有一定的容错性和鲁棒性。仿生计算应该属于上述自然计算的范畴，但因为它对 AI 芯片的未来发展会有重大影响，所以在这里专门列出。 


如前文所述，AI 芯片的一个关键问题是「自学习」及「自进化」。这依靠什么来解决呢？如果我们关注一下仿生芯片的几大特点，就会有比较清楚的认识了。 


（1）仿生芯片无须人工干预，通过自身在线进化，可以实现自动升级（不仅是软件升级，更主要是硬件升级）和自动设计。 


（2）仿生芯片可以根据环境变化自适应调整硬件电路结构。随着芯片技术的进步，现在已经可以做到接近实时（10～15 个时钟周期）的自适应速度，就是说像一条变色龙那样，到一个新环境，马上就「变色」（改变硬件电路）。 


（3）仿生芯片可以自己修复错误。现在芯片的线宽已经达到 7nm 及以下，非常难以控制故障的出现。仿生芯片具备容错功能，能自动恢复系统功能。 


（4）仿生芯片和深度学习加速器芯片结合，可以进一步提高运算速度，适应算法的不断更新。 


业界把仿生芯片的这些特点，总结为「有机计算」的范式，也成立了专门的国际组织来讨论和交流这一领域的进展和趋势（见第 12 章）。 


类似大脑架构和受自然界生物启发的自适应和模糊控制，投入实际使用已有 20 多年，但它们仍局限于仅面向某个功能，且对产品或服务的技术性能要求不高的应用。现在，随着 AI 芯片热潮的兴起，仿生芯片和其他智能芯片将会大有用武之地。仿生计算和其他智能计算的基本思路，既可以作为一种应用，用于 AI 芯片本身的扩展和完善，形成不同程度的「自学习」及「自进化」功能（如使用受量子物理启发的算法），又可以与目前的 AI 芯片结合起来，如应用仿生算法对 DNN 的架构进行神经架构搜索（Neural Architecture Search，NAS）及超参数优化，进一步提升深度学习的性能和能效。 


#### 基于忆阻器的芯片


20 世纪 80 年代初兴起的 CMOS 电路及其工艺实现，是芯片技术发展的一个重大里程碑。直到今天，芯片实现的基础还是 CMOS。除 CMOS 技术外，忆阻器（Memristor）也是深度学习加速器和类脑芯片的潜在硬件解决方案。「Memristor」一词是 Memory 和 Resistor 这两个英文单词的组合。忆阻器是新颖的两端元件，能够根据在其端子上施加的电压、电流来改变其电导率，最早是由美籍华人蔡少棠（Leon Chua）教授于 1971 年基于电路理论推理发现并证明的。蔡教授为找到电子学中除了电阻器、电感器、电容器之外的第 4 个基本电路元件而兴奋不已：当分别把电流、电压、电荷和磁通量画出 4 个区域时，其中 3 个区域可以对应电子学的 3 个基本电路元件——电阻器、电容器和电感器，剩下的一个区域对应一种非常独特的电子特性，但是当时只能从理论上证明会有这样一种基本电路元件存在（见图 1.10）。 


![img](https://pic4.zhimg.com/v2-3430c14d5fa4b1ca08e779a11064500d.webp)

电阻器由电压 v 和电流 i 之间的关系定义；电感器由磁通量 φ 和电流 i 之间的关系定义；电容器由电荷 q 和电压 v 之间的关系定义；忆阻器则通过磁通量 φ 和电荷 q 之间的关系来定义。忆阻器的特性 i/v 方程可近似表示为：


![img](https://pic1.zhimg.com/v2-16a81a6f9268a2176196d74f877b73c5.webp)

式中，i  M  R  、v  M  R  分别是忆阻器两端的电流和电压降；G(w，v  M  R  )是随施加电压变化的电导（假设电压或磁通量受控制）的元件模型；w 是物理特征参数，其变化通常由所施加电压的非线性函数 f  M  R  决定。 


直到 2008 年，惠普公司的斯坦利·威廉（Stanley Williams）等人第一次在实验室里将用二氧化钛（TiO  2  ）制成的纳米元件夹在两个铂电极之间（Pt-TiO  2  -x-Pt），做出了世界上第一个基于 TiO  2  薄膜的基本元件，即忆阻器。从那时起，研究人员已经发现并提出了许多纳米级的电阻材料和结构，最典型的是基于氧化还原的阻变存储器。 


![img](https://pic3.zhimg.com/v2-0a47f5454b77fe90900432acce9f56ee.webp)

从图 1.11 可以看到，忆阻器本身就像一个矩阵排列，两根交叉棒（Crossbar）的交叉点（Cross Point，又称交叉开关）就是可变电导与电压的相乘，而利用把这些交叉开关连起来的电流，就可以实现累加。具体来说，以电阻的电导为权重，电压为输入，电流为输出，来进行乘法运算；通过将不同忆阻器的电流相加来完成加法。这是根据基尔霍夫电流定律而来的，比用数字电路来实现乘积累加简单、直接得多。 


乘积累加操作可以通过将忆阻器这样的可编程阻变元件直接集成到非易失性高密度存储芯片中来实现。处理单元被嵌入存储器中，可减少数据移动。而在与动态随机存取存储器（Dynamic Random-Access Memory，DRAM）密度相当的情况下，将存储器与处理单元集成在一起，可使芯片密度大大提高，并可大大节省存取时间，从而降低功耗。这就是存内计算（Processing In Memory，PIM）  [[1]](#note_1)技术，又称为存算一体化。目前比较热门的新型非易失性存储器（Nonvolatile Memory，NVM）包括相变存储器（Phase ChangeMemory，PCM）、阻变存储器（RRAM 或 ReRAM）、导电桥 RAM（Conductive Bridging Random Access Memory，CBRAM）和自旋转移力矩磁性 RAM（STT-MRAM 或 STTRAM，是几种已知的磁性 RAM 技术之一）等。这些器件在耐久性（即可写入多少次）、保留时间、写入电流、密度（即单元尺寸）、不一致性和速度方面各有不同的特点。 


忆阻器的阵列结构最适合进行点积乘法和累加运算，而这类运算占深度学习算法中的绝大部分。由忆阻器组成的芯片因为不使用或很少使用有源器件，从而可以使芯片的功耗大大降低。对于 AI 芯片来说，这提供了使用数字模拟混合信号电路设计和先进 PIM 技术来提高效率的机会。当然，所有这些技术也应该结合起来考虑，同时要仔细了解它们之间的相互作用并寻找硬件和算法协同优化的机会。 


忆阻器除了适合大规模乘积累加运算外，它的结构也与基于脑神经元的类脑结构十分吻合。因此，用大量忆阻器阵列来组成类脑芯片，会有非常诱人的前景，也有各种硬件设计的创新机会。例如，把忆阻器做成可重构电路；把忆阻器做成多端口晶体管，以进一步模仿神经元中的多个突触；或者做成混沌电路后，利用混沌电路的非线性特性来模仿大脑行为等，这些新的想法正在引起研究人员的广泛兴趣（见图 1.12）。 


![img](https://pic3.zhimg.com/v2-47c6ea264ed4ecfcfb6001720dc8e5f0.webp)

虽然现在已有很多研究团队制作出了第一代含有忆阻器的 AI 芯片（基于 RRAM），但是目前还处于在实验室进行小批量试用的阶段。有的实现了模拟 RRAM，有的则实现了数模混合 RRAM 存内计算。但是，工艺不一致性、电路噪声、保持时间和耐久性问题等现实挑战，仍然阻碍着基于 RRAM 的 AI 芯片的商业化。而模拟矩阵乘法面临的精度低、器件不一致性高和模数转换功耗大等问题，还亟待解决。 


备案号:YX01jbkWgwB9w7Lle

