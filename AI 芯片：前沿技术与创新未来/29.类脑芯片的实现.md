## 29.类脑芯片的实现
SNN 的硬件实现可以或多或少地模拟类似生物的神经元和突触模型，这可能有助于获得更多关于大脑运作的知识。用 SNN 也可以完成深度学习应用，但是由于 SNN 的时间离散性，如在传统计算机上进行模拟将会花费大量时间。为此，把 SNN 用 ASIC 芯片或 FPGA 来实现是一个比较可行的方法，可以利用 SNN 的属性来降低功耗并提高推理速度，尤其是把神经元和突触做成硬件较友好的模型（如简单的 IF 神经元和恒定权重）。 


类脑芯片是由一个或几个神经核组成的芯片。神经核集成了处理神经元膜电位的处理单元、存储突触值和神经元状态的存储器、用于接收和发送脉冲的输入和输出接口及控制电路。针对超大规模应用的设计通常会实现大量的神经核，以在芯片级实现最大程度的并行化。 


SNN 实现的主要特点在于将操作简化为加法器和比较器，也就是说，如果是数字实现，只需要加法器，而不需要深度学习 AI 芯片中最主要的核心——MAC 运算（见第 3 章），因此可以大大降低计算成本。由于脉冲是二进制的，并且不需要在神经元之间传输模拟值，因此可以使用低功耗的比较器电路代替放大器。因此，电路构造能够变得简单并且使低功耗成为可能。但是，由于信号时间是 SNN 计算中最基本的机制考虑，因此必须存储定时信息（用于片上学习、漏电和不应期机制）和神经元状态变量（膜电位和阈值），这就需要辅助存储器。由于可能有大量的处理单元和连接，类脑芯片在一些网络架构中会使权重访问迅速成为瓶颈。解决这一瓶颈的方法之一是采用存内计算（见第 7 章）及新型非易失性存储器。 


如果使用一般基于 SRAM 的神经形态器件，在关闭电源后，存储在 SRAM 中的突触权重就会消失。另外，执行任务之前的学习常常是在脱机系统中使用与器件分开的普通计算机执行的，执行任务时仅仅将学习结果加载到 SRAM 中。为了克服这些挑战，很多研究人员正考虑采用非易失性存储器，把相变存储器和阻变存储器之类的存储器件作为突触。 


目前存内计算技术正在受到人们的广泛重视。存内计算的总体思想就是尽可能把存储器和处理器放在一起，以降低每次到存储器进行数据存取所消耗的能量。这其实也是生物神经系统除了高度互连和脉冲特性之外的另一个特征。类脑芯片架构也越来越多地引入了这种方法，即每个处理器都配置很小的本地存储容量。这种配置类似于人脑的组织，其中神经元同时进行数据存储和处理。研究人员认为，类脑芯片架构的这一元素可以更好地复制人类的学习和记忆。 


如第 3 章所述，为了达到较高的计算稀疏性，深度学习 AI 芯片需要辅助硬件来检查数据的无效性（为零的元素）。但是，类脑芯片本身是事件驱动硬件，稀疏性非常高，大大减少了切换次数。由于使用阈值操作，激活程度较小的神经元就不会被激发。 


类脑芯片可以使用模拟电路设计来实现 PE 和存储器，这可以根据不同的目标应用和所需的性能指标来选择。神经形态计算的目标是仿真精确的生物启发式神经元计算，达到比深度学习 AI 芯片更低的成本、更低的功率或更高的精度。如果使用集成模拟电路，一些神经元功能实现起来就会非常简单。例如，要实现数字相加，如果用模拟电路算出电流求和的树枝状输入信号的总和是相当方便的，比普通数字累加器要简便得多；或者用两象限乘法器实现数字相乘，仅需要 5 个晶体管；利用器件的非线性或寄生效应，还能够实现复杂的函数功能，如指数函数或平方根函数等。但是，模拟电路一般很难达到高集成度，因为一般要求晶体管有较大面积，以确保可接受的精度并提供成对晶体管的良好匹配，如电流镜或差分级中所使用的晶体管对。 


虽然 SNN 有上述固有的好处，但是也有一些方面比不上深度学习算法。例如，SNN 需要许多时间步骤算法来处理输出，在此期间需要连续发送输入信号。因此，脉冲网络的前馈操作必须实现多次，而 DNN 仅被处理一次。在这些条件下，数据集越复杂，相对于 DNN 来说，SNN 的每个分类所消耗的能量就越大。而对于网络运行的分类准确性而言，根据目前的测试情况，有的情况下 DNN 分类准确度高，有的时候 SNN 更高。一般来说，可能还需要根据输入数据的特点来加以区分，如果是事件驱动数据集，那么 SNN 就会占优势。 


另外，当前实际使用 SNN 的一个主要问题是训练。创建新的学习规则和机制对于神经形态系统适应特定应用的能力至关重要。通常，学习算法的目标是修改神经元之间突触连接的权重，以改善网络对某种刺激的响应。在 DNN 中，通常由误差反向传播来进行训练，但在 SNN 中，由于脉冲时间的不连续性，作为反向学习算法基本功能的梯度下降已经无法实现。 


尽管脉冲网络具有不连续性，但是已经有不少研究人员设法克服了这个缺点，仍能使脉冲神经元使用梯度下降法，在较长的时间序列上执行复杂的任务。这为训练 SNN 打开了大门  [76]  。还有些研究人员则使用了 DNN 来训练，然后直接映射到 SNN，并把输入激励转换成脉冲；另外，也有研究人员利用进化算法来训练 SNN。 


还有一些研究人员正在研究将时间进程包括在反向传播方法中的解决方案，从而可以在事件驱动的数据集上训练网络而无须任何转换。因此，为 SNN 找到一种很好的训练方法，最终发挥出全部潜力可能只是时间问题。 


#### 忆阻器实现


CMOS 实现神经形态并行架构的主要瓶颈之一，是神经元之间大规模突触互连的物理实现及突触的自适应性。为了实现自适应突触连接，CMOS 技术要求将大量电路用于模拟存储器或数字存储器模块，这要占去很大的芯片面积，而且在功耗上也很难达到需求指标。此外，还必须实施用于更新这些突触存储器件的学习规则。 


忆阻器的出现，正好解决了上述瓶颈，在神经形态架构的功率和速度方面具有优势。一些研究机构和公司已经根据忆阻器的工作机制研制了具有不同电导切换机制的器件，如相变存储器（PCM）、导电桥存储器（CBRAM）、铁电存储器（Ferroelectric Random Access Memory，FeRAM）、基于氧化还原的电阻切换存储器（RRAM）及有机忆阻器件（Organic Memristive Device，OMD）等。它们各自在紧凑性、可靠性、耐用性、存储器保留期限、可编程状态和能效方面表现出不同的特性。这些器件具有一些特别有价值的特性，可作为电子突触器件，例如： 


（1）忆阻器的特征尺寸可以缩小到 10nm 以下； 


（2）它们的存储状态可以保留几年； 


（3）它们可以以纳秒级的时标切换； 


（4）它们的电导率有类似于 STDP 的响应特性，可以进行基于脉冲的实时学习； 


（5）它们的结构与基于脑神经元的神经形态结构十分吻合； 


（6）忆阻器层可以 3D 堆叠。假设间距为比较接近实际的 30nm，理论上 10 个忆阻器层的叠加可以提供 1×10  11  个/cm  2  非易失性模拟单元的存储密度。原则上讲，这种方法可以在单块电路板上达到人脑的神经元和突触密度，包括学习能力  [77]  。 


尽管这些新颖的忆阻器件为电子技术提供了非常有前途的替代方案，但它们与 CMOS 器件在过去几十年中所达到的成熟度相比差距还很大。但是，如果将 CMOS 的 3D 集成与忆阻器技术合在一起，就可以提供紧密靠近 PE 的高密度存储器，从而克服冯·诺依曼架构的局限性。目前已经有非常密集的架构被提出，用于将 CMOS 处理单元与半导体/纳米线/分子集成电路（CMOS/nanowire/MOLecular，CMOL）  [78]  等纳米器件的交叉阵列进行 3D 集成。 


使用新型忆阻器件来实现类脑芯片，是学术界和产业界都在积极推进的工作。在 2019 年的 IEEE 第 11 届国际存储器论坛（IEEE 11th International Memory Workshop，IMW）上，日本松下的研究人员发布了一种电阻式模拟神经形态器件（Resistive Analog Neuromorphic Device，RAND）的存储器  [79]  ，并嵌入逻辑电路中。该存储器主要是 RRAM，由上电极、下电极及夹在其间的 Ta  2  O  5  和 TaO  x  层形成。当在该器件的上电极和下电极之间施加电压时，Ta  2  O  5  层中会形成导电细丝。细丝的尺寸根据电压而变化，这使得可以以模拟方式改变读取电流值。利用这种模拟功能，他们的目标是通过模块集成实现深度学习功能，并形成一个类脑计算机。 


日本 DENSO 和美国加利福尼亚大学圣塔芭芭拉分校（UCSB）展示的类脑芯片  [80]  具有 240 万个面积为 0.04 μm  2  的 Al  2  O  3  / TiO  2  -  x 忆阻器。忆阻器无源集成在 CMOS 晶圆的顶部，集成到 48×48 交叉开关电路中。芯片上总共有 1032 个交叉开关，并通过执行矢量矩阵乘法及模拟域中的忆阻交叉电路来实现各种神经网络。 


除 RRAM 之外，其他类型的新型器件如 PCM、FeRAM 等也已用于神经形态计算，但是至今还没有阵列级的实验展示。 


目前采用的 SNN 训练方法还包括无监督 SNN 训练，这种方法主要基于脉冲时序依赖可塑性（STDP）学习规则  [81]  。在 SNN 中，可以使用依赖脉冲时序的可塑性作为基本算法，目的是在整块芯片上实现 STDP。这种学习特性可以在忆阻器件中得到很好的实现  [82]  。 


通过将 PCM 放置在前后两级神经元电路之间，并改变两个神经元的脉冲施加时间，可以将器件的电导率改变为类似于 STDP 的响应特性  [83]  。脉冲训练提供了利用现实世界的感官数据中所包含的丰富的时间信息。这使得 SNN 可以比当前非脉冲深度学习系统更自然地解决任务，如视觉手势识别或语音识别。当处理动态信息（如视频序列）时，深度学习系统使用在恒定的周期性时间（在图像识别情况下为摄影时间）采样的静态图像序列执行计算，这些计算非常密集。但是，使用 SNN 会有利得多，在 SNN 中，计算会以连续的时间方式进行驱动，并且仅通过检测特定时空相关的脉冲来驱动。 


#### 自旋电子器件实现


近年来，已经出现了可以模拟神经元和突触功能的自旋电子器件。可以使用相同的材料将自旋电子器件设计为不同的特性，包括不易失性、可塑性及振荡和随机行为，这允许创建一系列模仿生物突触和神经元关键特征的组件。自旋电子器件还可以通过自旋电流、微波信号、电磁波和绝缘的磁纹理在较远处传送信息。这些功能意味着，基于自旋电子器件的类脑芯片可能比传统方法更紧凑和节能。自旋电子器件不但可以做成人工突触，也可以做成神经元。 


磁隧道结（MTJ）就是一种自旋电子器件，它组合了其他技术无法比拟的多个功能，包括非易失性、出色的读写耐久力、高速电压操作和高可扩展性等。MTJ 的工作原理如图 5.5 所示，它由两个铁磁层（图中灰色）组成，两个铁磁层之间由绝缘层（图中蓝色）隔开，其中一层的磁化强度固定，另一层的磁化强度并行（低电阻）或反并行（高电阻），称为自由层。标注「1」和「0」代表每个值的配置。MTJ 器件中的多个电阻状态可以通过在其自由层中合并畴壁来实现。畴壁将两个相反极化的磁畴分开。可以通过使用自旋转移力矩（Spin Transfer Torque，STT）现象移动该畴壁来调节器件电导，该现象通过从自旋极化的刺激电流转移自旋力矩来改变自由层中并行和反并行畴的相对比例。 


目前，已经有芯片代工厂把这种器件纳入到 CMOS 工艺流程，主要为了开发非易失性存储器 STT-MRAM。使用自旋电子器件增强神经形态计算的一种方法是将 MTJ 嵌入 CMOS 电路里，使高速非易失性存储模块非常靠近 PE（见图 5.5 右图，M1～M6 表示金属层）。MTJ 存储单元最近已用于存储神经网络的突触权重，称为关联存储器。类似于大脑中的突触，神经网络中的突触权重通常是实数值而不是二进制值，因此需要许多二进制 MTJ 来存储单个突触权重，这需要大面积和读写能量。 


![img](https://pic3.zhimg.com/v2-c41df047c42d0807c2fbd769d40baffb.webp)

磁性器件可以通过将模拟信息存储在磁性纹理中来做成忆阻器件  [84]  。例如，陈怡然等人提出了一种自旋电子忆阻器  [85]  ，基于自旋阀中磁畴壁的位移，根据畴壁位置产生较低或较高的电阻状态。自旋电子忆阻器具有很多优势，如可以将学习和记忆结合在一起，使其可能成为使用人工突触进行神经形态计算的独特的基本模块。已经有人将这样的人工突触组成了简单的神经网络，并应用于模式分类  [86]  。 


也有研究人员研究了反铁磁体/铁磁体异质结构中自旋轨道转矩切换的动力学，展示了该材料系统形成人工神经元和突触的异步 SNN 的能力。由单个电流脉冲或一系列脉冲驱动的磁性开关根据脉冲宽度（1 ns～1 s）、幅度、数量和脉冲间的间隔进行编码。它可以再现突触（STDP）和神经元（LTF）的主要功能。实验结果为基于自旋电子学的神经形态硬件打开了一条途径  [87]  。 


实现大量神经元的高度互连是一个很有挑战的问题，而自旋电子器件带来了新的机会。自旋电子系统由多层系统组成，这些系统自然可以在 3 个方向上堆叠。可以设想利用畴壁实现垂直和水平通信的三维自旋电子神经形态系统。通过光波或自旋力矩纳米振荡器发出的微波信号可以实现通信，但是可能需要通过外部电路进行放大才能实现高扇出。 


自旋电子忆阻器最大的问题之一是可扩展性，即如何在减小器件尺寸的情况下保持模拟性能。为了解决这个问题，需要找到能够在纳米级器件中容纳更多磁畴的工程材料。另外，与其他存储技术相比，MTJ 的一个缺点是电阻变化小，难以快速读取。尽管到目前为止，人们已经在该方向上进行了单个器件级别的演示，但是对于使用自旋器件的神经形态系统的阵列级别实现，仍然存在许多问题需要解决。 


备案号:YX01jbkWgwB9w7Lle

