## 42.小结
新兴的以存储器为中心的芯片技术有望解决当今系统中的带宽瓶颈问题，并提高运行速度和降低功耗。存内计算技术背后的想法是使存储器更接近处理单元，以加快系统速度。 


一些新型器件表现出了适用于新型 AI 架构或计算范式的独特特性，如逻辑器件的非易失性、可重构性、高计算密度等。存内计算中逻辑计算和内存合并的结构（包括数字和模拟）代表了一种前景光明的 AI 芯片架构方法。 


图 7.11 为逻辑电路（处理单元）与存储器之间距离的发展趋势。在没有使用存内计算之前，只能依靠尽量把存储器靠近处理单元来缩短它们之间的距离。而存内计算把逻辑电路直接嵌入存储器中，这样就把它们之间的距离降到了 10nm 以下。 


![img](https://pic1.zhimg.com/v2-da6f6f762d4782e37a8e8b9c158192e7.webp)

将计算功能从单独的 CPU 推入内存为系统架构师和程序员带来了新的挑战。为了使存内计算被带有各种工作负载的 AI 系统采用，而又不给大多数程序员带来沉重负担，必须解决许多挑战。这些挑战包括以下几个方面。 


（1）如果仍然以现有存储器为基本器件，如何对现有的存储器件只作最小程度的改变。一般来说，说服现有的 DRAM 或 SRAM 生产商对已有电路和工艺作出太大变动是非常困难的。 


（2）如何能够轻松地对存内计算系统编程，做出良好的编程模型，还有库、编译器和工具支持  [132]  。 


（3）如何设计利用存内计算的系统和系统软件（如存内计算逻辑代码的运行时调度、数据映射等）。 


（4）如何为存内计算设计高性能数据结构，其性能要优于多核计算机上的并发数据结构。 


（5）如何克服器件本身的不确定性和不一致性（如对工艺、温度和电压的依赖性），以及存内计算所需要的模拟计算特征（如噪声问题），解决封装和热约束问题等。如果使用新型 NVM，需要配合适当且灵活的接口设计。 


所有这些挑战都需要用跨层设计的思路来解决，在应用层、架构层、电路层和器件层等都需要结合在一起考虑和设计。 


从以处理器为中心，转到以存储器为中心的芯片技术，将从根本上解决数据移动问题。实现这种模式转变的研究对于未来 AI 芯片的创新和发展，都将非常有用。存内计算不仅是提高效率、降低成本和时延的一种手段，它还是一个设计具有全新能力的 AI 系统的机会。 


对于新型 NVM 来说，在电路级操纵存储单元，以便使用存储单元本身来执行逻辑操作，是一个新的发展方向。最近的许多工作表明，NVM 单元可用于执行完整的布尔逻辑运算系列，类似于可以在 DRAM 单元中执行的此类运算。忆阻器在逻辑运算中已经可以达到 8 个值以上（而布尔逻辑为 0 和 1 的二值运算），因此利用忆阻器的新功能，超越布尔运算的多值逻辑有可能会得到实现（详见本书 16.3 节）。 


尽管已经有不少研究人员在努力探索使用存内计算来运行 DNN 的方法，但其中大多数仅针对小型 DNN（通常是 2 层或 3 层的感知器），解决数据集上数字分类的简单问题。运行 DNN 网络的性能、功耗、预测精度等还未得到明确的实验结果。不同的工作负载、处于不同存储层级的逻辑计算，以及启动软硬件协同设计的能力，都对存内计算有很大影响。 


一些研究成果表明，在 DNN 的层数较少，但是每一层的网络较大的情况下，使用存内计算可能不会导致分类精度下降。另外，存内计算并不是在所有场合都能发挥优势，只有在需要处理和存储大量数据组的情况下才有意义。 


虽然基于硅材料的 CMOS 工艺仍将是今后较长一段时间的主流，但是二维材料的研发工作不但在学术界，而且在产业界也已经开始，基于二维材料的可打印、可弯曲的 RRAM 的最初原型也已出现。在未来，可能会有更多基于二维材料的存内计算范式展示出优异特性。 


存内计算除了在构建用于深度学习的理想存储器件的材料、器件和工艺集成方面需继续完善之外，另一个重要的研究方向是实现大型 AI 系统。这种 AI 系统可以将存内计算的本来优势转化为实际应用中的切实改进。存内计算的一些芯片原型展示了在能效和面积等方面的优势，但是它们在实际计算系统所需的异质架构中，受到了系统规模和集成度的限制。最近的研发工作已开始解决这些限制问题，正在开发复杂和可编程的体系结构，并将软件库集成到高级应用设计框架中。由此看来，基于存内计算技术的大型 AI 系统的出现，已经为时不远。 


备案号:YX01jbkWgwB9w7Lle

