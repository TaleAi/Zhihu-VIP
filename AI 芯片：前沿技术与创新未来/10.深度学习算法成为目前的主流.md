## 10.深度学习算法成为目前的主流
芯片计算能力的巨大提升及大数据的出现，使得神经网络中具有更高复杂性的「深度学习」（即深度神经网络，DNN）又一次得到人们的广泛关注。辛顿等人 2006 年提出了「深度信念网络」这个概念，有力推动了深度学习算法的应用。与其他传统机器学习方法相比，深度学习有着明显的优势。 


图 2.1 为传统机器学习算法与大型 DNN 在图像、语音识别领域的精度比较。深度学习比传统方法更具吸引力，除了算法本身的优势，主要还是由于数据量和芯片计算能力的不断提高，这导致了近年来深度学习技术的「寒武纪大爆发」。 


深度学习的实质是构建具有很多层的神经网络模型（保证模型的深度）和使用大量的训练数据，让机器去学习重要的特征，最终使分类或预测达到很高的准确性。深度学习基于感知器的模型，模仿人脑的机制和神经元的信号处理模式，可以让计算机自行分析数据，找出特征值。 


![img](https://pic1.zhimg.com/v2-d62844db68e12063bee1ad2ec16b0628.webp)

为了加速深度学习计算，更多地开发硬件是很自然的想法。半导体制造商开发出了一些 AI 芯片（深度学习加速协处理器）来处理深度学习。深度学习有各种各样的模型。例如，目前谷歌数据中心普遍使用以下 3 种 DNN。 


（1）多层感知器（Multilayer Perceptron，MLP）。MLP 的每个后续层是一组非线性函数，它们来自先前层所有输出（全连接）的权重和。 


（2）卷积神经网络（CNN）。在 CNN 中，每个后续层是一组非线性函数，它们来自先前层空间附近输出子集的权重和。权重在空间上复用。 


（3）循环神经网络（Recurrent Neural Network，RNN）。RNN 的每个后续层是权重、输出和前一状态的非线性函数的集合。最受欢迎的 RNN 是长短时记忆（Long Short-Term Memory，LSTM），其妙处在于能够自主决定忘记哪些状态及将哪些状态传递到下一层。权重在时间上复用。 


与传统人工神经网络相比，深度学习架构的一个优点是深度学习技术可以从原始数据中学习隐藏特征。每层根据前一层的输出训练一组特征，组成特征分层结构。将前一层特征重新组合的最内层，可以识别更复杂的特征。例如，在人脸识别模型的场景中，作为像素矢量的人头像的原始图像数据被馈送到其输入层中的模型。然后，每个隐藏层可以从前一层的输出中学习更多抽象特征。例如，第一层（隐藏层）识别线条和边缘；第二层识别面部，如鼻子、眼睛等；第三层组合所有先前的特征，从而生成一个人脸图像。 


DNN 成功的原因之一是它能够在连续的非线性层上学习更高级别的特征表示。近年来，构建更深层网络这方面的硬件和学习技术的进展，进一步提高了分类性能。ImageNet 挑战赛体现了更深层网络的趋势，该赛事在不同年度出现的最先进的方法已经从 8 层（AlexNet）发展到 19 层（VGGNet），进而发展到 152 层（ResNet）及 101 层（ResNext），如图 2.2 所示。然而，向更深层网络的发展极大地增加了前馈推理的时延和功耗。例如，在 Titan X GPU 上将 VGGNet 与 AlexNet 进行比较的实验显示，前者的运行时间和功耗增加了 20 倍，但误差率降低了约 4%。DNN 模型的规模正在呈指数级增长，基本上每年增加 10 倍（见图 2.3）。近年出现的几个大型网络模型都是针对自然语言处理，如 Megatron 是一种 48 层并行 Transformer 模型，它的参数数量达 83 亿个，约为 ResNet50 的 325 倍。Open AI 在 2020 年 6 月发布了 GPT-3，这是迄今为止训练的最大模型，具有 1750 亿个参数。 


早期的 DNN 模型，如 AlexNet 和 VGGNet，现在被认为太大型且过度参数化。后来提出的技术，在使用更深但更窄的网络结构来限制 DNN 大小（如 GoogleNet 和 ResNet）的同时，追求更高的准确性。这个任务在继续进行，其关键是大幅减少计算量及降低存储成本，特别是乘积累加和权重的数量。后来又出现了卷积核（又称过滤器，Filter）分解之类的技术，在构建针对移动设备的紧凑型 DNN（如 SqueezeNet 和 MobileNet）场景中流行。这种演变导致了更多样化的 DNN，其形状和大小各不相同。 


![img](https://pic4.zhimg.com/v2-45f487ba7a5d863b22b5e527c9ae31dc.webp)

![img](https://pic4.zhimg.com/v2-d2a9be582dbe65a3e01ec16bbc729fce.webp)

然而，现在所提出的对深度学习的改进大都是基于经验评估，仍然没有具体的理论分析基础来回答为什么深度技术优于传统神经网络。而且，关于隐藏层的数量，DNN 和传统神经网络之间并没有明确的边界。通常，具有两个或更多个隐藏层，且具备最新训练算法的神经网络就可以被认为是深度学习模型。但是，只具有一个隐藏层的 RNN 也被归为深度学习，因为它们在隐藏层的单元上具有循环功能，这可以等效为 DNN。 


#### 深度学习的优势与不足


深度学习最近在各种应用领域中取得了巨大的成功。然而，深度学习的高精度是以对计算和内存的高要求为代价的，尤其训练深度学习模型非常耗时，而且计算量大，这是因为需要在多个时间段内迭代地修改数百万个参数。高精度和高资源消耗是深度学习的特征。 


深度学习技术发展到今天，人们已经在最初定义的 DNN，即加很多隐藏层的神经网络基础上，开发了很多改进技术，使深度学习在资源消耗及性能方面都有了很大改进。这些改进有的是在算法上作些改动，如剪枝、压缩、二值或多值逻辑、稀疏网络等；有的是网络架构上的改动，如图神经网络（Graph Neural Network，GNN  [3]  ，不进行张量运算而改用图形的节点和边来计算）、胶囊网络（Capsule Network）  [4]  、深度森林（Deep Forest  [5]  ，基于决策树方法，超参数少，显示出无须反向传播即可构建深度模型的可能性）等。 


深度学习算法虽然已被大量实现为芯片并得到应用，但是很多人认为这种算法不可能一直延续下去，一定会有更好的算法来取代它。 


确实，DNN 是一种统计方法，本质上是不精确的，它需要大数据（即加标记的大型数据集）的支撑，而这是许多用户缺乏的。DNN 也比较脆弱，并不是一种十分稳固的结构：模式匹配在数据集不完整的时候，会返回十分奇怪的结果；而在数据集损坏时，则会返回误导的结果。因此，DNN 的分类精度在很大程度上取决于数据集的质量和大小。另一个主要问题是数据不平衡的情况：某个类别的数据在训练数据中可能只有很少的代表，如信用卡欺诈检测中真正的申请通常远远超过欺诈的申请，这样的不平衡就会给分类精度带来问题。 


DNN 模型最本质的问题是没有与生物大脑的学习模式相匹配：大脑神经元的激活过程是不是存在「反向传播」，还是一个有争议的问题。生物大脑和 DNN 存在明显的物理差异。因此，从这个意义上说，深度学习算法并没有真正「仿脑」，也不是真正意义上的「学习」，而只是一个数学模型。 


从 AI 芯片的研究方向来看，有不少研究人员主要致力于如何最大限度地提高数学运算的性能，同时满足商用硬件（目前都是冯·诺依曼架构）的局限性。这样的芯片设计，已经与深度学习模型本身没有太多关系了。 


#### 监督学习与无监督学习


AI 算法最初的应用之一是模式识别，也就是如何从采样数据中找到模式的问题，它有助于理解数据甚至产生新知识。为了实现这个目标，研究人员提出了一些学习算法来执行自动数据处理。如果算法被设计为与环境互动，在环境中执行某些操作，使一些累积「奖励」得到最大，则这种方式称为强化学习（Reinforcement Learning）。强化学习算法根据输出结果（决策）的优劣来训练自己，通过大量经验训练优化后的算法将能够给出较好的预测。然而，如果我们忽略累积奖励，仅仅考虑是否提供标签，那么学习算法可以分为 3 类：监督学习、无监督学习和半监督学习。 


（1）监督学习是基于观察数据的有限子集（称为训练集）来学习关于数据所有可能轨迹的函数。此类训练集还与附加信息相关联以指示其目标值。学习算法定义了一个函数，该函数将训练数据映射到目标值。如果目标值的数量是有限的，则这种问题被定义为分类问题；如果目标值的数量是无限的，则此问题被定义为回归问题。目前已经有许多有监督的机器学习算法，如决策树、朴素贝叶斯、随机森林、支持向量机（Support Vector Machine，SVM）等。神经网络是一种以生物神经系统为模型的机器学习方法，其中 CNN、RNN 及 LSTM 等都属于监督学习。 


（2）无监督学习是一种用于从没有标记的数据集中进行推理的学习算法，其目标基本上是在数据中找到结构或群集。一种常见的方法是聚类技术，用于进行数据分析以从一组数据中找到隐藏的模式。自组织映射（Self-Organizing Map，SOM）、自动编码器（Auto-Encoder，AE）、受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）等都属于无监督学习。无监督学习的一个主要挑战是如何定义两个特征或输入数据之间的相似性；另一个挑战是不同的算法可能导致不同的结果，这需要专家再来分析它们。 


（3）半监督学习与监督学习具有相同的目标。如果既有一些未知的模式也有一些已知的模式，我们通常将前者称为未标记数据，后者称为标记数据。当设计者获得的标记数据很受限时，就使用半监督学习。例如，群集边界可以用未标记数据来定义，而群集则用少量标记数据来标记。强化学习就是一种半监督学习。 


现在所设计并已投入应用的深度学习加速器，主要使用监督学习。图 2.4 为监督学习过程，包括训练和推理两个阶段：在设计阶段进行训练，在部署阶段进行推理。 


![img](https://pic3.zhimg.com/v2-9577db14e8fb1b56f74ac7934d5a142a.webp)

训练过程（又称学习过程）是基于训练数据生成模型，确定网络中权重（和偏差）的值，一般需要较长时间，所以是「离线」完成；但在有些特殊应用中也会「在线」进行。在训练过程中需要调整神经网络权重以使损失函数最小，可通过反向传播来执行训练以更新每层中的权重。矩阵乘法和卷积在反向传播中保持不变，主要区别在于它们分别对转置后的权重矩阵和旋转的卷积核执行运算。 


推理过程（又称评估过程）是使用未知的、模型此前没有测试过的数据来评估模型的准确性，训练所确定的权重就可用来计算，为给定的输入进行分类或得出预测，一般「在线」进行，以达到实时需求。该模型假设训练样本的分布与推理例子的分布相同。这要求训练样本必须足以代表推理数据。推理过程中只有正向传播而没有反向传播。 


这种训练和推理相当于人类大脑学习和判断的过程。通常，需要花很长时间来学习（即训练），而学会之后进行判断（即推理）的时间，只需要一刹那就行了。 


图 2.5 为图像分类的一个例子。预定义的一组图像类别先经过训练，推理平台通过深度学习算法给出不同的可信度分数。在经过推理得出的目标图像中，狗的概率最大。早在 2012 年就有过这种图像分类的大型试验，训练集包含 100 多万张图片，每张图片都加有类别标签（共有 1000 个种类）。当时，该试验使用了大量 GPU 芯片来完成。 


![img](https://pic4.zhimg.com/v2-fe79b5d786dbf0e000e592405aa7b33d.webp)

强化学习算法也可以用于训练权重。在给定的环境下，网络可以输出智能体应该采取的动作，使预期奖励最大。但这种奖励需要通过一系列的动作才可获得。 


#### AI 芯片用于云端与边缘侧


深度学习的应用开发可分成云端与边缘侧两大部分。云端指的是数据中心或超级计算机，具有强大的计算能力，利用海量的数据和庞大而复杂的 DNN 进行模型训练，也可以进行推理。边缘侧指的是数据中心外的设备，如自动驾驶汽车、机器人、智能手机、无人机或物联网（Internet of Things，IoT）设备，它们用训练好的模型进行推理。 


随着云计算技术的成熟，大量数据可以放到云端来计算，从而降低边缘侧的数据负载并减少计算能力需求。这样就形成了两种不同要求的 AI 芯片，一种用于云端，具有最大的计算能力和最高的性能，主要对深度学习算法模型进行训练，有时也进行推理。训练通常需要大数据，由于需要很多次权重更新迭代，因此需要大量的计算资源。在许多情况下，训练 DNN 模型仍需要几小时到几天，因此通常在云端执行。另一种 AI 芯片则用于数量庞大的边缘侧设备（含各种终端），它们的计算性能有限，主要使用从云端传来的训练好的模型和数据进行推理。 


训练和推理的要求不一样。训练要求高精度、高吞吐量（高性能），推理则不需要很高的精度。边缘侧设备不但受到计算能力和存储器成本的限制，而且对功耗要求非常高，这是因为边缘侧的很多终端是使用电池的。图 2.6 显示了不同 AI 芯片的耗电情况。从图 2.6 中可以看出，目前 AI 芯片（不论是用于云端还是边缘侧的 AI 芯片）的功耗还是很高的，具有内置 DSP 和片上存储器的 FPGA 更节能，但它们通常更昂贵。 


![img](https://pic1.zhimg.com/v2-d1bf73f28a6fa5005c5d3edea2d2b401.webp)

一般来说，衡量一个 AI 芯片数字计算效率的指标是每单位功率或每单位芯片面积每秒执行的操作数，常用单位为 TOPS/W 或 TOPS/mm  2  ，这里 TOPS（Tera Operations per Second）表示每秒万亿次操作，有时也使用 TFLOPS（Tera Floating-point Operations per Second），指每秒万亿次浮点操作。由于一个 AI 系统往往是由很多个 AI 芯片组成的，因此在设计一个 AI 系统时，除了考虑 AI 芯片自身的功耗，还需要考虑 AI 芯片之间通信所产生的功耗。 


GPU 支持多 TFLOPS 的吞吐量和大内存访问，但能耗很高，因此 GPU 适合大型 CNN。从性能角度来说，FPGA 和 ASIC 基本在一个等级上，GPU 要比它们高一个数量级，而 CPU 则比 FPGA 和 ASIC 低一个数量级。然而，由于各家公司的激烈竞争，若干年之后，AI 芯片的性能和功耗可能会出现另外一种景象。 


**1.云端 AI 芯片**


对于在云端用于训练和推理的 AI 芯片来说，最主要的指标是 AI 芯片的吞吐量。要达到尽可能高的吞吐量，最大的任务就是将大量的 AI 处理单元（PE）放置到单块芯片中。因此，云端 AI 芯片一般都使用最先进的半导体芯片工艺（如 7nm）来制造，芯片中集成的晶体管数量也不断创纪录，而使用的芯片面积约为 800 mm  2  ，几乎达到了目前芯片制造能力的极限。 


然而，提高吞吐量带来的更严重的问题是功率密度和热量耗散问题。功率密度增加带来的暗硅效应问题（见第 1 章），已经成为云端 AI 芯片继续发展最主要的瓶颈。为了降低高密度晶体管集成造成的高温，云端 AI 芯片也必须采用复杂的散热方法，如谷歌的 TPUv3 就采用了液体散热。 


因此，由于暗硅效应的影响，AI 处理单元的数量不能一直增加下去。目前，在一些主要的云端 AI 芯片设计中，已经把一大部分受暗硅效应影响的芯片面积腾出来，即不放置 AI 处理单元，而放置一些功耗相对低得多的电路，从而减弱暗硅效应的影响。图 2.7 展示了芯片上用作处理单元的逻辑电路所占面积比例随工艺进步不断减小的趋势（该图为示意图，不一定代表实际比例或实际值）。 


![img](https://pic2.zhimg.com/v2-8ddab61fd08f5cda77b0403ca645b144.webp)

从目前已投入应用的产品来看，云端 AI 芯片的吞吐量可以达到 100～500 TOPS，而功耗基本上都集中在 10～300 W 范围内，300 W 是基于 PCI 的加速卡的上限。在这个功率范围内，能效可能会因架构、精度和工作负载（训练与推理）等各种因素而异，一般都在 1 TOPS/W 以下（但目前已有几种推理解决方案和一些训练解决方案声称能效大于 1 TOPS/W）。目前的产品必须至少使用功耗为 100 W 的处理器或加速器来进行训练，低于 100 W 的都是仅用于推理。 


另外，训练需要浮点运算，因此 AI 芯片是否支持浮点运算也是区分训练和推理的标准。训练和推理所采用的芯片架构也是不一样的。 


英伟达的 GPU 曾经在云端 AI 芯片领域一枝独秀，但是后来有一些厂家和初创公司的芯片也开始被很多数据中心所采用。不过，英伟达的 GPU 也在不断提升性能。在 2017 年发布 V100 GPU（一款 Volta 架构 GPU）之后，英伟达的 GPU 被视为由 AI 部分和传统 GPU 部分组成。该公司最新推出的基于 7nm 工艺的 A100 GPU，在 AI 应用上的性能有了新的飞跃，表明在近年内其霸主地位还很难被打破。 


云计算巨头谷歌为了应对神经网络应用不断增加的算力需求，开发了自己的 ASIC——TPU。最早的 TPUv1 只用于推理，且只支持整数运算；但 TPUv2 和 TPUv3 不仅能够用于推理，也都能够用于训练。谷歌在 2017 年发布报告称，在其数据中心中，TPUv2 运行普通神经网络的速度比现代 CPU 或 GPU 快 15～30 倍，并且能效提升了 30～80 倍。最新露面的 TPUv4 的性能据称要比 TPUv3 提升 2.7 倍。特别是谷歌在关键矩阵乘法单元使用「脉动式」设计，可以让数据在处理器之间流动而不必每次都返回到存储器。 


Graphcore 在 2018 年推出了 IPU 芯片，最近又推出第二代 IPU 处理器 GC200。GC200 IPU 拥有 594 亿个晶体管，采用台积电 7nm 工艺制造，是目前世界上包含晶体管数量最多的 AI 芯片。每个 IPU 具有 1472 个强大的处理器内核，可运行近 9000 个独立的并行程序线程。它使用了该公司独特的「处理器内嵌存储器」（In-Processor-Memory）技术，而不是存内计算，吞吐量达到 250 TFLOPS。 


TPUv3、A100 GPU 和 IPU 芯片的吞吐量分布在 100～320 TFLOPS 范围。虽然都用于云端 AI 计算，但是这些芯片的架构和电路设计截然不同。本书第 4 章将会介绍一些比较著名的云端 AI 芯片，其中包括初创公司 Cerebras 的晶圆级「大芯片」。 


除了上述几家公司外，近年来其他公司也已研发出不少云端 AI 芯片，其中包括寒武纪思元 270、阿里巴巴含光 800、华为昇腾 910 等。 


随着训练数据集和神经网络规模的增加，单个深度学习加速器不再能够支持大型 DNN 的训练，不可避免地需要部署许多个加速器（组成群集）来训练 DNN。在这种情况下，需要着重优化加速器之间的通信方式，减少总的通信流量，以提高系统性能和能效。 


**2.边缘侧 AI 芯片**


在这几年兴起的自研 AI 芯片热潮中，有的公司开发的芯片用于云端，但更多的是用于边缘侧。这里也包含了 AI 专用 IP 核的开发，这些核可以被嵌入 SoC 里面。例如，苹果智能手机里最大的应用处理器（Application Processor，AP）芯片就是一块带有 AI 核的 SoC。这类 SoC 的性能一般可以达到 5～10 TOPS。 


不只自动驾驶汽车芯片，降低功耗、减小芯片面积是目前几乎所有 AI 芯片需要解决的重要课题，特别在边缘侧。例如，对于 AI 芯片需求极大的自动驾驶汽车来说，英伟达 Xavier 的性能可达 30 TOPS，但价格昂贵且功耗高达 30 W。而理想情况下自动驾驶汽车中的 AI 芯片功耗应该为 1～2 W 或更低。因此，这种类型的芯片正成为人们开发新型 AI 芯片的竞争焦点之一，汽车制造商们也正在期待着这样的低功耗、高性能芯片出现。 


对于智能物联网（AI Internet of Things，AIoT）来说，在保持芯片一定性能（如 1 TOPS 左右）、高精度的情况下，功耗达到 100mW 以下是基本要求。但是，某些需要几年甚至 10 年才能换一次电池的应用则需要 AI 芯片能够具备「自供电」（见本书第 14 章）及「永远在线」的功能，这就需要 AI 芯片的功耗非常低，甚至达到微瓦（μW）范围。目前，已经有不少研究人员在研发这类芯片。 


在降低功耗和提高性能方面，AI 芯片的算法和架构还大有潜力可挖。本书第 3 章将着重介绍量化、压缩、二值和三值神经网络等技术，以及 AI 芯片电路实现上出现的一些新技术；本书后续几章也将介绍很多新的计算范式和半导体器件等，其中包括用新型 NVM 实现存内计算的 AI 芯片。这种芯片的研发从 2018 年以来已经取得很大进展，很有可能在不久的未来实现产业化并广泛用作边缘侧 AI 芯片。研发这些新方法和新器件的主要目的，就是为了降低芯片功耗及进一步提高性能。 


另外一个需要考虑的方面是时延问题。与离线训练不同，无论是在自动驾驶汽车还是互联网应用中，推理期间的快速响应都至关重要。许多应用都希望在传感器附近进行 DNN 推理处理，如估测商店中的等待时间或预测交通模式，都期望从图像传感器（而不是云端的视频）中提取有意义的信息并直接进行处理。这样可以降低通信成本、减少时延。 


即使在云端进行训练或推理，时延也是最重要的考虑因素之一，但传统的 GPU 设计并不关心时延。谷歌的 TPU 在这方面作了很大改进。为了保证低时延，设计人员简化了硬件并省略了一些会使现代处理器忙碌及需要更多功率的常见功能。 


从长远来看，云端和边缘侧的 AI 计算所要实现的目标是不一样的，如图 2.8 所示。它们各自需要的 AI 芯片特性有很大的不同，将会走上不同的演进路径。基于云端的强大运算能力，AI 芯片最终可能实现自学习，从而超越人类智能。云端的 AI 可以完成一些「大事」，如发现人类未知的科学知识、解决社会问题等；而边缘侧的 AI 计算主要作为人类助手，取代人类所做的工作，并逐渐起到代替人类感官的作用。目前的 AI 只是取代了人类一些重复性的工作，但是未来将会取代人类创造性的工作。目前边缘侧的 AI 计算主要完成推理，但是在未来，将会把训练和推理交织在一起完成。 


![img](https://pic2.zhimg.com/v2-e2d341ca66670c9a526489583f9e12f2.webp)

**把 AI 计算从云端迁移到边缘侧**


目前在 AI 应用中，DNN 模型不一定在边缘侧进行充分训练或推理，而是通过以云端与边缘侧设备协调的方式工作。有一些号称具有 AI 功能的边缘侧设备（如大部分可穿戴设备），不但训练在云端完成，推理也在云端完成，边缘侧设备仅是嵌入一个收发器和简单的处理器而已。但是，不管边缘侧设备是否具备推理功能，都需要有先进的联网解决方案，以便在不同的边缘节点之间有效地共享计算结果和数据。 


对于 5G 网络来说，超高可靠低时延通信（Ultra-Reliable and Low Latency Communication，URLLC）功能已被定义用于要求低时延和高可靠性的关键任务应用场景。因此，将 5G URLLC 功能与边缘侧 AI 计算相集成，将会很有希望提供高可靠性、低时延服务。此外，5G 将采用软件定义的网络和网络功能虚拟化等先进技术，这将实现对网络资源的灵活控制，以支持计算密集型 AI 应用跨不同边缘侧节点的按需互联。因此，5G 和 AI 将在很多应用中密不可分。 


然而，不久的未来，绝大部分边缘侧设备都会带有推理功能。而较长远的趋势是 DNN 模型训练和推理全部都由边缘侧设备完成（可能借助边缘服务器或个人云的帮助），不再需要使用 5G 或其他无线通信网络。这就需要大大提高边缘侧 AI 芯片的性能，以便进一步做到「实时」运算，不需要把私人数据传到云端并在云端远程训练，而在本地解决所有计算（见图 2.8）。 


**为什么要在边缘侧部署 AI**


尽管 5G 移动通信的 URLLC 可以达到低时延（1 ms）传输，但是对于自动驾驶汽车、无人机导航和机器人技术等应用，依赖云端远程数据传输的时延和安全风险还是太高，因此需要本地处理，以尽可能提高安全性和可靠性，减小决策失误的风险，确保安全。例如，自动驾驶场景中的 AI 芯片如果能够自己对周边情况进行实时计算和分析，就不用通过移动通信网络把大量数据再传输到云端了。 


图 2.9 展示了不同的应用对于带宽和时延的要求。例如对自动驾驶来说，它的实时性要求是非常高的。对边缘侧终端设备来说，实时性、安全和隐私是非常重要的。对于推理过程，边缘侧 AI 计算具有更小的时延和更低的通信依赖性，而且在保护用户隐私方面具有特别优势。另外，边缘侧设备的市场规模远大于云端数据中心，在经济回报方面有很强的吸引力。目前，用于制定决策的一些数据可以在本地（边缘侧）处理，而计算量大的只能通过（云端）数据中心处理，但两边实施的比例将可能会发生很大变化。云端的技术正在渗透到边缘侧。从长远来看，随着边缘侧芯片计算能力的大幅提高，传统大型数据中心会被大量关闭。 


![img](https://pic1.zhimg.com/v2-686077a9eb6186be5dc75e12330a3967.webp)

从长远来看，AI 芯片将大量出现在各种形式的「虚拟分身」里面。一个虚拟分身包含一个相关数据的集合（数据库）和基于人工智能的分析能力，以数字形式复制某个人，且一个虚拟分身只对应和代表某一个人。未来，每个人都将可能随时拥有一个虚拟分身，这个分身可以连接到个人云或企业云（见图 2.10 及图 2.11）。 


![img](https://pic3.zhimg.com/v2-0eef757c0be8a3612178f61bea402a39.webp)

![img](https://pic4.zhimg.com/v2-908cc30cfc0dc6aced85392fcc89bc44.webp)

虚拟分身会学习这个人与数字世界交流时产生的所有数据，范围从计算机、网站、可穿戴设备到居家环境中的传感器（如摄像头、智能音箱、空调恒温器、移动通信基站）等。随着计算机算法的进步、更多个人数据的收集，虚拟分身将会越来越精确。虚拟分身对这个人的了解，将胜过其任何亲人或朋友。虚拟分身可以代替这个人聊天、阅读、学习、上网、作决策。另外，通过组合各种传感器和 AI 芯片，虚拟分身的知觉会达到甚至超过人的五感。可以说，将高存储容量、高计算能力、「超知觉」赋予虚拟分身，将是边缘侧 AI 芯片的第一个终极应用。 


虚拟分身将拥有超级计算机的处理能力，存储容量可以根据需要调整，并在开始阶段以智能手机等终端设备形式出现。因此，智能手机的处理性能必须至少提高到当前性能的 100 倍，或者从长期来看，需要提高到 1000 倍。基于新 AI 算法的新 AI 芯片将可以提供极强的处理能力。然而，这种处理能力极强的 AI 芯片及相应的虚拟分身，可能直到 2025～2030 年才会被大众市场采用。但无论如何，把 AI 计算从云端迁移到边缘侧的一天总会到来。 


**提高边缘侧 AI 计算能力的几个思路**


要做到不把数据放到云端（或少放到云端）去训练，而在边缘侧本地解决，需要边缘侧 AI 芯片具有强大的能力，不但需要高能效（单位为 TOPS/W）、高面积效率（单位为 TOPS/mm  2  ）、低时延，还需要高吞吐量，这可以称得上是一种「片上数据中心」了。这一领域正是可以发挥研究人员聪明才智的领域，给研究人员留出了极大的创新空间。一般来说，下面几个方向值得研究人员考虑和研究。 


（1）采用新的 AI 模型和算法来替代现在最常用的监督学习 DNN 算法。例如采用无监督学习算法，就不需要对数据加标签进行训练；或者采用神经形态计算、仿生计算、自然计算等。Graphcore 则认为目前的先训练后推理的方法并不合理，应该让位于可以学习并在部署之后能够持续进化的一种「学习系统」。 


（2）研究和发现新的大脑运作机制。例如，以顺序方式学习任务的能力对于人工智能的发展至关重要，但神经网络不具备此功能。而所谓的「灾难性遗忘」（Catastrophic Forgetting）是连接模型的必然特征：一旦训练去做新任务，网络会忘记先前的任务。然而，新的研究表明这种限制有可能被克服，训练网络会记住完成老任务的经验。对于边缘侧 AI 芯片来说，就无须对那些已长期不存在的样例再次训练，可以大大提高计算性能。 


（3）对现有的深度学习算法，在提高性能和降低功耗方面作进一步改进，如采用二值权重、修枝的办法，将其变成稀疏网络。权重和激活值的数值精度，在训练处现已从原来的 32 位降到 16 位或 8 位，在推理处采用 8 位或 4 位，大大降低了计算量和功耗。但这还不够，很有可能在不久的将来降到 1～2 位。为了更好地优化 AI 芯片设计，有的方法采用硬件感知（Hardware-Aware）的参数优化技术，或是神经架构搜索，在进行推理之前，甚至在训练模型之前，先预测和理解深度学习模型的硬件性能和功耗，并优化模型的准确性，达到算法、模型和芯片三者协调设计。 


（4）在进一步提高并行计算的效率上挖掘潜力。神经网络本质上是大规模的并行计算，而适合于并行计算的计算机架构已经有了几十年的研究积累，曾经有不少研究人员提出过很好的思路。现在，英国 AI 芯片初创公司 Graphcore 就使用了 20 世纪 80 年代哈佛大学教授提出的「整体同步并行计算」模型，这种模型可以实现并行计算所需的软、硬件的并行桥接，避免大量处理器之间通信的拥塞，是冯·诺依曼模型之外的一种另类选择。 


（5）采用「去中心化」的分布式深度神经网络架构。例如，谷歌提出的联邦学习（Federated Learning，FL）方法，让分布式的各个边缘侧设备来帮助「训练」，而不是集中在数据中心训练，从而获得更好的模型训练和推理性能。然而，「去中心化」方法需要解决通信开销、异质设备的互操作性和资源分配等问题。 


（6）在芯片实现上采用新的计算范式（如模拟计算、随机计算、存内计算、储备池计算等），这将带动芯片架构和电路设计的重大变革。例如，「谷歌大脑」项目组从原有硬件架构入手，设计了一种新的 CPU 架构来减轻 CPU 原有的「垃圾收集」冗余任务，从而提高了性能；如果采用模拟计算来实现神经网络，将可以使功耗降低几个数量级；而利用新型存储器（RRAM 等）来实现存内计算，已经展露出非常诱人的前景。 


（7）在半导体工艺方面进行创新，以期在保持低功耗的同时，把运算速度提高几个数量级。例如在 2020 年，半导体工艺已从 7nm 进步到 5nm，台积电已全面使用极紫外（Extreme Ultra-Violet，EUV）光刻技术来批量生产 5nm 芯片，使面积相同的芯片可以容纳更多处理单元，从而把计算能力提高 1～2 个数量级。如果在逻辑芯片或存储器芯片上实现三维堆叠，即单片 3D 芯片，将为 AI 芯片开辟一条全新的道路。 


（8）从最底层基础研究做起，开发新的半导体器件，这可能为 AI 芯片带来根本性的突破。例如，日本东北大学国际集成电子研发中心最近开发出一种使用磁隧道结（Magnetic Tunnel Junction，MTJ）器件的 AI 芯片，该器件是应用自旋电子学的存储器件。通过将适用于存储器和学习处理的 MTJ 与适用于判断处理的 CMOS 相结合，这种芯片实现了更高的性能和更低的功耗。美国得克萨斯大学圣安东尼奥分校（UTSA）也已开发了基于 MTJ 的深度学习加速器芯片。 


（9）把「电」改为「光」，即把神经网络架构改为用光子传输，不用电路，而是组成光路。由于目前硅光芯片工艺已经相当成熟，用硅光芯片制成 AI 芯片已经不再遥远，这将把目前 AI 芯片的性能和能效都提高几个数量级。 


总的来说，目前如果要仅靠边缘侧的 AI 芯片来完成数据处理和模型训练，实现如图像识别等应用，还有很大的挑战。例如，视频涉及大量数据，需要占用很大的带宽，要做成低成本的图像识别芯片有很大难度。但我们需要努力去克服这些困难。对语音来说，实现边缘侧 AI 计算的问题还不算很大，语音识别使我们能够与智能手机等电子设备无缝进行互动。虽然目前苹果 Siri 和亚马逊 Alexa 语音服务等应用的大多数处理都在云端，但现在市场上新出现的很多智能音箱产品其实已经在边缘侧设备上实现了语音识别功能，减少了时延和对网络连接的依赖，并提高了隐私保护和安全性。 


然而，一些 AI 不会走向边缘侧，如聊天机器人、会话 AI、监控欺诈的系统和网络安全系统。这些系统将从基于规则的系统发展为基于深度学习的 AI 系统，这实际上会增加云端的推理工作量。 


备案号:YX01jbkWgwB9w7Lle

