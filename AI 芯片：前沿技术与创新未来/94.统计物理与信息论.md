## 94.统计物理与信息论
统计物理是以玻尔兹曼等人提出的，以最大混乱度（也就是熵）理论为基础，通过配分函数，将有大量组成成分（通常为分子）系统中的微观物理状态（如动能、位能）与宏观物理量统计规律（如压力、体积、温度、热力学函数、状态方程等）联系起来的科学。这些微观物理状态包括伊辛模型中磁性物质系统的总磁矩、相变温度和相变指数等。 


统计物理中有许多理论影响着其他学科，如信息论中的信息熵，化学中的化学反应、耗散结构，和发展中的经济物理学等。从这些学科当中都可看到统计物理在研究线性与非线性等复杂系统中的成果。 


基于统计物理的方法已应用于传统物理领域之外的几个领域。例如，来自无序系统的统计物理学的分析和计算技术已经应用于计算机科学和统计学的各个领域，包括推理、机器学习和优化。 


由于现在计算资源变得很普及且很强大，这就促进了这些方法向邻近学科领域的传播。一个例子是在 20 世纪中期马可夫链蒙特卡洛方法的有效使用。另一个重要例子是为分析具有多个自由度的无序系统而开发的分析方法。大量的粒子加在一起会有无法计算的自由度量，无法计算出它们全体的总运动效果，只能用统计方法计算。统计物理方法已经在数学类比的基础上应用于各种问题，这些类比的对应关系从数学角度来看非常清楚。 


人工神经网络的奠基人之一约翰·霍普菲尔德（John Hopfield）当时就指出了这种类比，引发了物理学界对神经网络和类似系统的极大兴趣。这些类比包括动态神经网络和无序磁性模型材料的简单模型的概念相似性，所谓的吸引子神经网络（如 Little-Hop 场模型）中的平衡和动态效应，各种各样机器学习场景的分析，包括前馈神经网络的监督训练和结构化数据集的无监督分析等。 


这些分析和类比涵盖了广泛的概念和领域，其中包括信息论、随机微分方程相变的数学分析、平均场理论、蒙特卡洛模拟、变分微积分、重整化群以及各种其他分析和计算方法  [296]  。 


信息论是由贝尔实验室的工程师克劳德·香农（Claude Shannon）在 20 世纪 40 年代后期开创的，它的目的是对一个信号的信息量进行量化，而信息熵则代表了通信系统中接收的每条消息中包含的信息的平均量。 


香农的理论始于以下观察：确定数据中的信息量并不总是像数那些所包含的字母、数字或其他符号的数目那样简单。明显的例子是重复的消息。例如，在公共交通系统（如在地铁中）进行广播通知，人们肯定不会因为重复说同一件事两次而获得两倍的信息。然而，这种冗余虽然不会增加新信息，但能够检查收到的信息的一致性，因此它在保持消息的精确度方面非常有用。 


有些数据包含了很多冗余。像 BBGBBBBGBGBB 这样的字符串包含的信息要多于具有重复模式（如 BGBGBGBGBGBG）的字符串中的信息，后者具有一些可以简化的冗余，而前者则没有。某种意义上，前者的数据比后者的数据包含更多的信息，因为它的可预测性较差，因此很难以重复模式进行描述。这表明，不可预测性是衡量信息的很好的指标。而信息熵，即一条消息中混乱的程度，可以很好地衡量其信息内容。低熵的消息可以比高熵的消息压缩更多，因为它们的信息含量较低。 


信息论被广泛地应用于机器学习领域。从基于相对熵概念的差异来看，互信息测量和数据比较刺激了机器学习数据分析的新方法。例如，来自非扩展统计物理学的 Tsallis 熵  [297]  可用于改进使用决策树的学习  [298]  和基于内核的学习  [299]  。最近的方法将 Tsallis 熵与强化学习联系起来  [300]  。特别是波尔兹曼-吉布斯（Boltzmann-Gibbs）统计是自适应过程中必不可少的工具。GAN 可以用信息论来进行理论推导。图神经网络也可以使用信息最大化理论来推导。另外，在布尔逻辑电路的设计中，也可使用信息论作为设计基于熵的拟合函数的基础。 


2015 年，以色列希伯来大学的计算机科学家和神经科学家纳夫塔利·蒂什比（Naftali Tishby）等人提出用信息瓶颈理论作为分析深度学习的框架  [301]  。信息瓶颈这个概念是蒂什比等人在 1999 年提出的，表示网络摆脱多余细节的嘈杂输入数据，仅保留与一般概念最相关的数据，就好像通过一个瓶颈压缩信息一样。 


任何 DNN 都可以通过网络层与输入和输出变量之间的互信息来量化。使用这种表示，可以计算 DNN 提取相关信息方面的最佳信息理论极限，并获得有限的样例泛化边界。在极限情况下，网络已尽可能地压缩了输入，而不会牺牲准确预测其标签的能力。他们认为，一个深度学习网络的最佳架构、层数及每一层的特征与连接都和信息瓶颈折中的分叉点有关，即输入层相对于输出层的相关压缩。这种新见解为深度学习算法带来了新的最优化界限。 


美国硅谷的初创公司 Perceive 把信息论应用到他们的 AI 芯片上，颠覆了神经网络的算法。如前所述，深度学习 AI 芯片的计算是以大量的矩阵乘积累加运算作为核心的。但是，这家公司应用了信息论中将信号与噪声区分开的数学方法，对信息量进行量化。 


如果要识别一只猫，普通的 DNN 能够根据看到的许多猫的图片来进行归纳，因为它们至少可以发现噪声中的一些信号，但这是以经验法得到的，而不是严格的数学方法。这意味着信号会携带噪声，从而使 DNN 变得非常庞大，并容易受到对抗性例子和其他技巧的影响。然而，如果根据信息论的数学理论，可以采取严格的数学量化方法，把在大数据中只占很少一部分的信号（如一只猫）从海量的噪声中提取出来，从而大大缩小神经网络模型的规模。 


基于信息论来设计深度学习算法，代表着一种新的神经网络处理方式。这家公司的 AI 芯片称为 ERGO（见图 16.7），也采用了全新的芯片架构，没有采用 MAC 阵列，从而使这款芯片的能效表现达到市场上同类产品的 20～100 倍，具有 55 TOPS/W 的超高能效。它在以 30 f/s 的速度运行 YOLOv3（这是一个具有 6400 万个参数的大型网络）时功耗仅为 20mW。 


![img](https://pic1.zhimg.com/v2-b049517a39ac3bc0ee099f577853c4ad.webp)

将信息科学与物理学联系起来是当今学科联合的一大趋势。例如，量子纠缠被认为是暗能量、重力和时空本身的来源。全息原理也可能与纠缠和 RBM 有关。而前面章节也已提到，量子场论可以对 RBM 和 DNN 作出解释。尤其是这几年深度学习的兴起，很多人希望找到有力的分析工具，以解开深度学习的运作之谜。统计物理已经成功地用于分析浅层的神经网络，而 DNN 和其他复杂架构中的学习机理，也逐渐被证明和解释  [302]  。 


统计物理学已经对机器学习和推理中相关现象的理解作出了重要贡献，并且仍在继续帮助人们更清楚地理解很多复杂的 AI 算法。统计物理学与 AI 算法的重要联系，已经受到 AI 领域人士的高度关注。 


备案号:YX01jbkWgwB9w7Lle

