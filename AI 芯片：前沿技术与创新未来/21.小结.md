## 21.小结
本章分别介绍了在架构、算法和电路方面提高深度学习 AI 芯片性能和能效的几种比较典型的方法。实际上，架构、算法和电路应该是协同设计的，即硬件架构需要贴近算法的思路，而算法更应该具备「硬件友好」的特点，这样才能组合成最佳的 AI 芯片解决方案。 


通常，对于任何 DNN 硬件实现，在设计空间中有许多潜在的解决方案需要探索。设计一种可应用于每种 DNN 的通用硬件架构并非易事，尤其是在考虑对计算资源和存储器带宽的限制时。 


对于 AI 芯片来说，相比网络压缩，可能模型加速更为重要，如何给 MAC 提速，值得关注。有不少研究人员从研究数学的角度出发，将矩阵乘法和加法变得更简洁和快速。例如，把乘法和加法转为逻辑和位移运算就是一种很好的思路。然而，网络的压缩率和加速率是高度相关的，较小的模型通常会导致训练和推理的计算速度更快。 


现在的大多数 DNN 都由卷积层和全连接层组成。本章探讨的设计和优化方法大部分集中在卷积层，这是因为全连接层的计算要求明显低于卷积层。为了提高性能和最大化资源利用率，许多技术通过对不同的输入特征图进行分组组合来确定批量，然后在全连接层中一起处理它们。对于图像分类任务，浮点运算主要在最初的少量卷积层中进行，通常在开始时就有很大的卷积计算量。因此，网络的压缩和加速应针对不同的应用，集中在不同类型的层上。 


有些方法看上去效果很好，但是需要大量训练，这种时间成本也不应该被忽视。值得注意的是，鉴于存储器访问的能耗成本远高于 MAC 操作的能耗成本，需要优先考虑如何把数据移动减到最少。如果通过扩展片上存储器容量来解决，将大幅增加芯片面积，从而增加芯片成本。但这个问题可能在未来几年会得到解决，因为届时大容量三维堆叠存储器芯片将趋于成熟。另外，未来深度学习加速芯片可能将会和多核 CPU 处理器集成在同一块芯片里。 


现在，尽管深度学习的架构取得了巨大成功，但仍有许多领域需要进一步研究。这些方案在实际应用中还存在很大的挑战，不管在能效、性能、成本，还是在实时性方面，都还存在不少问题。好消息是新型半导体器件已经出现，将给深度学习的芯片实现开辟一条全新的道路。可以预计，随着新型存储器和相关产业的日益成熟，今后深度学习 AI 芯片架构的研究重心，将逐步转移到可以更加节省电能的模拟计算、存内计算、可逆计算等领域，这将在本书后面几章分别讨论。 


模拟计算、存内计算等新方法的提出，可能会从根本上改变 AI 芯片的架构；而类脑芯片、自然计算芯片、量子启发芯片等的出现，又为 AI 芯片增添了全新的品种和应用。新型元器件和电路技术的出现，能够探索极大规模的集成系统来模拟复杂的生物神经元结构，从而可以有效地用于这些新型 AI 芯片，并将使现有的深度学习得到高出几个数量级的加速。 


神经网络算法的进步不会停止，它将一代一代不断更新，将会出现更复杂的神经算法。随着脑科学和认知科学的不断发展，神经网络算法也将引入更复杂的认知功能，将会使 AI 算法的智能程度越来越接近人类。同时，新的算法也将带来网络架构和芯片架构的变革，要实现新架构又必须具备相适应的电路和元器件。算法进步带来的好处不亚于半导体工艺的进步带来的好处，甚至更大，但是它又很难像摩尔定律那样较准确地加以预测。这些算法的突破本质上是非确定性的，每次发生，都会让它们的市场地位重新洗牌。 


本章讨论了深度学习算法和芯片设计优化的一些方法。这些方法都是靠专家凭经验完成的，在某个方面、某个指标上可能比其他人的成果更优越，但不能说一定达到了最优。第 9 章将介绍如何使用自然算法和仿生算法，挖掘搜索空间，找出更进一步的优化方案，确定最佳超参数（如最佳网络层数等），在专家经验的基础上，用人工智能来设计人工智能，向真正的最优解决方案靠近。 


为边缘侧的深度学习 AI 芯片设计轻量级、实时且节能的体系结构是下一步的重要研究方向。当前，几乎所有的深度学习加速器架构都专注于加速器内部 DNN 推理的优化，较少有人考虑训练用的加速器的优化。随着训练数据集和神经网络的规模扩大，单个加速器不再能够支持大型 DNN 的训练，不可避免地需要部署多个加速器来训练 DNN。在训练用的深度学习 AI 芯片方面，英伟达的 GPU 仍然占了主导地位，但也有少数几家初创公司研发了针对训练的 AI 芯片，从而已经对英伟达构成了一定挑战。 


第 4 章将介绍目前生产深度学习 AI 芯片的几家厂商的开发进展情况，以及几种比较典型的 AI 芯片，还将介绍近年来由学术界和初创公司推出的先进的 AI 芯片（包括云端用 AI 芯片和边缘侧 AI 芯片）。 


备案号:YX01jbkWgwB9w7Lle

