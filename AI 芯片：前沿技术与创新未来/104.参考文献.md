## 104.参考文献
[1] Hartnett K. To Build Truly Intelligent Machines, Teach Them Cause and Effect[J]. Quanta Magazine, 2018. 


[2] Chua, Leon O. How We Predicted the Memristor[J]. Nature Electronics, 2018, 1(5): 322-322. 


[3] Gori M, Monfardini G, Scarselli F. A New Model for Learning in Graph Domains[C]// Proceedings of the International Joint Conference on Neural Networks, IEEE Press, 2009, 2: 729-734. 


[4] Sabour S, Frosst N, Hinton G E. Dynamic Routing Between Capsules[C]// 31st Conference on Neural Information Processing Systems, 2017. 


[5] Zhou Z H, Feng J. Deep Forest[J]. National Science Review, 2019. 


[6] Hoehne B. Data Center and Optical Network Innovation: Enabling the 5G Ecosystem[C]// Keysight World, 2019. 


[7] Sze V, Chen Y H, Yang T J, et al. Efficient Processing of Deep Neural Networks: A Tutorial and Survey[J]. Proceedings of the IEEE, 2017, 105(12): 2295-2329. 


[8] Goodfellow I, Bengio Y, Courville A. Deep Learning[M]. The MIT Press, 2016. 


[9] Cong J, Xiao B. Minimizing Computation in Convolutional Neural Networks[M]// Artificial Neural Networks and Machine Learning – ICANN 2014. Springer International Publishing, 2014. 


[10] Sze V. Designing Hardware for Machine Learning: The Important Role Played by Circuit Designers[J]. IEEE Solid-State Circuits Magazine, 2017, 9(4): 46-54. 


[11] He K, Sun J. Convolutional Neural Networks at Constrained Time Cost[C]// IEEE. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015: 5353-5360. 


[12] Li F, Liu B. Ternary Weight Networks[Z/OL]. (2016-11-19). arXiv: 1605.04711. 


[13] Antonio Polino, Razvan Pascanu, et al. Model Compression via Distillation and Quantization[C]// International Conference on Learning Representations(ICLR), 2018. 


[14] Russakovsky O, Deng J, Su H, et al. ImageNet Large Scale Visual Recognition Challenge[J]. International Journal of Computer Vision, 2015, 115(3): 211-252. 


[15] Choi J, Venkataramani S, et al. Accurate and Efficient 2-bit Quantized Neural Networks[C]// Proceedings of the 2nd SysML Conference, Palo Alto, CA, USA, 2019. 


[16] Nagel M, Baalen M V, et al. Data-Free Quantization through Weight Equalization and Bias Correction[Z/ OL]. (2019-06-11). arXiv: 1906.04721v1. 


[17] Montero R M, et al. Template-based Posit Multiplication for Training and Inferring in Neural Networks [Z/ OL]. (2019-07-09). arXiv: 1907.04091v1. 


[18] Horowitz M. Computing』s Energy Problem (and What We Can Do About It)[C]// IEEE International SolidState Circuits Conference(ISSCC), 2014: 10-14. 


[19] Jacob B, Kligys S, Chen B, et al. Quantization and Training of Neural Networks for Efficient IntegerArithmetic-only Inference[C]// The IEEE Conference on Computer Vision and Pattern Recognition, 2018. 


[20] Louizos C, Reisser M, Blankevoort M, et al. Relaxed Quantization for Discretized Neural Networks[C]// International Conference on Learning Representations (ICLR), 2019. 


[21] Moons B, Goetschalckx K, et al. Minimum Energy Quantized Neural Networks[Z/OL]. (2017-11-23). arXiv: 1711.00215v2. 


[22] Gopalakrishnan K. DNN Training and Inference with Hyper-Scaled Precision[C]// ICML, 2019. 


[23] Han S, Maon H, Dally W J. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding [Z/OL]. (2015-10-01). arXiv: 1510.00149. 


[24] Abu-Mostafa Y S. The Vapnik-Chervonenkis Dimension: Information Versus Complexity in Learning[J]. Neural Computation, 1989, 1(3): 312-317. 


[25] Schmidhuber J. Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability[J]. Neural Networks, 1997, 10(5): 857-873. 


[26] Lin Z, Courbariaux M, Memisevic R, et al. Neural Networks with Few Multiplications [Z/OL]. (2015-10-11). arXiv: 1510.03009. 


[27] Kim M, Smaragdis P. Bitwise Neural Networks [Z/OL]. (2016-01-04). arXiv: 1601.0607. 


[28] Dettmers T. 8-Bit Approximations for Parallelism in Deep Learning [Z/OL]. (2015-11-14). arXiv: 1511. 4561. 


[29] Gupta S, Agrawal A, Gopalakrishnan K, et al. Deep Learning with Limited Numerical Precision[C]// Proceedings of the International Conference on Machine Learning, Lille, France, 2015: 1737-1746. 


[30] Zhou S, Wu Y, Ni Z, et al. Dorefa-net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Z/OL]. (2016-06-20). arXiv: 1606.06160. 


[31] Chen, Y, Li J, Xiao H, et al. Dual Path Networks[J]. Advances in Neural Information Processing Systems, 2017: 4467-4475. 


[32] Mahmud M, Kaiser M S, Hussain A, et al. Applications of Deep Learning and Reinforcement Learning to Biological Data[J]. IEEE Transactions on Neural Networks and Learning Systems, 2018, 29: 2063-2079. 


[33] Zhu C, Han S, Mao H, et al. Trained Ternary Quantization[Z/OL]. (2016-12-04). arXiv: 1612.01064. 


[34] Umuroglu Y, Conficconi D, Rasnayake L, et al. Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing [Z/OL]. (2019-06-11). arXiv: 1901.00370v2. 


[35] Malossi A C I, Schaffner M, Molnos A, et al. The Transprecision Computing Paradigm: Concept, Design, and Applications, Design[C]// Automation & Test in Europe Conference & Exhibition (DATE). 2018. 


[36] Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the Inception Architecture for Computer Vision[C]// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 2016, 2818-2826. 


[37] Iandola F N, Han S, Moskewicz M W, et al. Squeezenet: Alexnet-level Accuracy with 50x Fewer Parameters and < 0.5 mb Model Size [Z/OL]. (2016-11-04). arXiv: 1602.07360. 


[38] Mao H Z, Han S, et al. Exploring the Granularity of Sparsity in Convolutional Neural Networks [Z/OL].(2016-06-05). arXiv: 1705.08922. 


[39] Park D H, Wesley T, Beaumont J, et al. A 7.3 M Output Non-Zeros/J, 11.7 M Output Non-Zeros/GB Reconfigurable Sparse Matrix-Matrix Multiplication Accelerator[J]. IEEE Journal of Solid-State Circuits, 2020, 55(4). 


[40] Knowles S. Intelligence Processors[Z]. Graphcore, CWTEC, 2017. 


[41] Graphcore. IPU Programmer』s Guide[Z]. 2020. 


[42] Y Chen, Luo T, Liu S, et al. DaDianNao: A Neural Network Supercomputer[J]. IEEE Transactions on Computers, 2016, 66(1): 73-88. 


[43] Wang S, Zhou D, Han X, et al. Chain-NN: An Energy-Efficient 1D Chain Architecture for Accelerating Deep Convolutional Neural Networks[C]// Proceedings of the IEEE/ACM Proceedings Design, Automation and Test in Europe (DATE), 2017: 1032-1037. 


[44] Farabet C, Martini B, Akselrod P, et al. Hardware Accelerated Convolutional Neural Networks for Synthetic Vision Systems[C]// IEEE International Symposium on Circuits and Systems, 2010: 257-260. 


[45] Sim J, Park J S, Kim M, et al. 14.6A 1.42TOPS/W Deep Convolutional Neural Network Recognition Processor for Intelligent IoT Systems[C]// International Solid-State Circuits Conference, San Francisco, California, 2016: 264-265. 


[46] Hinton G E, Salakhutdinov R R. Reducing the Dimensionality of Data with Neural Networks[J]. Science, 2006, 313: 504-507. 


[47] Zhang C, Li P, Sun G, et al. Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks[C]// International Symposium on Field-programmable Gate Arrays, Monterey, California, 2015: 161-170. 


[48] Chen Y H, Krishna T, Emer J S, et al. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks[J]. IEEE Joural of Solid-State Circuits, 2017, 52(1): 127-138. 


[49] Bankman D, Yang L, Moons B, et al. An Always-On 3.8 μJ/86% CIFAR-10 Mixed Signal Binary CNN Processor With All Memory on Chip in 28-nm CMOS[J]. IEEE Journal of Solid-State Circuits, 2019: 54(1). 


[50] Nurvitadhi E, et al. Can FPGAs Beat GPUs in Accelerating Next Generation Deep Neural Networks[C]// Proc. ACM/SIGDA Int. Symp. Field-Program. Gate Arrays, 2017: 5-14. 


[51] Zidan M A, Strachan J P, Lu W D. The Future of Electronics Based on Memristive Systems[J]. Nature Electronics, 2018, 1(1): 22-29. 


[52] Andrew G H, et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications [Z/ OL]. (2017-04-17). arXiv: 1704.04861v1. 


[53] Andrew G H, et al. Searching for MobileNetV3 [Z/OL]. (2019-06-12). arXiv: 1905.02244v3. 


[54] Teerapittayanon S, McDanel B, Kung H T. BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks[C]// 23rd International Conference on Pattern Recognition, 2016. 


[55] Hinton G, et al. Distilling the Knowledge in a Neural Network[C]// in Proceeding of NIPS Workshop, 2014. 


[56] Yang T J, et al. NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications[C]. Computer Vision-ECCV, 2018. 


[57] Chen B D, Medini T, et al. SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for LargeScale Deep Learning Systems [Z/OL]. (2020-03-01). arXiv: 1903.03129v2. 


[58] Tan M X, Le Q V. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks[Z/OL].(2019-11-23). arXiv: 1905.11946. 


[59] Wu Y N, Emer J S, Sze V. Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs[C]// 2019 IEEE/ACM International Conference on Computer-Aided Design, Westminster, CO, USA, 2019. 


[60] Parashar A, et al. Timeloop: A Systematic Approach to DNN Accelerator Evaluation[C]// IEEE International Symposium on Performance Analysis of Systems and Software, Madison, WI, USA, 2019. 


[61] Chen Y H, Yang T J, Emer J, et al. Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices [Z/OL]. (2018-07-10). arXiv: 1807.07928. 


[62] Chen Y H, Krishna T, Emer J S, et al. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks[J]. IEEE Journal of Solid-State Circuits, 2017, 52(1). 


[63] Han S, Liu X Y, et al. EIE: Efficient Inference Engine on Compressed Deep Neural Network [Z/OL]. (2016-05-03). arXiv: 1602.01528v2. 


[64] Shin D, Lee J, Lee J, et al. DNPU: An Energy-Efficient Deep-Learning Processor with Heterogeneous Multi-Core Architecture[J]. IEEE Micro, 2018, 38(5): 85-93. 


[65] Moons B, Uytterhoeven R, Dehaene W, et al. ENVISION: A 0.26-to-10 TOPS/W Subword-Parallel Dynamic-Voltage-Accuracy-Frequency-Scalable Convolutional Neural Network Processor in 28nm FDSOI[C]// IEEE ISSCC, 2017. 


[66] Ueyoshi K, Ando K, Hirose K, et al. QUEST: A 7.49 TOPS Multi-Purpose Log-Quantized DNN Inference Engine Stacked on 96MB 3D SRAM Using Inductive-Coupling Technology in 40nm CMOS[C]// IEEE ISSCC, 2018. 


[67] Lee J, Lee J, Han D, et al. LNPU: A 25.3 TFLOPS/W Sparse Deep-Neural-Network Learning Processor with Fine-Grained Mixed Precision of FP8-FP16[C]// IEEE ISSCC, 2019. 


[68] Trader T. Cerebras Debuts AI Supercomputer-on-a-Wafer[Z]. Cerebras Systems, 2019. 


[69] Freund K. World-Record AI Chip Announced by Habana Labs[Z]. Forbes, 2019. 


[70] Cutress I. Hot Chips 31 Analysis: In-Memory Processing by UPMEM[Z]. AnandTech, 2019. 


[71] Bouvier M, et al. Spiking Neural Networks Hardware Implementations and Challenges: a survey[J]. ACM Journal on Emerging Technologies in Computing Systems, 2019, 15(2): 22. 


[72] Benjamin B V, Gao P, McQuinn E, et al. Neurogrid: A Mixed-Analog-Digital Multichip System for LargeScale Neural Simulations[C]// Proceedings of the IEEE, 2014, 102(5): 699-716. 


[73] Thorpe S, Delorme A, Rullen R V. Spike-based Strategies for Rapid Processing[J]. Neural Networks, 2001, 14: 715–725. 


[74] Huys Q, Zemel R, Natarajan R, et al. Fast Population Coding[J]. Neural Computation, 2007, 19: 404-441. 


[75] Rullen R V, Thorpe S J. Rate Coding versus Temporal Order Coding: What the Retinal Ganglion Cells Tell the Visual Cortex[J]. Neural Computation, 2001, 13: 1255-1283. 


[76] Huh D, Sejnowski T J. Gradient Descent for Spiking Neural Networks [Z/OL]. (2017-06-14). arXiv: 1706.4698. 


[77] Luis A, Mesa C, Barranco B L, Gotarredona T S. Neuromorphic Spiking Neural Networks and Their Memristor-CMOS Hardware Implementations[J]. MDPI Materials, 2019, 12: 2745. 


[78] Likharev K, Strukov D. CMOL: Devices, Circuits, and Architectures[C]. Cuniberti G, Fagas G, Richter K. Introducing Molecular Electronics. Heidelberg: Springer, 2005: 447-477. 


[79] Mikawa T. Neuromorphic Computing Based on Analog ReRAM as Low Power Consumption Solution for Edge Application[C]// International Memory Workshop, 2019. 


[80] Kataeva I, Ohtsuka S, et al. Towards the Development of Analog Neuromorphic Chip Prototype with 2.4M Integrated Memristors[C]// IEEE International Symposium on Circuits and Systems, Sapporo, Japan, 2019: 1-5. 


[81] Kuzum D, Jeyasingh R, Lee B, et al. Nanoelectronic Programmable Synapses based on Phase Change Materials for Brain-Inspired Computing[J]. Nano Letters, 2012, 12(5): 2179-2186. 


[82] Saïghi S, et al. Plasticity in Memristive Devices for Spiking Neural Networks[J]. Front Neurosci, 2015, 9(51). 


[83] Gerstner W, Ritz R, Hemmen J L. Why Spikes? Hebbian Learning and Retrieval of Time-Resolved Excitation Patterns[J]. Biological Cybernetics, 1993, 69: 503-515. 


[84] Sharad M, Augustine C, Panagopoulos G, et al. Spin-based Neuron Model with Domain-Wall Magnets as Synapse[J]. IEEE Transactions on Nanotechnology, 2012, 11(4): 843-853. 


[85] Wang X, Chen Y, Xi H, et al. Spintronic Memristor Through Spin-Torque-Induced Magnetization Motion[J]. IEEE Electron Device Letters, 2009, 30(3): 294-297. 


[86] Grollier J, Querlioz D, Camsari K Y, et al. Neuromorphic Spintronics[J]. Nature Electronics, 2020, 3: 360-370. 


[87] Kurenkov A, Duttagupta S, Zhang C, et al. Artificial Neuron and Synapse Realized in an Antiferromagnet/ Ferromagnet Heterostructure Using Dynamics of Spin–Orbit Torque Switching[J]. Advanced Materials, 2019, 31(23): 1900636. 1-1900636.7. 


[88] Bodo R, Iulia-Alexandra L, Yuhuang H, et al. Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification[J]. Frontiers in Neuroence, 2017, 11: 682. 


[89] Lee C, et al. Enabling Spike-based Backpropagation for Training Deep Neural Network Architectures [Z/ OL]. (2019-08-11). arXiv: 1903.06379v3. 


[90] Moreira O, Yousefzadeh A, et al. NeuronFlow: A Hybrid Neuromorphic – Dataflow Processor Architecture for AI Workloads[C]// IEEE International Conference on Artificial Intelligence Circuits and Systems, 2020. 


[91] DVS Introduction[Z]. Zurich: iniVation AG, 2020. 


[92] Merolla P A, et al. A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface[J]. Science, 2014, 345(6197): 668-673. 


[93] Mayberry M. Intel』s New Self-Learning Chip Promises to Accelerate Artificial Intelligence[Z]. Intel, 2017. 


[94] Davies M, Srinivasa N, Lin T H, et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning[J]. IEEE Micro, 2018: 82-99. 


[95] Benjamin B V, Gao P, Mcquinn E, et al. Neurogrid: A Mixed-Analog-Digital Multichip System for LargeScale Neural Simulations[J]. Proceedings of the IEEE, 2014, 102(5): 699-716. 


[96] Neckar A S. Braindrop: A Mixed Signal Neuromorphic Architecture with a Dynamical Systems-Based Programming Model[C]. Ph. D. Thesis, Stanford University, Stanford, CA, USA, 2018. 


[97] Neckar A, Fok S, Benjamin B V, et al. Braindrop: A Mixed-Signal Neuromorphic Architecture With a Dynamical Systems-Based Programming Model[J]. Proceedings of the IEEE, 2018, 107(1): 144-164. 


[98] Schemmel J, Briiderle D, Griibl A, et al. A Wafer-Scale Neuromorphic Hardware System for Large-Scale Neural Modeling[J]. Proceedings of the IEEE International Symposium on Circuits and Systems, Paris, France, 2010: 1947-1950. 


[99] Furber S B, Galluppi F, Temple S, et al. The SpiNNaker Project[J]. Proceedings of the IEEE, 2014, 102(5): 652-665. 


[100] Nikonov D E, Young I A. Benchmarking Delay and Energy of Neural Inference Circuits[J]. IEEE Journal on Exploratory Solid-State Computational Devices and Circuits, 2020, 5(2): 75-84. 


[101] Kravtsov K S, Fok M P, Prucnal P R, et al. Ultrafast All-Optical Implementation of a Leaky Integrate-andFire Neuron[J]. Optics Express, 2011, 19(3): 2133-2147. 


[102] Nahmias M A, Shastri B J, Tait A N, et al. A Leaky Integrate-and-Fire Laser Neuron for Ultrafast Cognitive Computing[J]. IEEE Journal of Selected Topics in Quantum Electronics, 2013, 19(5): 1-12. 


[103] Rosenbluth D, Kravtsov K, Fok M P, et al. A High Performance Photonic Pulse Processing Device[J]. Optics Express, 2009, 17(25): 22767-22772. 


[104] Fok M P, Deming H, Nahmias M, et al. Signal Feature Recognition Based on Lightwave Neuromorphic Signal Processing[J]. Optics Letters, 2011, 36: 19-21. 


[105] Toomey E, Segall K, Berggren K K. A Power Efficient Artificial Neuron Using Superconducting Nanowires [Z/OL]. (2019-06-29). arXiv: 1907.00263. 


[106] Cowan G E R, Melville R C, Tsividis Y P. A VLSI Analog Computer/Digital Computer Accelerator[J]. IEEE Journal of Solid-State Circuits, 2005, 41(1): 42-53. 


[107] Guo N, Huang Y, Mai T, et al. Energy-Efficient Hybrid Analog/Digital Approximate Computation in Continuous Time[J]. IEEE Journal of Solid State Circuits, 2016, 51(7): 1-11. 


[108] Wan Z. Scalable and Analog Neuromorphic Computing Systems[D]. Los Angeles: University of California, 2020. 


[109] Li W, Xu P, Zhao Y, et al. TIMELY: Pushing Data Movements and Interfaces in PIM Accelerators Towards Local and in Time Domain [Z/OL]. (2020-05-03). arXiv: 2005.01206v1. 


[110] Joshi V, Gallo M L, et al. Accurate Deep Neural Network Inference Using Computational Phase-Change Memory[J]. Nature Communications, 2020, 11: 2473. 


[111] Haensch, Wilfried, Gokmen, et al. The Next Generation of Deep Learning Hardware: Analog Computing[J]. Proceedings of the IEEE, 2019, 107(1). 


[112] Gong N, Idé T, Kim S, et al. Signal and Noise Extraction from Analog Memory Elements for Neuromorphic Computing[J]. Nature Communications, 2018, 9(1): 2102. 


[113] Gokmen T, Vlasov Y. Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices[J]. Frontiers in Neuroscience, 2016, 10(51). 


[114] Ando T, Narayanan V. Machine Learning for Analog Accelerators[Z]. IBM』s Blog, 2018. 


[115] Kim S, Gokmen T, Lee H M, et al. Analog CMOS-based Resistive Processing Unit for Deep Neural Network Training[C]// 2017 IEEE 60th International Midwest Symposium on Circuits and Systems(MWSCAS). IEEE, 2017. 


[116] Rasch M J, et al. Efficient ConvNets for Analog Arrays [Z/OL]. (2018-07-03). arXiv: 1807.01356v1. 


[117] Skrzyniarz S, Fick L, Shah J, et al. 24.3 A 36.8 2b-TOPS/W Self-calibrating GPS Accelerator Implemented Using Analog Calculation in 65nm LP CMOS[C]// 2016 IEEE International Solid-State Circuits Conference (ISSCC). IEEE, 2016: 420-422. 


[118] Miyashita D, Yamaki R, Hashiyoshi K, et al. An LDPC Decoder With Time-Domain Analog and Digital Mixed-Signal Processing[J]. IEEE Journal of Solid-State Circuits, 2013, 49(1): 73-83. 


[119] Hsin-Yu T, Stefano A, Pritish N, et al. Recent Progress in Analog Memory-Based Accelerators for Deep Learning[J]. Journal of Physics D Applied Physics, 2018, 51. 


[120] Han S, Liu X, Mao H, et al. EIE: Efficient Inference Engine on Compressed Deep Neural Network[J]. Acm Sigarch Computer Architecture News, 2016, 44(3): 243-254. 


[121] Mutlu O, Ghose S, Luna J G, et al. INVITED: Enabling Practical Processing In and Near Memory for Data-Intensive Computing[C]// 2019 56th ACM/IEEE Design Automation Conference (DAC). IEEE, 2019: 1-4. 


[122] Zhang J, Wang Z, Verma N. In-Memory Computation of a Machine-Learning Classifier in a Standard 6T SRAM Array[J]. IEEE Journal of Solid-State Circuits, 2017, 52(4): 1-10. 


[123] Chang K K. Understanding and Improving the Latency of DRAM-Based Memory Systems[D]. Pittsburgh: Carnegie Mellon University, 2017. 


[124] Seshadri V, Mowry T C, Lee D, et al. Ambit: In-Memory Accelerator for Bulk Bitwise Operations Using Commodity DRAM Technology[C]// IEEE/ACM International Symposium. ACM, 2017. 


[125] Donghyuk, Lee, Saugata, et al. Simultaneous Multi-Layer Access: Improving 3D-Stacked Memory Bandwidth at Low Cost[J]. ACM Transactions on Architecture and Code Optimization (TACO), 2016. 


[126] Abu L M, Abunahla H, Mohammad B, et al. An Efficient Heterogeneous Memristive XNOR for InMemory Computing[J]. Circuits and Systems I: Regular Papers, IEEE Transactions on, 2017: 1-11. 


[127] Zha Y, Nowak E, et al. Liquid Silicon: A Nonvolatile Fully Programmable Processing-In-Memory Processor with Monolithically Integrated ReRAM for Big Data/Machine Learning Applications[C]// Symposium on VLSI Circuits, 2019. 


[128] Yan B, Yang Q, Chen W H, et al. RRAM-based Spiking Nonvolatile Computing-In-Memory Processing Engine with Precision-Configurable In Situ Nonlinear Activation[C]// 2019 Symposium on VLSI Technology. IEEE, 2019. 


[129] Feng X W, Li Y D, et al. First Demonstration of a Fully-Printed MoS2 RRAM on Flexible Substrate with Ultra-Low Switching Voltage and its Application as Electronic Synapse[C]// Symposium on VLSI Technology, 2019. 


[130] Chen W H, et al. A 65nm 1Mb Nonvolatile Computing-in-Memory ReRAM Macro with Sub-16ns Multiply-and Accumulate for Binary DNN AI Edge Processors[C]// IEEE International Solid-State Circuits Conference(ISSCC). IEEE, 2018. 


[131] Su F, Chen W H, Xia L, et al. A 462GOPs/J RRAM-based Nonvolatile Intelligent Processor for Energy Harvesting IoE System Featuring Nonvolatile Logics and Processing-in-memory[C]// 2017 Symposium on VLSI Technology. IEEE, 2017. 


[132] Verma N, Jia H, Valavi H, et al. In-Memory Computing: Advances and Prospects[J]. IEEE Solid-State Circuits Magazine, 2019, 11(3): 43-55. 


[133] Sidiroglou S, Misailovic S, Hoffmann H, et al. Managing Performance vs. Accuracy Trade-offs with Loop Perforation[C]// ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering. 2011: 124-134. 


[134] Shi Q, Hoffmann H, Khan O. A HW-SW Multicore Architecture to Tradeoff Program Accuracy and Resilience Overheads[J]. IEEE Computer Architecture Letters, 2014, 14(2): 1-1. 


[135] Gupta V, Mohapatra D, Raghunathan A, et al. Low-Power Digital Signal Processing Using Approximate Adders[J]. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2013, 32(1): 124-137. 


[136] Yang Z, Jain A, Liang J, et al. Approximate XOR/XNOR-based Adders for Inexact Computing[C]// 13th IEEE International Conference on Nanotechnology (IEEE-NANO), 2013. 


[137] Lu S L. Speeding Up Processing with Approximation Circuits[J]. Computer, 2004, 37(3): 67-73. 


[138] Kung J, Kim D, Mukhopadhyay S. A Power-Aware Digital Feedforward Neural Network Platform with Backpropagation Driven Approximate Synapses[C]// IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 2015: 85-90. 


[139] Sarwar S S, Venkataramani S, Raghunathan A, et al. Multiplier-less Artificial Neurons Exploiting Error Resiliency for Energy-Efficient Neural Computing[C]// Design, Automation and Test in Europe. EDA Consortium, 2016. 


[140] Gupta V, Mohapatra D, Park S P, et al. IMPACT: IMPrecise Adders for Low-power Approximate Computing[C]// Low Power Electronics & Design. IEEE, 2011: 409-414. 


[141] Kulkarni P, Gupta P, Ercegovac M. Trading Accuracy for Power with an Underdesigned Multiplier Architecture[C]// 2011 24th Internatioal Conference on VLSI Design, 2011: 346-351. 


[142] Whatmough P N, Lee S K, Lee H, et al. 14.3 A 28nm SOC with a 1.2 GHz 568 nj/prediction Sparse Deep-Neural-Network Engine with > 0.1 Timing Error Rate Tolerance for IoT Applications[C]// IEEE International Solid-State Circuits Conference (ISSCC), 2017: 242-243. 


[143] Whatmough P N, Das S, Bull D M. A Low-Power 1-GHz Razor FIR Accelerator with Time-Borrow Tracking Pipeline and Approximate Error Correction in 65nm CMOS[J]. IEEE Journal of Solid-State Circuits, 2014, 49(1): 84-94. 


[144] Koppula S, Orosa L, et al. EDEN: Enabling Energy-Efficient, High Performance Deep Neural Network Inference Using Approximate DRAM [Z/OL]. (2019-10-12). arXiv: 1910.05340v1. 


[145] He K, Gerstlauer A, Orshansky M. Controlled Timing-Error Acceptance for Low Energy IDCT Design[C]// 2011 Design, Automation & Test in Europe, 2011: 1-6. 


[146] Ranjan A, et al. Approximate Storage for Energy Efficient Spintronic Memories[C]// 52nd ACM/EDAC/ IEEE Design Automation Conference, 2015: 1-6. 


[147] Li B, Gu P, Shan Y, et al. RRAM-Based Analog Approximate Computing[J]. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2015, 34(12): 1-1. 


[148] Temam O. A Defect-tolerant Accelerator for Emerging High-Performance Applications[C]// International Symposium on Computer Architecture. IEEE, 2012. 


[149] Alaghi A, Qian W, et al. The Promise and Challenge of Stochastic Computing[J]. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37(8): 1515-1531. 


[150] Ma X L, Zhang Y P, et al. An Area and Energy Efficient Design of Domain-Wall Memory-Based Deep Convolutional Neural Networks using Stochastic Computing [Z/OL]. (2018-02-03). arXiv: 1802.01016v1. 


[151] Kim K, Lee L, Choi K. Approximate De-Randomizer for Stochastic Circuits[C]// International SoC Design Conference (ISOCC), 2015. 


[152] Alawad M, Lin M. Stochastic-Based Deep Convolutional Networks with Reconfigurable Logic Fabric[J]. IEEE Transactions on Multi Scale Computing Systems, 2016, 2(4): 242-256. 


[153] Kim K, Kim J, et al. Dynamic Energy-Accuracy Trade-off Using Stochastic Computing in Deep Neural Networks[C]// 53nd ACM/EDAC/IEEE Design Automation Conference (DAC), 2016. 


[154] Larkin D, Kinane A, Muresan V, et al. An Efficient Hardware Architecture for a Neural Network Activation Function Generator[C]// International Symposium on Neural Networks. Springer, Berlin, Heidelberg, 2006. 


[155] Cheemalavagu S，Korkmaz P，Palem K, et al. A Probabilistic CMOS Switch and Its Realization by Exploiting Noise[C]// IFIP-VLSI SoC, 2005: 452-457. 


[156] Camsari K Y, Faria R, Sutton B M, et al. Stochastic p-bits for Invertible Logic[J]. Physical Review X, 2016, 7(3). 


[157] Onizawa N, et al. In-Hardware Training Chip based on CMOS Invertible Logic for Machine Learning[J]. IEEE Transactions on Circuits and Systems I: Regular Papers, 2020, 67(5): 1541-1550. 


[158] Cai R, Ren A, et al. A Stochastic-Computing based Deep Learning Framework using Adiabatic Quantum-Flux-Parametron Superconducting Technology [Z/OL]. (2019-07-22). arXiv: 1907.09077. 


[159] Gilbert A C, Zhang Y, et al. Towards Understanding the Invertibility of Convolutional Neural Networks [Z/ OL]. (2017-05-24). arXiv: 1705.08664v1. 


[160] Daly, Erica L, Bernhard, Jennifer T. The Rapidly Tuned Analog-to-Information Converter[C]// 2013 Asilomar Conference on Signals, Systems and Computers, Pacific Grove, CA, USA, 2013: 495-499. 


[161] Frank M P. Throwing Computing into Reverse[J]. IEEE Spectrum, 2017, 54(9): 32-37. 


[162] Zhang C X, Mlynski D A. Mapping and hierarchical self-organizing neural networks for VLSI placement[J]. IEEE Transactions on Neural Networks, 1997, 8(2): 299-314. 


[163] 谭营.烟花算法引论[M]. 北京：科学出版社，2015. 


[164] Gao M, Pu J, Yang X, et al. TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory[J]. Acm Sigops Operating Systems Review, 2017, 51(2): 751-764. 


[165] Ye F. Particle Swarm Optimization-based Automatic Parameter Selection for Deep Neural Networks and Its Applications in Large-scale and High-dimensional Data[J]. Plos One, 2017, 12(12): e0188746. 


[166] Jaderberg M, Dalibard V, et al. Population Based Training of Neural Networks[Z/OL]. (2017-11-28). arXiv: 1711.09846v2. 


[167] Real E, Aggarwal A, et al. Regularized Evolution for Image Classifier Architecture Search [Z/OL]. (2019-02-16). arXiv: 1802.01548v7. 


[168] Liu H X, Simonyan K, Vinyals O, et al. Hierarchical Representations for Efficient Architecture Search [Z/ OL]. (2018-02-22). arXiv: 1711.00436v2. 


[169] Lorenzo P R, Nalepa J. Memetic Evolution of Deep Neural Networks[C]// the Genetic and Evolutionary Computation Conference. ACM, 2018: 505-512. 


[170] Liang J, Meyerson E, et al. Evolutionary Neural AutoML for Deep Learning [Z/OL]. (2019-04-09). arXiv: 1902.06827v3. 


[171] Singh P, Dwivedi P. Integration of New Evolutionary Approach with Artificial Neural Network for Solving Short Term Load Forecast Problem[J]. Applied Energy, 2018, 217: 537-549. 


[172] Kenny A, Li X. A Study on Pre-training Deep Neural Networks Using Particle Swarm Optimisation[C]// Asia-Pacific Conference on Simulated Evolution and Learning, 2017: 361-372. 


[173] Badem H, Basturk A, Caliskan A, et al. A New Efficient Training Strategy for Deep Neural Networks by Hybridization of Artificial Bee Colony and Limited–memory BFGS Optimization Algorithms[J]. Neurocomputing, 2017, 266(Nov. 29): 506-526. 


[174] Banharnsakun A. Towards Improving the Convolutional Neural Networks for Deep Learning Using the Distributed Artificial Bee Colony Method[J]. International Journal of Machine Learning and Cybernetics, 2019, 10(6): 1301-1311. 


[175] Leke C, Ndjiongue A R, Twala B, et al. A Deep Learning-Cuckoo Search Method for Missing Data Estimation in High-Dimensional Datasets[C]// International Conference in Swarm Intelligence, Springer, Cham, 2017: 561-572. 


[176] Wang B, Xue B, Zhang M J. Particle Swarm Optimisation for Evolving Deep Neural Networks for Image Classification by Evolving and Stacking Transferable Blocks [Z/OL]. (2020-03-21). arXiv: 1907.12659v2. 


[177] Palangpour P. FPGA Implementation of PSO Algorithm and Neural Networks[D]. Lola: Missouri University of Science and Technology, 2010. 


[178] Ameur B, Anis S. FPGA Implementation of Parallel Particle Swarm Optimization Algorithm and Compared with Genetic Algorithm[J]. International Journal of Advanced Computer ence & Applications, 2016, 7(8). 


[179] Fang Y, Wang Z, Gomez J, et al. A Swarm Optimization Solver Based on Ferroelectric Spiking Neural Networks[J]. Frontiers in Neuroence, 2019. 


[180] Eberhart R, Kennedy J. A New Optimizer Using Particle Swarm Theory[C]// the Sixth International Symposium on Micro Machine and Human Science, Nagoya, Japan, 1995, 1: 39-43. 


[181] Cai F X, Kumar S, et al. Harnessing Intrinsic Noise in Memristor Hopfield Neural Networks for Combinatorial Optimization [Z/OL]. (2019-04-03). arXiv: 1903.11194. 


[182] Lucas A. Ising Formulations of Many NP Problems [Z/OL]. (2014-01-24). arXiv: 1302.5843. 


[183] Someya K, Ono R, Kawahara T. Novel Ising Model Using Dimension-control for High-speed Solver for Ising Machines[C]// 2016 14th IEEE International New Circuits and Systems Conference (NEWCAS). IEEE, 2016. 


[184] Nishimori H. Statistical Physics of Spin Glasses and Information Processing: An Introduction[M]. Oxford University Press, 2001. 


[185] Yoshimura C, Hayashi M, Okuyama T, et al. FPGA-based Annealing Processor for Ising Model[C]// Fourth International Symposium on Computing & Networking. IEEE, 2017. 


[186] Yan K. Accelerated Optimization Using Coherent Ising Machines[D]. Tokyo: The University of Tokyo, 2013. 


[187] Yamamoto Y, Aihara K, Leleu T, et al. Coherent Ising Machines—Optical Neural Networks Operating at the Quantum Limit[J]. Npj Quantum Information, 2017, 3(1): 49. 


[188] Wang T S, Roychowdhury J. Oscillator-based Ising Machine[D]. Berkeley: University of California, 2017. arXiv: 1709.08102v2. 


[189] Takata K, Utsunomiya S, Yamamoto Y. Transient Time of An Ising Machine Based on Injection-locked Laser Network[J]. New Journal of Physics, 2012, 14(1): 013052. 


[190] Yamaoka M, et al. A 20k-Spin Ising Chip to Solve Combinatorial Optimization Problems With CMOS Annealing[C]// IEEE Journal of Solid-State Circuits, 2016, 51(1): 303-309. 


[191] Kazuo Yano. Artificial Intelligence as a Hope: AI for Taking on the Challenges of an Unpredictable Era[Z]. Hitachi Review, 2016, 65(6). 


[192] Goto H, Tatsumura K, Dixon A R. Combinatorial Optimization by Simulating Adiabatic Bifurcations in Nonlinear Hamiltonian Systems[J]. Science Advances, 2019, 5(4). 


[193] Pierangeli D, Marcucci G, Conti C. Large-scale Photonic Ising Machine by Spatial Light Modulation[J]. Physical Review Letters, 2019, 122(21): 213902.1-213902. 6. 


[194] Mcmahon P L, Marandi A, Haribara Y, et al. A Fully Programmable 100-spin Coherent Ising Machine with All-to-all Connections[J]. Science, 2016, 354(6312): 614-617. 


[195] Roques C C, et al. Heuristic Recurrent Algorithms for Photonic Ising Machines[Z/OL]. (2019-11-19). arXiv: 1811.02705v3. 


[196] Yang K, Chen Y F, et al. High Performance Monte Carlo Simulation of Ising Model on TPU Clusters[C]// the International Conference for High Performance Computing, Networking, Storage and Analysis, Denver, Colorado, 2019. 


[197] 韩德尔，张臣雄. 人工智能 +：AI 与 IA 如何重塑未来[M]北京：机械工业出版社，2018. 


[198] Finn C, et al. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks[Z/OL]. (2017-07-18). arXiv: 1703.03400v3. 


[199] Frans K, et al. Meta Learning Shared Hierarchies[Z]. (2017-10-26). arXiv: 1710.09767v1. 


[200] Bohnstingl T, Scherr F, Pehle C, et al. Neuromorphic Hardware Learns to Learn[J]. Frontiers in Neuroence, 2019, 13: 483. 


[201] Cox M T, Raja A. Metareasoning: Thinking about Thinking[M]. The MIT Press, 2011. 


[202] Higgins I, Matthey L, et al. β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework[C]// International Conference on Learning Representations (ICLR), 2017. 


[203] Burgess C P, Higgins I, Pal A, et al. Understanding Disentangling in β-VAE[D/OL]. (2018-04-10). arXiv: 1804.03599v1. 


[204] Chen X, Duan Y, et al. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets[D/OL]. (2016-06-16). arXiv: 1606.03657. 


[205] Kulkarni T D, Whitney W, Kohli P, et al. Deep Convolutional Inverse Graphics Network[C]// Neural Information Processing Systems(NIPS), 2015. 


[206] Shanahan M, et al. An Explicitly Relational Neural Network Architecture[Z/OL]. (2019-12-20). arXiv: 1905.10307v3. 


[207] Goodfellow I J, et al. Generative Adversarial Nets[C]// Neural Information Processing Systems (NIPS), Montreal, 2014, 2: 2672-2680. 


[208] Karras T, et al. Progressive Growing of GANs for Improved Quality Stability and Variation[Z/OL]. (2018-02-26). arXiv: 1710.10196v3. 


[209] Zhu J Y, et al. Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks[C]// IEEE International Conference on Computer Vision (ICCV), 2017: 2223-2232. 


[210] Yazdanbakhsh A, et al. FlexiGAN: An End-to-End Solution for FPGA Acceleration of Generative Adversarial Networks[C]// 26th IEEE International Symposium on Field-Programmable Custom Computing Machines (FCCM), 2018. 


[211] Kang S, Han D, et al. 7.4 GANPU: A 135TFLOPS/W Multi-DNN Training Processor for GANs with Speculative Dual-Sparsity Exploitation[C]// IEEE International Solid-State Circuits Conference (ISSCC), San Francisco, CA, USA, 2020: 140-142. 


[212] Chen F, Song L, Chen Y. ReGAN: APipelined ReRAM-based Accelerator for Generative Adversarial Networks[C]// 23rd Asia and South Pacific Design Automation Conference, Jeju, Korea, 2018. 


[213] Mao H, Song M, Li T, et al. LerGAN: AZero-free, Low Data Movement and PIM-based GAN Architecture[C]// 51st Annual IEEE/ACM International Symposium on Microarchitecture, Fukuoka, Japan, 2018: 81-669. 


[214] Jantsch A, Dutt N, Rahmani A M. Self-Awareness in Systems on Chip-A Survey[J]. IEEE Design and Test, 2017: 99. 


[215] Dutt N, Jantsch A, Sarma S. Toward Smart Embedded Systems: A Self-aware System-on-Chip (SoC) Perspective[J]. ACM Transactions on Embedded Computing Systems, 2016, 15(2): 1-27. 


[216] Chandra A, Lewis P R, Glette K, et al. Reference Architecture for Self-Aware and Self-Expressive Computing Systems[A]// Lewis P R, Platzner M, Rinner B, Torresen J, et al. Self-Aware Computing Systems: An Engineering Approach[C], 2016: 37-49. 


[217] Bouajila A, Zeppenfeld J, et al. Autonomic System on Chip Platform[M]// Organic Computing―A Paradigm Shift for Complex Systems, Springer, 2011. 


[218] Wilson D G, et al. Evolving Simple Programs for Playing Atari Games[Z/OL]. (2018-06-14). arXiv: 1806. 5695v1. 


[219] Sekanina L. Virtual Reconfigurable Circuits for Real-World Applications of Evolvable Hardware[A]// Tyrrell A M, Haddow P C, Torresen J. Evolvable Systems: From Biology to Hardware. Berlin: Springer, 2003: 186-197. 


[220] Mora J, Salvador R, Eduardo D L T. On the Scalability of Evolvable Hardware Architectures: Comparison of Systolic Array and Cartesian Genetic Programming[J]. Genetic Programming & Evolvable Machines, 2019, 20(2): 155-186. 


[221] Kim C, Kang S, Shin D, et al. A 2. 1TFLOPS/W Mobile Deep RL Accelerator with Transposable PE Array and Experience Compression[C]// 2019 IEEE International Solid-State Circuits Conference(ISSCC). IEEE, 2019. 


[222] Gomez F, Schmidhuber J, Miikkulainen R, et al. Accelerated Neural Evolution through Cooperatively Coevolved Synapses[J]. Journal of Machine Learning Research, 2008: 937-965. 


[223] Bontrager P, Lin W, Togelius J, et al. Deep Interactive Evolution[Z/OL]. (2018-01-24). arXiv: 1801.08230v1. 


[224] Fellicious C, Transfer Learning and Organic Computing for Autonomous Vehicles(2018-08-16). arXiv: 1808. 05443v1. 


[225] Harris N C. Programmable Nanophotonics for Quantum Information Processing and Artificial Intelligence[D]. Cambridge: Massachusetts Institute of Technology, 2017. 


[226] Harris N C, Carolan J, Bunandar D, et al. Linear Programmable Nanophotonic Processors[J]. Optica, 2018, 5(12). 


[227] Shen Y C, Harris N C, Skirlo S, et al. Deep Learning with Coherent Nanophotonic Circuits[Z/OL]. (2016-10-07). arXiv: 1610.02365v1. 


[228] Maass W, Natschläeger T, Markram H. Real-Time Computing without Stable States: A New Framework for Neural Computation based on Perturbations[J]. Neural Computation, 2002, 14(11): 2531-2560. 


[229] Jaeger H. The 「Echo State」 Approach to Analysing and Training Recurrent Neural Networks-with an Erratum Note[R]. Bonn: German National Research Center for Information Technology GMD Technical Report, 2001. 


[230] Steil J J. Backpropagation-Decorrelation: Online Recurrent Learning with O(N) Complexity[C]// IEEE International Joint Conference on Neural Networks. IEEE, 2004, 1: 843-848. 


[231] Goudarzi A, Lakin M R, Stefanovic D. Reservoir Computing Approach to Robust Computation Using Unreliable Nanoscale Networks[C]// International Conference on Unconventional Computation and Natural Computation. Springer, Cham, 2014. 


[232] Vandoorne K, Mechet P, Van V T, et al. Experimental Demonstration of Reservoir Computing on a Silicon Photonics Chip[J]. Nature Communications, 2014, 5. 


[233] Katumba A, Freiberger M, Laporte F, et al. Neuromorphic Computing Based on Silicon Photonics and Reservoir Computing[J]. IEEE Journal of Selected Topics in Quantum Electronics, 2018, 24(6): 1-1. 


[234] Bong K, Choi S, Kim C, et al. 14.6 A 0.62mW Ultra-low-power Convolutional-neural-network Facerecognition Processor and A CIS Integrated with Always-on Haar-like Face Detector[C]// 2017 IEEE International Solid-State Circuits Conference (ISSCC). IEEE, 2017. 


[235] Liu Q, Yildirim K S, Paweczak P, et al. Safe and Secure Wireless Power Transfer Networks: Challenges and Opportunities in RF-Based Systems[J]. IEEE Communications Magazine, 2016, 54(9): 74-79. 


[236] Shafique K, Khawaja B A, et al. Energy Harvesting Using a Low-Cost Rectenna for Internet of Things (IoT) Applications[J]. IEEE Access, 2018, 6. 


[237] Rosa R L, Zoppi G, Finocchiaro A, et al. An Over-the-distance Wireless Battery Charger based on RF Energy Harvesting[C]// 2017 14th International Conference on Synthesis, Modeling, Analysis and Simulation Methods and Applications to Circuit Design (SMACD). 2017. 


[238] Rosa R L, Trigona C, Zoppi G, et al. RF Energy Scavenger for Battery-free Wireless Sensor Nodes[C]// 2018 IEEE International Instrumentation and Measurement Technology Conference (I2MTC ). IEEE, 2018. 


[239] Guerra R, Finocchiaro A, Papotto G, et al. An RF-powered FSK/ASK Receiver for Remotely Controlled Systems[C]// 2016 IEEE Radio Frequency Integrated Circuits Symposium (RFIC). IEEE, 2016: 226-229. 


[240] Fan F R, Tian Z Q, Wang Z L. Flexible Triboelectric Generator[J]. Nano Energy, 2012, 1(2): 328-334. 


[241] Lin Z M, Chen J, Yang J. Recent Progress in Triboelectric Nanogenerators as a Renewable and Sustainable Power Source[J]. Journal of Nanomaterials, 2016. 


[242] Liu J, Gu L, Cui N, et al. Fabric-Based Triboelectric Nanogenerators[J]. AAAS Research, 2019, 2019: 1-13. 


[243] Niu S, Wang X, Yi F, et al. A Universal Self-charging System Driven by Random Biomechanical Energy for Sustainable Operation of Mobile Electronics[J]. Nature Communications, 2015, 6: 8975. 


[244] EPIC Semiconductors. Smart Dust AI Chip With Feelings[Z]. 2019. 


[245] Ylli K, Hoffmann D, et al. Human Motion Energy Harvesting for AAL Applications[J]. Journal of Physics: Conference Series, 2014, 557: 012024. 


[246] Ryckaert J, Schuddinck P, Weckx P, et al. The Complementary FET (CFET) for CMOS scaling beyond N3[C]// 2018 IEEE Symposium on VLSI Technology. IEEE, 2018. 


[247] Chau R. A Bright Future for Moore』s Law[Z]. Intel, 2020. 


[248] Aly M M S, Wu T F, Bartolo A, et al. The N3XT Approach to Energy-Efficient Abundant-Data Computing[J]. Proceedings of the IEEE, 2018, 107(1): 19-48. 


[249] Kim D, Kung J, Chai S, et al. Neurocube: A Programmable Digital Neuromorphic Architecture with HighDensity 3D Memory[C]// ACM/IEEE 43rd Annual International Symposium on Computer Architecture(ISCA). IEEE, 2016: 380-392. 


[250] Walker C. New Intel Core Processor Combines High-Performance CPU with Custom Discrete Graphics from AMD to Enable Sleeker, Thinner Devices[Z]. Intel, 2017. 


[251] Seemuth D P, Davoodi A, Morrow K. Automatic Die Placement and Flexible I/O Assignment in 2.5D IC Design[C]// International Symposium on Quality Electronic Design. IEEE, 2015. 


[252] Zheng L, Zhang Y, Bakir M S. A Silicon Interposer Platform Utilizing Microfluidic Cooling for HighPerformance Computing Systems[J]. IEEE Transactions on Components, Packaging and Manufacturing Technology, 2015, 5(10): 1379-1386. 


[253] Kannan A, et al. Enabling Interposer-based Disintegration of Multi-core Processors[C]// The International Symposium on Microarchitecture (MICRO), 2015. 


[254] Kim M M, Mehrara M, Oskin M, et al. Architectural Implications of Brick and Mortar Silicon Manufacturing[J]. Acm Sigarch Computer Architecture News, 2007, 35(2): 244. 


[255] Cianchetti M, et al. Implementing System-in-Package with Nanophotonic Interconnect[C]// Workshop on the Interaction between Nanophotonic Devices and Systems, 2010. 


[256] Demir Y, et al. Galaxy: A High-performance Energy-efficient Multi-chip Architecture Using Photonic Interconnects[C]// International Conference on Supercomputing(ICS), 2014. 


[257] Shilov A. AMD Previews EPYC 「Rome」 Processor: Up to 64 Zen 2 Cores[Z]. AnandTech, 2018. 


[258] Mahajan R, Sankman R, Patel N, et al. Embedded Multi-die Interconnect Bridge (EMIB)— A High Density, High Bandwidth Packaging Interconnect[C]// Electronic Components & Technology Conference, 2016: 557-565. 


[259] Brinda Bhowmick. Hetero Double Gate-dielectric Tunnel FET with Record High ION/ IOFF Ratio[C]// International Conference on VLSI, Communication & Instrumentation (ICVCI), 2011. 


[260] Knoch J, Appenzeller J. A Novel Concept for Field Effect Transistors—the Tunneling Carbon Nanotube FET[C]// Proceedings of the 63rd DRC, 2005, 1: 153-158. 


[261] Rasmussen F A, Thygesen K S. Computational 2D Materials Database: Electronic Structure of TransitionMetal Dichalcogenides and Oxides[J]. Journal of Physical Chemistry C, 2015, 119: 13169-13183. 


[262] Wong H S P. Metal-Oxide RRAM[J]. Proceedings of the IEEE, 2012, 100(6): 1951-1970. 


[263] Yu S, Gao B, Fang Z, et al. Stochastic Learning in Oxide Binary Synaptic Device for Neuromorphic Computing[J]. Frontiers in Neuroence, 2013, 7: 186. 


[264] Manipatruni S, Nikonov D E, Lin C C, et al. Scalable Energy-efficient Magnetoelectric Spin-orbit Logic[J]. Nature, 2019, 565(7737): 35-42. 


[265] National Institute of Standards and Technology. NIST』s Superconducting Synapse May Be Missing Piece for 「Artificial Brains」[Z]. 2018. 


[266] Gambetta J, et al. Building Logical Qubits in a Superconducting Quantum Computing System[J]. Nature Quantum Information, 2017, 3. 


[267] Bhatti S, et al. Spintronics Based Random Access Memory: A Review[J]. Materials Today, 2017, 20(9): 530-548. 


[268] Chen A, Rosing T S, Datta S, et al. A Survey on Architecture Advances Enabled by Emerging BeyondCMOS Technologies[J]. IEEE Design & Test, 2019: 46-68. 


[269] Cohen T S, Weiler M, Kicanaoglu B, et al. Gauge Equivariant Convolutional Networks and the Icosahedral CNN[Z/OL]. (2019-05-13). arXiv: 1902.04615v3. 


[270] Lee J W. Quantum Fields as Deep Learning[Z/OL]. (2017-08-18). arXiv: 1708.07408. 


[271] Wilson K G, Kogut J. The Renormalization Group and the ε Expansion[J]. Physics Reports, 1974, 12: 75-199. 


[272] Pankaj M, Schwab D J. An Exact Mapping Between the Variational Renormalization Group and Deep Learning[Z/OL]. (2014-10-14). arXiv: 1410.3831. 


[273] Efrati E, Wang Z, Kolan A, et al. Reviews of Modern Physics[J]. Real-space Renormalization in Statistical Mechanics, 2014, 86: 647. 


[274] Cai W, Shalaev V, Optical Metamaterials: Fundamentals and Applications[M]. Heidelberg: Springer Science, 2010. 


[275] Yu N, Genevet P, Kats M A, et al. Light Propagation with Phase Discontinuities: Generalized Laws of Reflection and Refraction[J]. Science, 2011, 334(6054): 333-337. 


[276] Lin X, Rivenson Y, Yardimei N T, et al. All-Optical Machine Learning Using Diffractive Deep Neural Networks[J]. Science, 2018, 361(6406): 1004-1008. 


[277] Lu L, Zeng Z, Zhu L, et al. Miniaturized Diffraction Grating Design and Processing for Deep Neural Network[J]. IEEE Photonics Technology Letters, 2019, 31(24): 1952-1955. 


[278] Manzalini A. Complex Deep Learning with Quantum Optics[J]. Quantum Reports, MDPI, 2019. 


[279] Trabelsi C, et al. Deep Complex Networks[Z/OL]. (2018-02-25). arXiv: 1705. 09792v4. 


[280] Wu H, Zhou J, Lan C, et al. Microwave Memristive-like Nonlinearity in a Dielectric Metamaterial[J]. Scientific Reports, 2014, 4: 5499. 


[281] Zhang W R, Peace K E. Revealing the Ubiquitous Effects of Quantum Entanglement—Toward a Notion of God Logic[J]. Journal of Quantum Information Science, 2013, 3. 


[282] Zhang W R. YinYang Bipolar Relativity: A Unifying Theory of Nature, Agents and Causality with Applications in Quantum Computing, Cognitive Informatics and Life Sciences[M]. Information Science Reference, 2011. 


[283] Zhang W R. The Road from Fuzzy Sets to Definable Causality and Bipolar Quantum Intelligence—To the Memory of Lotfi A. Zadeh[J]. Journal of Intelligent & Fuzzy Systems, 2019, 36(4): 3019-3032. 


[284] Statistics From Google Ngram Viewer[Z]. 2020. 


[285] Adcock J, et al. Advances in Quantum Machine Learning[Z/OL]. (2015-12-09). arXiv: 1512.02900. 


[286] Arunachalamand S, Wolf R D. Guest Column: A Survey of Quantum Learning Theory[J]. ACM SIGACT News, 2017, 48(2). 


[287] Kak S. On Quantum Neural Computing[J]. Information Sciences, 1995, 83(3-4): 143-160. 


[288] Smolensky P. Information Processing in Dynamical Systems: Foundations of Harmony Theory[D]. Boulder: University of Colorado Boulder, Department of Computer Science, 1986. Technical Report No. CU-CS-321-86. 


[289] Wiebe N, Kapoor A, Svore K M. Quantum Deep Learning[Z/OL]. (2015-05-22). arXiv: 1412.3489v2. 


[290] Brassard G, Hoyer P, Mosca M, et al. Quantum Amplitude Amplification and Estimation[J]. Communications In Contemporary Mathematics, 2002, 305: 53-74. 


[291] Amin M H, Andriyash E, Rolfe J, et al. Quantum Boltzmann Machine[Z/OL]. (2016-01-08). arXiv: 1601. 2036. 


[292] Biamonte J, Wittek P, Pancotti N, et al. Quantum Machine Learning[J]. Nature, 2017, 549: 195-202. 


[293] Porotti R, Tamascelli D, Restelli M, et al. Coherent Transport of Quantum States by Deep Reinforcement Learning[Z/OL]. (2019-01-20). arXiv: 1901.06603. 


[294] Steinbrecher G R, Olson J P, Englund D, et al. Quantum Optical Neural Networks[J]. npj Quantum Information, 2019, 5(1): 60. 


[295] Lloyd S, Weedbrook C. Quantum Generative Adversarial Learning[Z/OL]. (2018-04-24). arXiv: 1804. 9139v1. 


[296] Saitta L, Giordana A, Cornu¡äejols A. Phase Transitions in Machine Learning[M]. Cambridge University Press, 2011. 


[297] Tsallis C. Introduction to Nonextensive Statistical Mechanics: Approaching a Complex World[M]. Springer, 2009. 


[298] Maszczyk T, Duch W. Comparison of Shannon, Renyi and Tsallis Entropy used in Decision Trees[C]// Rutkowski L, Tadeusiewicz R, Zadeh L, et al. Artificial Intelligence and Soft Computing-Proc. of the 9th International Conference, Zakopane, 2008: 643-651. 


[299] Ghoshdastidar D, Adsul A, Dukkipati A. Learning With Jensen-Tsallis Kernels[J]. IEEE Transactions on Neural Networks and Learning Systems, 2016, 10: 2108-2119. 


[300] Lee K, Kim S, Lim S, et al. Tsallis Reinforcement Learning: A Unified Framework for Maximum Entropy Reinforcement Learning[Z/OL]. (2019-02-7). arXiv: 1902.00137v2. 


[301] Naftali T N, Zaslavsky N. Deep Learning and the Information Bottleneck Principle[Z/OL]. (2015-03-09). arXiv: 1503.02406v1. 


[302] Angelov P, Sperduti A. Challenges in Deep Learning[C]// Verleysen M. Proceedings of the European Symposium on Artificial Neural Networks (ESANN), 2016: 489-495. 


[303] Wu Z H, Pan S R, Chen F W, et al. A Comprehensive Survey on Graph Neural Networks[Z/OL]. (2019-12-04). arXiv: 1901.00596. 


[304] Vaswani A, Shazeer N, Parmar N, et al. Attention is All You Need[J]. Advances in Neural Information Processing Systems, 2017: 5998-6008. 


[305] Han S, Kang J, Mao H, et al. ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA[C]// Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, Monterey, CA, 2017: 75-84. 


[306] Shin D, Lee J, Lee J, et al. 14.2 DNPU: An 8.1TOPS/W Reconfigurable CNN-RNN Processor for GeneralPurpose Deep Neural Networks[C]// Proceedings of the 2017 IEEE International Solid-State Circuits Conference, San Francisco, CA, 2017: 240-1. 


[307] Gao C, Neil D, Ceolini E, et al. DeltaRNN: A Power-Efficient Recurrent Neural Network Accelerator[C]// Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, Monterey, CA, 2018: 21-30. 


[308] Shazeer N, Mirhoseini A, et al. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-ofExperts Layer[Z/OL]. (2017-01-23). arXiv: 1701.06538. 


[309] Zhang S, Kang M, et al. Reducing the Energy Cost of Inference via In-sensor Information Processing[Z/ OL]. (2016-07-03). arXiv: 1607.00667v1. 


[310] Sanchez-Lengeling B, et al. Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules[Z/OL]. (2019-10-25). arXiv: 1910.10685v2. 


[311] Yoo H J. Brain Inspired Intelligent SoCs and Applications[C]// IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC-16), Lyon, France, 2016. 


[312] Wang Z, Joshi S, Sergey E, et al. Memristors with Diffusive Dynamics as Synaptic Emulators for Neuromorphic Computing[J]. Nature Materials, 2016, 16(1): 101-108. 


[313] Fu T, Liu X, Gao H, et al. Bioinspired Bio-voltage Memristors[J]. Nature Communications, 2020, 11(1): 1861. 


[314] Miguel R, Philippe T, Sumito T, et al. Vowel Recognition with Four Coupled Spin-torque Nanooscillators[J]. Nature, 2019, 563: 230-234. 


[315] Yao X, Klyukin K, Lu W, et al. Protonic Solid-state Electrochemical Synapse for Physical Neural Networks[J]. Nature Communications, 2020, 11(1): 3134. 


备案号:YX01jbkWgwB9w7Lle

