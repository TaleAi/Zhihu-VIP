## 14.深度学习 AI 芯片
目前用得最多的 AI 芯片是基于深度学习，也就是深度神经网络（DNN），模拟生物神经系统，以并行方式进行计算的芯片。这样的 AI 芯片又被称为深度学习加速器。 


在过去的几年里，DNN 的性能有了很大改进。在一些应用领域里，深度学习已经实现了比人类大脑更高的精度，如计算机视觉、语音处理、自然语言处理、大数据问题等。这些发展使基于深度学习的 AI 芯片得到了极为广泛的应用，已经成为当前数据中心的一个主要需求，且这种需求仍在增长。从商业、金融、教育、医疗、通信等场合的各种设备到家用电器和手机，这类 AI 芯片都已经或即将付诸应用。 


通常情况下，设计人员会在现成的商用硬件上运行高度并行的深度学习 AI 计算，其中最典型的案例就是 GPU。在训练阶段，尤其是计算密集的情况下，这些芯片是非常适用的。系统参数可以在该阶段以经过验证的样例得到调整。在推理阶段，深度学习的应用对存储器的频繁访问和快速响应有着更高的要求，过去很多年来也在用 GPU 实现推理。然而，为了应对快速增长的需求，各家公司正在竞相开发更直接提高深度学习能力的 AI 加速器芯片，这些加速器可以快速执行专门任务，在训练和推理上有更为出色的表现。 


虽然一些研究人员尝试将 DNN 的模型和算法更多地与生物学（如脑科学）相关联，并取得了一定的成果，但大多数研发人员只关心对已有的深度学习算法进行加速，不太关心神经形态的原理。他们主要致力于最大限度地提高性能，同时关心如何突破商用冯·诺依曼硬件架构的局限性，如何能够在能效和性能上优于 GPU。这些因素推动了 ASIC 和 FPGA 的研发。 


备案号:YX01jbkWgwB9w7Lle

