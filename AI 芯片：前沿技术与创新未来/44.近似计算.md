## 44.近似计算
近似计算（Approximate Computing，AC）不是新的概念。自从引入浮点变量和模数转换器以来，这已成为计算领域的一个「古老」概念。物理世界是连续不断的，但计算机却是数字的，它将物理信息转换为离散的有限位数据，这本身就是近似。长期以来，我们已经接受了近似的需求，但几乎没有发挥其潜力。如今，程序员和架构师比以往任何时候都更趋向于采用近似计算来压缩计算规模，并缓解由于 CMOS 工艺达到极限而导致的可靠性问题。同时，随机计算、可逆计算和量子计算之类的姐妹计算范式也引起了人们的极大兴趣，这进一步表明，也许是时候抛弃我们的精确计算范式并在考虑近似值的前提下重建计算机了。 


现在，除了计算范式局限于精确计算之外，对于执行计算的半导体芯片，也一直执行十分苛刻的质量筛选，以保证不出任何差错。另外，对于输入的大数据，更不容许有半点差错。这种不容出错的苛刻条件带来的后果就是高功耗和高昂的费用。而生物大脑的运作并不强调精确计算，而是有非常好的容错机制，因此生物大脑的能耗非常低。这样就出现了一种想法：是否可以从逻辑电路、存储器到算法，重新设计和定义计算范式，使计算从「精确」变成可以接受的「近似」，并且带入「容错」功能？除了计算之外，如果再把数据的误差、芯片参数的偏差这两个维度考虑进去，就组成了一个如图 8.1 所示的三维容错模型。 


![img](https://pic2.zhimg.com/v2-af4ccff7d4e2c14c9a6552dbe8f19a4b.webp)

为了在应用中达到高精度，深度神经网络（DNN）使用大数据量和非常大的模型，这些模型需要大量的数据存储空间、极强的计算能力和大带宽的数据移动。尽管计算能力取得了很大进步，但是在大型数据集上训练最先进的 DNN 仍需要很长时间，这直接限制了应用推广的速度。 


因此，在 DNN 技术中，目前已越来越多地使用近似计算来解决上述问题。近似计算可以在 AI 计算的不同层级上实现：在软件编程语言上可以实现近似计算，在算法级、电路级，甚至器件级，都可以实现近似计算及后面要讲到的随机计算（见图 8.2）。本书第 3 章已经介绍了算法级运用近似计算的一些方法，如量化方法（降低数值位宽）、二值化或三值化、网络剪枝、减少卷积层等，目的都是减少计算量及数据存储量，同时提高能效。 


很多研究表明，DNN 可以抵抗近似计算带来的数值误差，如果近似计算使用恰当，网络的识别或分类精度并没有太多损失，不影响实际使用效果，但是计算量却会大大减小。从另一个角度来看，使用了近似计算，也会提高在系统各个层级的容错性，这对器件来说非常重要，因为半导体芯片工艺很容易出现各种缺陷，如果系统可以把这些缺陷忽略不计，那就大大提高了稳定性和可靠性。 


![img](https://pic2.zhimg.com/v2-dac8964a159f659cfa5ab8ef8bddc9a1.webp)

神经网络显示出很高的容错能力。近似计算范式通过在计算过程中故意引入可接受的错误来换取这种容错，以换取能效的显著提高。近年来，在 AI 研究领域中，已经出现了大量适用于神经网络的近似计算方法，可能有一二百种之多。除了算法层的近似计算（见第 3 章），本章的介绍涉及编程、电路和器件，分成两方面的用途：一是在功能齐全的芯片上使用近似计算来达到降低功耗的目标，其精度虽然降低，但仍在可接受的范围；二是芯片电路本身有缺陷、故障或误差，使用近似计算可以容忍这些问题，并对精度没有太大影响。 


#### 减少循环迭代次数的近似计算


这种方法是在应用层使用近似值的例子。在软件编程语句中有大量循环语句，为了节省计算量，有时可以把每次迭代次数减少，如下列 C 语言类的语句： 


![img](https://pic4.zhimg.com/v2-e55deebc7e8fe595809ef5c0b868d19c.webp)

这里面加了一个省略因子（skip\_factor），表示每次跳过的迭代次数。这个方法称为循环穿孔方法，已经和蒙特卡洛模拟、迭代优化和搜索空间枚举等算法配合使用，取得了很好的效果，一般可以达到 1 倍以上的运算速度提升。对于如何选取最优化的省略因子，以及使用循环穿孔方法达到系统容错效果等，也有不少研究人员进行了研究  [133,134]  。 


#### 近似加法器和近似乘法器


在深度学习加速器里用得最多的是加法器和乘法器，而近似加法器和近似乘法器电路可以说是近似计算研究中最活跃的领域。在电路级简化加法器和乘法器等算术模块，会使它们在某些情况下不精确，但却可以使它们变得更小、更快，因此更加节能。这些电路都已经在近年来的一些芯片中实现。下面分别介绍两种近似全加器和近似乘法器。 


在几种近似实现中，多位加法器分为两个模块：较高有效位的（精确）上部和较低有效位的（近似）下部。对于每个低位，一个单比特的近似加法器执行一个修改过的不精确的加法功能。这通常是通过在电路级简化全加器设计来完成的，等效于在功能级上修改全加器真值表中的某些条目。 


近似加法器有多种设计。一种为近似镜像加法器（Approximate Mirror Adder，AMA）  [135]  。镜像加法器（Mirror Adder，MA）是常见而有效的加法器设计。从晶体管级减少一个逻辑运算，即通过移除一些晶体管以获得较低的功耗和电路复杂性，可以获得 5 个 AMA。AMA 中节点电容更快速的充放电也将导致更短的时延。因此，AMA 用降低精度换来了能效、性能方面的提高，并节省了面积。 


另外一种近似加法器是基于 XOR/XNOR 设计的。这种近似加法器基于使用带有多路复用器（MUX）的 XOR/XNOR 门和晶体管实现了的 8 个晶体管组成的加法器  [136]  ，其中 4 个晶体管用于 XNOR 门，如图 8.3 所示。图中，Sum 对所有 8 种输入组合中的 6 种都是准确的，而 Cout 对所有可能的配置都是准确的。它的操作特性显示出在功耗、性能方面的一定优势，同时它具有很高的精度。尽管使用传输晶体管会降低噪声容限，但是当可以容忍较低的精度时，近似加法器很有用，其他设计指标也有显著提高。 


![img](https://pic2.zhimg.com/v2-68ba09af3e62d8c84c546ba20360ec54.webp)

与对近似加法器的研究相比，近似乘法器的设计较少受到关注。有人提议通过使用近似加法器来计算部分乘积之和，从而实现一种近似乘法器  [137]  。然而，就牺牲精度来节省能量和面积而言，直接应用近似加法器来设计近似乘法器可能没有效率。对于一个近似乘法器来说，一个关键的设计是减少添加部分乘积的关键路径。由于乘法通常是由级联的加法器阵列实现的，因此省略部分乘积中的一些次有效位、在阵列中删除一些加法器，可以实现更快的操作。还有人提议在浮点乘法期间舍弃尾数，以及在累加时使用近似加法器来实现近似乘法器。 


除了更改加法器之外，也有研究人员通过减少存储器使用的位宽，或设计近似的 MAC 单元等来实现近似。另外一种在电路级实施近似计算的方法是在处理神经网络时先找出一些对精度影响不大的权重，然后把这些权重用近似电路来处理  [138]  。具体来说，是用一种方法来确定一组对输出误差影响很小的权重，这是通过检查在训练过程中反向传播期间计算的灵敏度因子来完成的，然后再通过降低位宽精度和近似乘法器技术来处理这些确定的权重。仿真结果显示，在采用这种方法后，DNN 系统功耗降低了 38%，而 MNIST 数据集的分类精度只降低了 0.4%。 


由于乘法运算在神经网络的硬件实现中占了能耗的很大一部分，为了降低乘法能耗，有研究人员提出了一种近似的字母集乘法器  [139]  ，它利用计算共享的概念来实现数字神经网络。在这种电路中，传统的乘法被简化的字母移位和加法运算取代（因此称为没有乘法器的神经元）。其中字母是 4 位值，可用于表示任何数字的二进制数，可使用字母将乘法运算分解为较小的分量。 


#### 降低电源电压的近似计算


近似电路的另外一个比较简单的实现方法是降低电源电压，这将以少量瞬态错误为代价来降低功耗  [140,141]  。当电压值降低到安全值以下时，它会降低功耗，但也会由于电路中对时间敏感的路径发生故障而导致发生错误。该策略可以应用于处理单元和存储器，并且可以与其他建议的硬件近似策略一起使用。 


美国哈佛大学的研究人员在 2017 年的 ISSCC 上展示了一个全连接 DNN 加速器芯片  [142]  。它在电源电压大幅度降低时，把间歇性发生的时序违规做得十分有规律，这是通过算法和电路级容差技术的结合来实现的。对权重存储器和数据路径的操作都证明了可以对算法误差容错，而数据通路表现出相对较差的容错度。因此，研究人员使用了最初为 DSP 加速器开发的电路技术  [143]  ，可以进一步改进数据通路的容错性。这个 28nm 工艺芯片原型的测量结果表明，大幅度降低电源电压可以降低 30% 的功耗。另一方面，他们又使用数字电路的超频来提高吞吐量，可以实现 80% 的吞吐量提升。 


一般来说，降低电源电压可以大大降低功耗，因为电源电压和功耗两者之间是二次方关系。但是需要通过近似电路来弥补电路的不稳定性并尽量减小电压值降低所造成的误差。传统的芯片设计方法保证了每条电路路径都必须满足时序要求，无论如何激励，当电源电压降低得太多时，就会出现较大的时序误差，并迅速降低输出信号的质量。近似计算技术都在设法将电压值降低，但不会低于电路的安全电压，而在保证所有路径上的时序正确性这方面采用了不同方法。 


降低电源电压的方法也可以用到 DRAM 存储器上。瑞士联邦理工学院在 2019 年提出了「近似 DRAM」的概念  [144]  。为了降低深度学习神经网络的能耗及时延，他们降低了 DRAM 的电源电压，并且调整其时序参数，由此产生了一定的错误率，需要用一个特殊的 DRAM 分配机制来与神经网络本身的容错度相匹配，以达到所需的系统精度。根据测试结果，在 CPU、GPU、ASIC 加速器上用了这种「近似 DRAM」之后，可以降低 20%～30% 的能耗，并可提高约 8% 的 CPU 速度。 


在有些使用近似计算的设计中，不直接接受时序误差影响产生的结果，并完全忽略这类中间结果。从门电路设计的角度来看，这种方法仍可保证所有数字操作的时序正确性。另外一些设计方法直接接受错误计算的结果，当然前提是要严格控制误差的大小  [145]  。这种时序错误接受策略是通过使用操作统计信息，动态地对累加重新排序来减少错误，以减少早期时序错误的发生，防止全局信号质量严重下降。 


图 8.4 为反离散余弦变换（Inverse Discrete Cosine Transform，IDCT）电路计算的两个测试图像，对比了使用这种近似计算前后的技术的结果。图 8.4a 中的左图为传统 IDCT 计算在降低电源电压前的结果，右图为降低电源电压后的结果；图 8.4b 中左、右两个图分别显示了采用近似计算降低电源电压前后的结果。可以看出，采用近似计算的情形中，电源电压降低后的图像质量得到了明显改善  [145]  。近似计算中任何技术的输出质量都应该是动态且可调的，以便仅在应用允许的情况下才能降低精度。 


![img](https://pic1.zhimg.com/v2-3accc069c6903f37b75c53abae6fe0c6.webp)

然而，上述近似电路技术都是基于传统 CMOS 工艺的。近年来，器件领域的创新已经为设计全新的电路架构带来了极好的机会，这些新型器件将大大提高计算系统的性能和能效。 


对于新型 NVM 来说，通过降低电源电压来降低功耗是一个不错的选择。例如，对于 STT-MRAM，可以通过降低元器件两端的电压或减少访问时间来节省 STT-MRAM 消耗的能量  [146]  ，虽然降低电源电压和减少访问时间都会导致偶尔的错误，但使用视频解码的测试显示出了良好的结果，质量没有明显下降。 


#### 基于 RRAM 的近似计算


在新型 NVM 器件中，用来设计高效可重构近似计算框架非常有前景的是 RRAM。RRAM 器件能够在较小的空间内支持大量的信号连接。更重要的是，RRAM 器件可用于构建阻变交叉开关阵列（详见第 7 章），可以自然地对输入信号的加权组合进行传输并产生输出电压，以很巧妙的方式实现矩阵-矢量乘积累加。 


RRAM 器件是基于 TiO  x  、WO  x  、HfO  x  或其他具有可变电阻状态材料的无源二端器件。目前最成熟的是基于 HfO  x  的 RRAM。 


图 8.5 为一种基于 RRAM 的模拟近似计算硬件实现的框图  [147]  ，其中框架由 RRAM 处理单元（RRAM PE）组成。每个 RRAM PE 都包含多个 RRAM 近似计算单元（Approximate Computing Unit，ACU），以完成代数运算。每个 RRAM PE 还配备有自己的数模转换器（DAC），以生成用于处理的模拟信号。另外，RRAM PE 还可以具有几个本地存储器来存储数据，如以 RRAM 器件电阻状态形式存储的模拟数据，或存储在 DRAM 或 SRAM 中的数字数据。本地存储器的使用和类型都取决于应用的要求。所有 RRAM PE 均由两个采用循环算法的多路复用器组成。基于 RRAM 的框架构建了基于神经网络的近似加速器，可应用于很多领域。 


![img](https://pic2.zhimg.com/v2-038559a159b05d2025ff0e55549aa9de.webp)

基于 RRAM 的近似计算方法虽然具有巨大的潜力，但是要做成 DNN 这样的规模、成为独立的深度学习 AI 芯片，还面临许多挑战。一个主要问题是由互连电阻引起的 IR 电压降下降会影响 RRAM 的计算质量，并严重限制交叉开关系统的规模。实现深度学习需要很大的交叉开关规模，这就需要降低 IR 电压降或采用补偿技术。此外，因为 RRAM 的研发还属于很初级的阶段，许多 RRAM 特有的问题，如温度对电阻开关行为和 I-V 关系的影响等，还需要作大量的进一步研究。 


#### 应对电路故障的近似计算


近似计算的另一个重要目的是容忍不精确的电路或带有缺陷的芯片。在最先进的半导体集成电路中，晶体管性能的变化非常明显，并且概率物理参数占主导地位。设计、制造和测试包含数十亿个晶体管的硅芯片是非常具有挑战性的，半导体芯片的良率和可靠性直接关系到成本和用户体验，任何可以放宽这种精确性要求的方法都是非常有意义的。 


为了用常规的芯片设计方法来补偿晶体管和电路可能发生的误操作，必须确保较大的设计裕量，这是基于确定性算术处理的常规芯片设计方法的本质缺陷。为了解决这个问题，如果算术处理本身是在近似和概率行为的假设下构建的，则它就可以克服一般芯片设计技术的弱点，而与前沿的半导体芯片具有良好的兼容性。 


模仿生物大脑的人工神经网络具有容错性，本身就可以放宽这种精确性要求。很多实验证明，神经网络模型中少数节点的故障通常对分类精度几乎没有影响。奥利维尔·特马姆（Olivier Temam）的研究调查了 AI 芯片的逻辑门中注入晶体管级故障对简单全连接 DNN 的影响  [148]  ，证明了神经网络算法的容错能力可以转化为硬件神经网络加速器的容错能力。这个研究实验强调了一点：增加并行性也会增加容错性。当采用时分复用将较大的网络连接到较少的硬件处理单元时，单个硬件故障会影响网络中的多个节点，因此，空间扩展是必要的。这项工作的结果表明，即便深度学习所考虑的网络规模不大，也可以容忍大量的故障，而且分类精度的损失可以忽略不计。 


神经网络的容错能力不仅是理论上的概念，而且将变成 AI 芯片的实际属性。上述这些研究结果，为深度学习 AI 芯片提供了一种本质上能够解决硬件故障的方式，而不必识别和禁用故障部件。 


备案号:YX01jbkWgwB9w7Lle

