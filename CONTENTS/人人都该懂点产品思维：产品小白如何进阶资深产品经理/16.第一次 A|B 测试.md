## 16.第一次 A/B 测试
在着手梳理手头产品现状和改进点的同时，阿聪接到了一个小需求，调整推荐首页的频道顺序。 


在视频 App 内分频道进行信息展示是产品设计的常态，但是频道的名称和排列顺序对于用户的点击情况和消费时长都是有影响的。所以，我们需要在上线一段时间后，基于用户的使用情况和反馈情况对子频道进行调整以获得更大的收益。 


「500W DAU 的 Tab 页，6000W 的 VV 数据。怎么样，这个项目作为练手不错吧？」我笑道。 


听到 6000W 的 VV 数据，阿聪两眼一亮：「阿呆老师，我该怎样着手呢？」 


「先跑历史数据，计算出每个频道的渗透率，再根据频道的渗透率和频道的 VV 进行顺序的调整。」我道。 


阿聪很快基于已有的频道列出了数据表格，并基于频道的人均 VV 数据进行了新的顺序调整，除了推荐频道外，将其他频道按照人均播放次数进行了逆序排列（图中数据均为样例数据）。 


![img](https://pic1.zhimg.com/v2-1f6be8136b93264a73080dbbf7c5c786.webp)

「阿呆老师，我是这么分析这个项目的，我们可以将人均播放量高的频道放在前面，这样渗透率的提高能够带来更多的用户，就可以拉升总播放量了……」阿聪道，「所以我觉得我们可以按照这个方案上线了。」 


「这就上线？你想得太简单了吧。」我诧异道。 


「对啊，有理有据有节，接下来当然要上线啊。」阿聪不明白我为什么诧异。 


「现在早就不是产品经理拍脑袋决定上线的时代了，我们需要更为完善的 A/B 测试来验证我们的判断。」我道，「而且，这个方案里有一个明显的问题，其实你没有注意到。」 


「呃。」阿聪盯着数据始终没有看出个所以然来，「好吧，那就按您说的意思开始 A/B 测试吧。但是，什么是 A/B 测试呢？」 


什么是 A/B 测试
----------


A/B 测试的概念源于医学上的双盲实验：实验中，病人被随机分成两组，并分别给予安慰剂和测试用药。经过一段时间，再来观察比较两组病人的表现是否具有显著的差异，从而判断试用药是否有效。 


类似地，当产品经理在两个方案间举棋不定的时候，就可以将用户分为 50% VS 50% 的两组，两组用户分别应用方案 A 和方案 B。在线上实验运行一段时间后，可以通过两个组的核心数据比较，来确定哪个方案具有统计学意义的优势，从而选用数据表现更好的那个方案。A/B 测试方案既可以用于样式调整的方案选择，也可以用于与样式无关只影响数据流的策略调整方案。 


将 A/B 测试应用于产品迭代，本质上是产品经理的工作从经验主义走向实验主义的体现：通过多个方案的线上并行，积累真实的用户数据，依靠用户行为的后验数据而不是产品经理的先验按主观偏好决定产品方案的去留。 


以美团首页的频道导航为例： 


频道导航使用两行还是三行、图标的大小、右上角角标的文案调整等，都是这一区域可以进行 A/B 测试的样式调整点。 


哪些频道可以展示在一级页面，哪些需要折叠进「全部分类」，是一个可以进行 A/B 测试的策略点。 


如果根据频道的收益规模和影响面来确定露出和折叠，那就是产品驱动的调整。 


如果进一步根据用户的行为习惯，个性化地确定展现方式，那就是技术驱动的调整。 


上述例子主要为了说明 A/B 测试是什么，能应用于什么样的场景和领域。接下来，我们就围绕 A/B 测试的各个环节进行更详尽的系统化的补充。拆解定义，我们不难发现，一次 A/B 测试具有如下环节： 


* 用户分组以确定实验对象；
* 实验运行一段时间以积累数据；
* 比较两个组的核心数据；
* 选择具有统计学优势的方案。

用户分组（用户采样）是 A/B 测试面临的第一个问题。 


首先，确定用户流量规模。在小体量的应用或功能上，我们尚可以比较粗略地选择五五开进行两组实验。当用户体量比较大的时候，我们通常不可能开那么大比例的实验，就需要从功能的影响规模倒推需要多少实验流量。比如，我们在一个 DAU 百万的应用上进行 A/B 测试，需要实验组和对照组分别有一万用户。如果是首页或主要产品路径上的实验，那么，我们只要开 1% 的流量就能够选取到足够的用户；而针对 20% 使用频率的子功能 A/B 测试，需要 5% 的流量才能有足够数量的用户进入实验组和对照组。 


其次，确定用户采样方式。我们需要确保选用的流量具有足够代表性。换言之，我们希望在 5% 的用户身上得出的实验结论，能够平移推导到全量用户身上。分组不均或者选用了不具代表性的流量都会给实验结果造成偏差。较为恰当的方式是，我们对用户的特征进行适度梳理，在分组的时候注意不同特征维度上的平衡。 


我们在实验中常用的几个基础用户特征维度有：新老用户（功能的深度用户和浅度用户，一般升级频率比较高、活跃度比较高的用户可能更容易尝鲜，也更容易推高部分数据）、系统类型（如 Android 和 iOS 平台之分，iOS 内部又有不同手机厂商之分，特定平台和手机厂商的用户可能表现出更好的留存或付费意愿等）用户、男女用户等。 


为了确保分组的均衡，我们通常会使用 AABB 的方式，即两个对照组、两个实验组。如果测试正确进行的话，那么 A/A 测试里的两组将不会有区别。 


当然，真实的生产环境中不可能只有一组 A/B 测试开启，多组实验并行的时候就涉及了流量的重叠使用。A/B 测试中，我们经常会涉及分层和正交的概念。 


同一层的实验彼此间有干扰，不能相互重叠；不同层的实验针对不同的功能点，可以相互交叉。比如，对于列表页的信息展示我们可以同时进行交互样式和推荐算法的两组实验，那么用户流量实际上会分为四种： 


1. 新交互样式和老推荐算法。 


2. 新交互样式和新推荐算法。 


3. 老交互样式和老推荐算法。 


4. 老交互样式和新推荐算法。 


![img](https://pic4.zhimg.com/v2-665ddb1ce63bd4cb9c7138a6f9240866.webp)

  



如上图所示，流量被分为了两层，每层中的实验彼此独立，但层和层之间的流量却被重新打散。一个用户只可能落入｛Expl，Exp2，Exp3｝三者之一、｛Exp4，Exp5，Exp6｝三者之一。一个命中实验 Expl 的用户，可能会命中｛Exp4，Exp5，Exp6｝之一。在千万量级以上的服务上，同时在线上运行的会有成百上千个 A/B 测试。系统会在用户身上追加一系列 Exp 标签以进行溯和区分，一个用户在系统中的标识可能会是： 


uuid\_OSType\_AppVersion\_ExpInfoArray[Exp 1, Exp4, Exp8，…，ExpN] 


实验运行周期的确定是 A/B 测试面临的第二个问题，其本质亦是有效数据累计的问题。由于实验内的使用规模是单日规模与实验周期的乘积，我们可以通过设定试运行周期的长短来累计实验的数据规模，对于不同特点的功能可以有不同的方式。 


对于影响面大的小功能，通常会在周四上线，周五验证一下有没有漏洞或埋点错误，然后下周一、周二进行实验结果分析。这样，一个实验在线上跑一个周末后，经过三至五天的时间就可以得到功能的效果表现数据。 


对于大功能而言，稳定是第一优先级。通常建议在周一上线，每天更新实验数据和用户反馈，如果出现明显问题及时停止实验止损。 


对于可能会影响留存的功能，则通常会开更长的时间，如用一个月时间来检验新功能是否会对新老用户的活跃和留存情况产生影响。 


如果是影响面小的功能，更容易出现的问题是用户数不足，那么要么通过主观经验来选定上线方案来降低工程复杂度，要么开更长时间的实验来积累足够的用户数据。 


核心数据的比较是 A/B 测试面临的第三个问题。 


由于 A/B 测试已经有明确的目的，所以直接观测指标的确定和比较往往不会有什么问题，比如，优化商品回流页对于 App 的下载要考核激活率，优化用户主页或信息页对于特定用户的关注要考核新增关注关系规模等。我们在这一部分想额外阐述的是同时观察功能自身的核心指标和全局核心指标，即关注局部 VS 全局的关系。 


在全产品线迭代过程当中，每个产品人都会有自己负责的功能点和领域，大家都在努力改进自己负责领域的功能指标，不免会有局部收益提升、全局收益下降的情况发生。比如，回流页使用了强刺激性的引导文案提高了 App 激活率，却可能因为造成了用户错误的使用预期而降低了后续的 App 留存；通过引导或元素突出的方式，可能会短时间增加用户和用户间的关注关系，但是新增的用户关系并没有带来进一步基于关系的互动。 


为了避免这种情况的发生，我们在考核 A/B 测试时，就需要兼顾业务线的功能指标和整体产品的指标。只有在整体产品指标保持稳定的情况下，才进行业务指标的比较，避免丢了西瓜捡芝麻的情况发生。 


实验结果具有统计学意义，是搞清 A/B 测试的最后一个问题。 


很多实验中，方案 A 与方案 B 有 1% 的差别，而有些又只有 0.1% 的差别。那么，这样的改进是波动的偶然现象还是确凿的稳定现象呢？这个改进带来的收益是否置信呢？ 


为了解决这个置信度的问题，我们需要了解统计学的概念「P 值」。首先，我们对 A/B 测试提出如下假设： 


H  0  ：实验组和对照组没有明显差异； 


H  1  ：实验组和对照组有明显的差异。 


P 值是指当原假设 H  0  为真时，获得和观察数据一致情况的概率。P 值越小，说明原假设 H  0  越不成立，因为概率太小了。如当 P 值为 0.01 时，说明如果 H  0  为真，我们获得实验数据样本可能性只有 1%，这时，我们有理由相信实验组和对照组是有明显差异的。 


![img](https://pic2.zhimg.com/v2-872ed444fee0b2530b3a168e31cc2744.webp)

通常，在统计学上有一个显著性水平 α，当 P 值小于 α 的时候，即可推翻原假设 H0。在生产环境中，我们通常选用 α=0.05 作为衡量标尺。所以，并不是说实验组和对照组之间的差异大于 1% 就置信，只有 0.1% 就不置信，还是需要通过 T 分布理论计算 P 值来衡量。 


为了计算 P 值，我们首先通过如下公式计算出统计检验量 Z 值，公式中的相关变量为：x 代表两个实验方案各自的均值、S 代表各自的方差（标准差）、n 代表样本的大小。再通过正态分布 Z 值表查询到对应的 P 值。 


![img](https://pic1.zhimg.com/v2-bc6d75ff5dfd50af225609845ba9f996.webp)

在做完调整频道顺序的 A/B 实验后，阿聪认识到：直接按照人均播放 vv 数据进行调整并不能带来最优的结果，因为不同频道渗透率的不同，不仅仅同频道位置相关，也同用户需求和认知相关： 


* 一个频道的渗透用户量 = 有稳定认知、主动访问的用户量 + 无稳定认知的过路用户量。
* 一个频道的播放量 = 稳定认知用户数 × 人均高播放量 + 无稳定认知用户数 × 人均低播放量。

第四位的频道换到第二位，会增加大量无稳定认知的过路用户，但是这样的用户，人均播放量较低，虽然人数多了，但并不能带来更高的播放量。 


因而，需要多次调整频道顺序，才能达到播放量全局最大的最优解。在折腾了一周，终于把频道顺序确定下来之后，阿聪突然灵光一闪，对我道：「阿呆老师，四个字的频道名字会在导航栏占据 1.5 倍的宽度。我们是否把它的名字调整成两个字，或者把后面的频道放到前面来，把『相声小品』频道放在首屏最后，只露出两个字？」 


「不错，孺子可教。」我笑道。单单从频道名称和频道组织、频道顺序的角度，可以挖掘的东西就很多，我们再来深挖一下： 


* 「奇趣」「开眼」这样的名词，用户能否理解？如果不能理解的话，我们是否可以改用更好的名字来降低用户理解成本？
* 「原创」这样的频道，用户是否关心？用户看的是内容，不管是内容方、UGC 还是 OGC，用户甚至不会关心这个内容的版权部分。原创这个频道存在的必要性是什么？

这个案例中，频道名和频道顺序只是一个非常小的功能点。但是，深挖下去能够带来显著的收益。通过 A/B 测试不断验证我们的想法，才能最终找出最优解，全量上线。 


备案号:YX01W1dyn6MOMrNWw


###### 2020-07-03 11:07
