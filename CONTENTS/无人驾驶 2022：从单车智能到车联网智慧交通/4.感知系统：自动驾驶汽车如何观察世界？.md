## 4.感知系统：自动驾驶汽车如何观察世界？
当我们人类在驾驶汽车时，需要用眼睛来识别汽车外部的环境，例如车道线位置、周围障碍物、信号灯，然后再结合自身所在的位置向目标驾驶。对于自动驾驶而已，它的驾驶过程与人类是如出一辙的。它们也需要先用「眼睛」感知世界，然后用「大脑」处理收集到的信息，最后再做出相应的决策。


那么对无人车而言，它们的「眼睛」是由什么组成呢？


**一、无人车的眼睛**
------------


与人类的眼睛不同，图像信息不过是无人车的「眼睛」可以收集到的诸多信息里的一种。如下图所示，许多自动驾驶汽车上都配有摄像头、激光雷达和声波雷达来收集周围的信息， 这些传感器合起来便组成了无人车的「眼睛」。下面我就简要介绍下这些传感器到底有何神通。


![img](https://pic1.zhimg.com/v2-3171f5a1f5c87a3a162602fcf4ba0154.webp)

**1. 摄像头（Camera）**


摄像头是无人车中最常使用的传感器，也可以看做是最类似人眼睛的传感器，常见的有**单目(Mono)、双目(Stereo)、深度相机（RGB-D）**三种类型。


单目相机价格较为便宜，然而测距较为不准。双目相机的价格基本是单目相机的两倍，它利用俩个相机的视差，通过软件算法进行物体距离预测，精准度要远大于单目相机。深度相机则是利用红外线结构光或者 TOF 进行物理测距，所以与双目相机相比，它会节省许多计算资源，但它测量范围窄、噪声大、视野小、易受日光干扰、无法测量透射材质。所以在选择使用摄像头时可以根据分辨率、视野、动态范围等来选择最合适的种类。


**2. 激光雷达（LiDAR）**


摄像头虽然可以获得丰富的信息，但受光线影响极大，而且捕捉范围有限，所以自动驾驶往往还装配着 3D 传感器——激光雷达。


根据线束数量多少， LiDAR 可以划分为 128 线、64 线、32 线、16 线。LiDAR 测距原理是将光束发射到环境中并接收反射回来的光束，通过测量光束飞行时间来得到障碍物距离，其输出为三维的点云图。在选择使用 LiDAR 时可以根据线束数量、转速、视野、每秒收集点数等来选择合适的 LiDAR, 目前一流的激光雷达测距已可过百米。


**3. 毫米波雷达（Radar）**


Radar 测距原理和 LiDAR 相似，只不过其发射的是毫米波，同时 Radar 可以根据多普勒频移效应测量与障碍物之间的相对速度。其优点是近距离测距、测速比较准、价格低廉，缺点是噪点很大，检测范围有限。


通过上述的传感器，自动驾驶汽车已经获得了周围环境信息。然而就如人类需要用大脑来处理我们双眼收集到的信息一样，自动驾驶汽车也无法直接理解这些初始信息，需要通过一些列算法来将它们转化为自己可以理解的内容。这些传感器与对应的算法合起来便构成了无人车的**感知系统**，而感知系统里最为强大的、最广为人知的算法当属**神经网络**。


**二、感知系统的秘密武器——神经网络**
---------------------


若想理解神经网络是如何工作的，就必须先了解图像的本质。


图像可以看作是以数组形式存储在计算机中，数组元素为图像像素值，一般可以分为 RGB（红绿蓝）三通道。


如下方左图在我们看来是一张小猫的照片，但是在计算机看来不过是右图的数值矩阵，图像每一个位置都有一个对应的像素数值，数值越大代表这一点越「亮」。所以神经网络在对图像进行处理时，其实是对图像像素值进行运算。在计算机视觉中目前常用的神经网络类型为**卷积神经网络**，能够很有效的提取图像深层信息[[1]](#ref_1)。


![img](https://pic3.zhimg.com/v2-da073bc34318fb056950cd01b47fef35.webp)

**那么什么是卷积神经网络呢？**它是由一些列的卷积核组成，而每个卷积核的本质其实也是一个数值矩阵。 不过与输入图像不同，卷积核的尺寸一般要小得多，通道要大得多（图像只有三通道，它可能会有上百个通道），它的数值也会在不断地学习中不停地变化。


下面我们来看一个简单的例子。


如下图所示，输入是一张维度为 6x6x3 （长 6，宽 6，3 通道）的图像，这里使用到了两个卷积核对图像进行运算，卷积核大小为 3x3x3 。每一个卷积核分别对图像进行卷积操作，卷积后图像变为 4x4 ，由于这里使用到了两个卷积核，最终图像维度变为 4x4x2 。这个经过卷积操作后的**卷积特征**在我们人类看来可能会变得很抽象，但是却能被计算机更好地理解。这些处理过后的信息经过许多次卷积后，便能被计算机彻底理解，从而抽取出具象的信息，例如障碍物的位置、车道线的位置。


![img](https://pic3.zhimg.com/v2-97e68776abf8f00339eecdc8a00f5e53.webp)

这里大家需要注意的是，**卷积神经网络更像是一种统称**，它有许许多多的种类，虽然这些种类都是由卷积核组成，但由于卷积核可以有不同的组合、不同的类别，不同的尺寸，最终的效果也是大不相同。感知系统的使命，便是利用传感器丰富的输入与不同的卷积神经网络，将周围环境所有的具象信息都提取出来。


**三、感知系统的使命**
-------------


整体来说，感知系统的任务可以大致分为目标检测、目标追踪、行驶空间分割和传感融合。其中目标检测既可以使用 Camera, 也可以使用 LiDAR 来完成。


**1. 目标检测**


**① 基于 Camera:**


**定义**：  给定由 Camera 收集的行驶实时视频/图像数据，对场景中关键类物体进行检测，包括但不不限于物体 2D/3D 的位置以及种类。常见的例子便是车辆目标检测，如下图所示，所有汽车的 3D 位置被标注了出来。


![img](https://pic1.zhimg.com/v2-f867b45fad86740181e047d3e3397bbe.webp)

  



**方法**：目前常见的目标检测大概有两种流派。


第一种是**双阶段检测器**，，非常具有代表性的是 Faster RCNN[[2]](#ref_2) 。法如其名，它一共含有两个阶段来检测物体。在第一阶段，它会利用一系列的卷积核找到多个可能含有物体的大概区域，并且提取出每一个区域的卷积特征。在第二阶段，又会有另外的卷积神经网络对上一阶段提取的区域进行卷积，找到物体更精准的位置，并且识别物体的类别。双阶段检测器的优点是**精度高**，但是**速度非常慢**，因而演化出了第二种分支。


![img](https://pic3.zhimg.com/v2-c67b9559576f993a60a0f6dc4bfc25e3.webp)

 第二种是**单阶段检测器**，以 Yolo 家族（yolo1-5）为代表。**Yolo 系列检测器**舍弃了双阶段检测器的常规特征提取方法，直接将区域粗略选择与精细检测、物体分为合为一体，一步到位。它的速度大大提高，但是提取的卷积特征质量有所下降，所以精度也会有下滑。


![img](https://pic1.zhimg.com/v2-8c0b8dcaa3df4fbf975915e972a6f89c.webp)

**② 基于 LiDAR：**


相对与 Camera 产生的图像数据来说，LiDAR 得到的点云数据通常是无序、不规则、稀疏的。根据点云表示方法，它一般是有四个维度，X, Y, Z, Intensity. X、Y、Z 代表着它空间的三维位置，intensity 则是该点的反射强度，不同的材质反射强度一般不同。虽然与图像不同（点云多了一个维度），但我们依旧可以用卷积神经网络来处理，只要把卷积核也相应增加一维即可。


**点云检测最经典的一种便是** **PointRCNN** [[3]](#ref_3) 。和上述所讲的 Fast RCNN 类似，PointRCNN 是一个两阶段的检测器。第一阶段通过卷积学习点云特征，判断每个 3D 点是否有可能属于检测物体，然后产生一个物体的大概区域。第二阶段对每一个大致区域内的点云再次进行特征学习、优化，最后得到精准的目标框。基于点的检测能够有效捕获目标空间结构信息，检测精度更高，但是其计算代价也更高。


![img](https://pic3.zhimg.com/v2-a9cf3bccc4b045e1a48b1cf09c0a5480.webp)

**③ 目标跟踪：**


目标检测一般是基于单帧图像，而目标跟踪则是基于连续的视频。一般我们检测到一个物体后，便会用目标跟踪的算法根据历史信息来推测它下一步的位置。


在这里一定有人会问，**我们如果每一帧都做目标检测，为什么还需要目标跟踪呢？**


试想这样一个场景，我们的自动驾驶汽车检测到了左前侧有人类在横穿马路，这时左边又停了另一辆车挡住了视线，如果只用单纯的目标检测，那么汽车就会认定前方无任何行人经过，如此一来就有可能有车祸发生。但如果我们用了目标跟踪的算法，那么我们便能根据前几秒的信息推断出行人的大概位置。


![img](https://pic4.zhimg.com/v2-eb03c2eb9562737cd659cb4b0365b0db.webp)

**④ 图像分割:**


对给定实时行驶场景下的具体目标进行分割，期望获得像素级别的精准切分。一些常见的场景有车道线分割，以及可行驶空间的分割。


![img](https://pic4.zhimg.com/v2-591c77ef9cd79dca5802e3a1c122d9c6.webp)

**⑤ 传感器融合**


正如前文所介绍，照相机与 LiDAR 都可以做到目标检测，所以工业界常常把这两种不同的传感器的信息融合到一起进行目标检测，达到 1+1>2 的效果。常见的融合方法分为前融合与后融合，前融合即是将原始的图像与点云重合到一起，然后用神经网络进行检测。后融合则是在两种不同的传感器输出单独的结果后，用传统概率算法将它们的结果融合，得到最终的目标检测信息。


**四、感知系统的挑战**
-------------


以上我们简单聊了一下感知系统一些对应的具体任务。实际应用中，还有一些与任务相伴的挑战牵涉其中。以下是一些具体例子，


1. 感知系统需要的数据信息量大，像素级任务较多，任务之间有多类，如何完整的进行标注，并且针对感知系统设计更加精准的标注细则，是一个很有挑战性的工作。
2. 感知系统的模型需要能在海量数据下快速迭代，如何找到一个合适的模式，需要逐步探索。
3. 从产品层级来说，自研算法需要与产品有较高的重合度和匹配值，要具备嵌入式，高度优化的产品化能力并不容易。

从上面的讨论可见，感知系统虽然是自动驾驶的一个模块，却和设计，技术，产品三个方向相互关联。因而对于一个感知模块的开发来说，并不是仅仅只有开发高精度的算法那么容易。虽然感知系统中，做相关算法研发最火，但是其实算法仅仅是其中的一个小模块。


**五、感知系统的食粮——大数据**
------------------


正如上文提到，数据的质与量直接关乎到感知系统里神经网络模型的准确性。用一个不太恰当的比喻，数据之于模型，犹如粮食之于人类。


高质量的数据下训练一个优异的模型，和有大量噪音下训练出来一个模型，难度千差万别。因而一个高质量的数据集，对于提升感知模型整体表现，有着巨大的贡献。而自动驾驶领域，复杂的车道线，拥堵的车队，以及复杂的地形，都对数据提出了更高的要求。


因而针对于数据集的问题，各大研究机构和无人车公司[[4]](#ref_4) ，都有相当的投入，也开源了不少高质量数据集。比如 Karlsruher institute[[5]](#ref_5)  开源的 KITTI 数据集，UC Berkeley 开源的 BDD 数据集和 NuTonomy 公司提出的 NuScenes 数据集。在 CVPR 2019 上，Waymo、Argo 和 Lyft 等无人车公司也通过竞赛的方式，以 workshop 的形式提供了公司数据集。


![img](https://pic4.zhimg.com/v2-22aaa86b68d685df8444a673287a7e64.webp)

下面我们简单来看一下几个比较典型的开源数据集


* **KITTI 数据集:** KITTI 数据集是现在无人驾驶领域很多论文采用的数据集，包含了自动驾驶感知和预测，定位和 SLAM 技术等方向。其中，立体评估，三维重建，2/3D 物体检测， 物体追踪等领域有很多论文引用并实验，这也使得该数据集成为了一个评价算法好坏的重要衡量标准。
* **Bdd 数据集:** Bdd 数据集是伯克利大学发布的一个大规模、多样化的驾驶视频数据集。BDD100K 数据集包含 10 万段高清视频，每个视频约 40 秒，720p，30 fps 。每个视频的第 10 秒对关键帧进行采样，从而得到 10 万张图片，每张尺寸 1280\*720。标注涉及道路目标边界框， 可行驶区域，车道线标记，全帧实例分割等。涉及了源图像的 URL、类别标签、大小（起始坐标、结束坐标、宽度和高度）、截断、遮挡和交通灯颜色等信息。
* **nuScenes:** 是由 nuTonomy 发布的自动驾驶数据集。NuTonomy 编辑了 1000 多个场景，其中包含 140 万幅图像、40 万次激光雷达扫描（判断物体之间距离）和 110 万个三维边界框（用 RGB 相机、雷达和激光雷达组合检测的物体）。该数据集使用了 6 个摄像头、1 个激光雷达、 5 个毫米波雷达、GPS 及惯导系统收集数据，覆盖了对于自动驾驶系统有挑战性的复杂道路、天气条件等。

**另外我想和大家聊一下，工业界落地用的数据是如何收集的？**


不同的公司在这个问题上有不同的选择，其中特斯拉的做法比较特别。特斯拉计划借助大部分掌握在客户手中的车队车辆来收集数据，因为这对配置第二代硬件设施的汽车来说尤其重要。实现这一途径的方法就是 Autopilot 2.0，也就是通过自家公司的软件+用户协议来完成。在这一过程中，特斯拉对视频数据的收集是零星且随机的，具体是通过 7 到 8 个摄像头捕捉外部场景。车身主摄像头和前部窄摄像头以 10 秒内 30 帧/秒的速度间隔进行片段拍摄，此外还包括雷达摄像头的快照。相对应的，通用公司和 Waymo 则是选择自己部署无人车收集数据。


**六、小结**
--------


以上是对无人驾驶感知方向的简单介绍。现在为大家简单总结一下，我们基本讨论了


* 无人驾驶里的感知系统是什么，有什么组成
* 感知系统的研发方向
* 感知系统对应的设计，技术，产品，数据之间的交互
* 感知系统需要什么样的数据，可参考的数据集，以及如何使用数据提升感知系统

备案号:YX01APK9jDXMmP4pY


参考
--

1. [^](#ref_1_0)深度卷积网络：卷积神经网络基础 <https://blog.csdn.net/cg129054036/article/details/104739513>
2. [^](#ref_2_0)从编程实现角度学习 Faster R-CNN（附极简实现）， 人工智障的深度瞎学之路，  <https://www.sohu.com/a/215920394_717210>
3. [^](#ref_3_0)PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud,  <https://arxiv.org/pdf/1812.04244.pdf>
4. [^](#ref_4_0)解构无人驾驶：有哪些关键技术？哪些关键公司？， 来源于新浪网， <https://finance.sina.com.cn/world/gjcj/2018-11-15/doc-ihnvukff4458194.shtml>
5. [^](#ref_5_0)KITTI Vision benchmark suit, karlsruhe institute,  <http://www.cvlibs.net/datasets/kitti/>

###### 2021-08-03 05:04
