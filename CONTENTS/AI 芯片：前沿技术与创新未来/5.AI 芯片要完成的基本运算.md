## 5.AI 芯片要完成的基本运算
AI 芯片是模仿大脑运作的芯片，它使用模拟神经元和突触的模型来尝试再现人类智能。 


#### 大脑的工作机制


大脑中有许多神经细胞，可以通过连接传达信息，并建立记忆。具有这种作用的神经细胞被称为神经元。神经元在连接处不断发送电子信号以传输信息，而突触就位于该连接处。换句话说，大脑中的神经细胞是神经元，它们的交界处是突触。 


如图 1.2 所示，神经元由树突、突触、核及轴突构成。单个神经元只有激活与未激活两个状态，激活条件为从其他神经元接收到的输入信号量总和达到一定阈值。神经元被激活后，电脉冲产生并沿着轴突经突触传递到其他神经元。现在我们用「感知机」的概念模拟神经元行为，需要考虑权重（突触）、偏置（阈值）及激活函数（神经元）。 


![img](https://pic1.zhimg.com/v2-ce29c67a3bf4196e7e7fb0024d637853.webp)

上面讲的只是一个概念，我们有必要通过软件和硬件使计算机更容易处理神经元行为。为达到此目的，我们为神经元建立一个模型（见图 1.3）。 


![img](https://pic4.zhimg.com/v2-0c960e5ebe6aecccfa8c8c9689856420.webp)

图 1.3 中，函数 f 是输入和输出之间的关系，称为激活函数；w  i  是突触特征的权重，权重为正表示突触兴奋，权重为负表示突触处于抑制状态；x  i  是另一个神经元的输出；b 是神经元激活（将输出传递到后续阶段）的阈值。 


激活函数 f（x）的部分常见形式如图 1.4 所示。使用激活函数的目的是引入非线性，使输入和输出脱离线性关系。 


![img](https://pic2.zhimg.com/v2-e235ffe0c5e474af47697df5249f4cf6.webp)

a）线性函数 b）符号函数 c）Sigmoid 函数（S 形函数） d）Softmax 函数 


e）双曲正切 S 形函数 f）ReLU 函数 g）硬双曲正切 S 形函数 h）阶跃函数 


目前用得最多的激活函数是图 1.4f 所示的修正线性单元（Rectified Linear Unit，ReLU）函数。ReLU 函数的表达式非常简单，就是小于 0 的部分全部置为 0，其他保持不变。S 形（Sigmoid）函数（见图 1.4c）是传统的神经元模型最常使用的激活函数，它基于神经科学仿生而得出。在第二波神经网络的浪潮中，S 形函数被用来模拟之前提到的神经元的激活过程，表现出了高度适用的效果。但该函数也容易带来饱和效应问题（即梯度弥散效应），会造成很长的「学习」时间。看似简单的 ReLU 函数解决了这个问题，因此成为深度学习算法的重大改进点之一。 


#### 模拟大脑运作的神经网络的计算


把神经元连起来，就成为一个神经网络（见图 1.5）。深度学习，即深度神经网络（DNN）的特点是在输入层和输出层中间加了很多层，称为隐藏层。目前较新的 DNN，已经有几百个甚至 1000 个以上的隐藏层。在 DNN 中，通过添加层的数量，可以将精度和吞吐量提高，并可以通过改变权重来响应各种功能。由于深度学习的采用，神经网络的层数显著增加，导致模型非常大。一般需要多达数千万的权重来定义神经元之间的大量连接，这些参数是在很费时间的学习过程中确定的。 


AI 芯片是一种高速执行神经网络输入到输出计算过程的芯片。这里重要的是如何快速计算图 1.3 中的函数 f，其中主要是下面的计算：


![img](https://pic4.zhimg.com/v2-bb84d78d35b47610da33da3e929cb27e.webp)

如果把此式展开，可写成： 


![img](https://pic2.zhimg.com/v2-6914e42df76919b87105141877438690.webp)

  



这是乘积累加（Multiply Accumulation，MAC）运算。传统上，通过使用诸如数字信号处理器（Digital Signal Processing，DSP）和 ARM 处理器的 NEON 之类的单指令多数据流（Single Instruction Multiple Data Stream，SIMD）指令，可以对此进行高速计算。 


式（1.1）可视为两个矢量的点积，即(x  0  , x  1  , x  2  ,…, x  n  )(w  0  , w  1  , w  2  ,…, w  n  )。点积是计算机中使用的最基础的线性代数运算，仅仅是两个矢量中相关元素的乘积累加。矢量的集合是矩阵（也可称为一种张量，矩阵是二阶张量）。换句话说，如果矩阵和矩阵相乘，则可以高速处理矩阵和矢量的乘法，可以同时执行许多乘积累加运算。 


![img](https://pic1.zhimg.com/v2-b672d136d3d7fd7c037b27cae18f32bd.webp)

矩阵和矩阵的乘法及矩阵和矢量的乘法可以针对每个行和列并行计算，因此很适合多处理器并行处理。此外，在神经网络中，由于只有前一层的结果（神经元值）涉及特定神经元的计算，并且不依赖同一层的神经元计算，因此神经元值可以并行处理。 


由此可见，AI 芯片（这里指用于深度学习加速的芯片）的本质是「高速张量运算及并行处理」。神经网络处理单元主要就是由乘积累加模块、激活函数模块和汇集模块组成的。 


#### 深度学习如何进行预测


通过使用构造相对简单的深度神经网络（DNN），可以实现传统上用复杂逻辑才能实现的功能，如识别和预测等。这里举一个简单的例子，来说明深度学习如何进行预测。图 1.6 是一个正弦函数 y=sin（Ax），参数 A 控制正弦波的频率。对于人类来说，一旦我们理解了正弦函数，就能知道它在任何参数 A 下的行为。如果我们得到一个部分正弦波，就可以弄清楚它应该是什么样的波，并且可以将波外推到无穷大。 


![img](https://pic3.zhimg.com/v2-348aa92222354ad715d7ef50edec6e28.webp)

深度学习可以预测参数 A 未知的正弦波。这是一个时间序列预测问题。我们希望能在训练期间模型从未见过参数 A 的情况下，预测其未来的值。 


正弦波通常通过泰勒级数展开等方法计算。但是如果创建一个对应表，就将立即获得一系列正弦波的值，给定一些与函数 sin（Ax）匹配的数据点，就可以尝试预测未来的值。简单地说，神经网络的学习只是等同于制作图 1.6 的对应表。 


图灵奖得主、贝叶斯网络之父朱迪亚·珀尔（Judea Pearl）在近年的一篇访谈  [1]  中直言：「当前的深度学习只不过是『曲线拟合』（Curve Fitting）。这听起来像是亵渎……但从数学的角度，无论你操纵数据的手段有多高明，从中读出来多少信息，你做的仍旧只是拟合一条曲线罢了。」珀尔还指出，「当前的机器学习系统几乎完全以统计学或盲模型的方式运行，不能由此做出未来的高度智能机器。」他认为突破口在于因果革命，借鉴结构性的因果推理模型，能对自动化推理做出独特贡献。 


#### 提高性能和降低功耗


人类大脑大约有 1000 亿个神经元。在任何给定的时刻，单个神经元可以通过突触将指令传递给数以千计的其他神经元——即传递到神经元之间的空间中，而神经递质通过这个空间交换。大脑中有超过 100 万亿个突触介入神经元信号传导，在剪除大量连接的同时加强一些连接，使大脑能够以闪电般的速度识别模式、记住事实并执行其他学习任务。 


图 1.7 是根据目前世界上主流超级计算机的性能绘出的计算机性能发展与人脑计算性能对比的示意图。一般来说，由于网络带宽等限制因素，超级计算机实际达到的性能（图中红线）要比理论值（图中蓝线）低；另外，超级计算机的耗电量极大，导致运行成本非常高。如果按照图 1.7 中曲线展示的趋势继续发展，说不定再过几十年，超级计算机在性能上确实可以达到人脑的性能（超过 1 ZFLOPS，即每秒超过 10  21  次浮点运算）。这说不定就是人们常说的「奇点」。 


![img](https://pic2.zhimg.com/v2-b8da085d4b4ee8398a3abbe4bedd72be.webp)

但是，如果按照目前的技术水平，超级计算机达到人脑性能需要耗费的电力将会是个天文数字，所以实际上是不可行的。在高性能芯片上工作的深度学习系统能效很低，运行功耗非常大。例如，如果为了完成计算密集型任务而并行使用多个 GPU，功率很容易超过 1000 W。人脑则通常不会完全执行详细的计算，而只是进行估算，功耗几乎不到 20 W，这只是一般灯泡所需的功率。2016 年，AlphaGo 对战围棋九段高手李世石时，运行该 AI 程序的服务器功耗达 1 MW，将近人脑的 5 万倍。 


因此，对于 AI 芯片设计来说，节能是一个亟待解决的重大课题，这里涉及半导体器件、电路甚至半导体材料本身的问题。 


另外，与其他应用不同，深度学习算法对网络带宽的需求也异常高。在目前一些超级计算机架构设计中，深度学习的运行速度非常慢，其原因是并行深度学习涉及大量参数同步，需要极大的网络带宽。如果网络带宽不足，那么在某些时候，添加到系统的计算机（或处理单元）越多，深度学习就越慢。因此，针对深度学习的新架构设计非常重要。 


备案号:YX01jbkWgwB9w7Lle

