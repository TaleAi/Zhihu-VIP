## 81.超低功耗 AI 芯片
深度学习 AI 芯片要完成由卷积层和全连接层组成的 DNN 的运算，这些运算需要计算单元和存储器两大部分，对应的推理计算量和存储容量量级分别如下所示。 


（1）计算量：即使是中等大小规模的推理，每次推理通常也会超过 10 亿次算术运算。 


（2）存储容量：100 KB～150 MB。一个典型例子是 ResNet，这个网络模型的大小为 120 MB，然而其训练所需要的存储容量将会达到 21 GB。另外一个例子是基于 LSTM 的 NLP，它的模型大小达到了 2.5 GB，而在训练时所需的存储容量达到 40 GB。 


如此庞大的计算量和存储容量，都要消耗大量能源。由此可见，要真正做到超低功耗，首先需要把神经网络的规模降下来。 


「超低功耗」这个词常常出现在芯片的电路设计层面，几十年来，已经出现了许多卓有成效的降低功耗的方法和创意。现在一直在使用的 CMOS 工艺，就是在 20 世纪 80 年代初出现的一场降低功耗的重大革命，这种方法一直沿用至今。近 10 年以来，近阈值电压（Near Threshold Voltage，NTV）方法被使用在一些处理器的设计上，又一次降低了芯片的功耗。 


在将近 30 年的时间里，芯片的电源电压一直在下降，到 2005 年左右达到了 1 V。由于芯片的动态功率与电压的二次方成正比，因此，小幅降低 10% 的电压可将动态功率降低 19%。而晶体管开始导通并传导少量电流的电压点被称为阈值电压，在现代工艺技术中，阈值电压为 0.2～0.3 V（尽管晶体管电流直到电压为 1 V 左右时才达到饱和）。在 ISSCC 2012 上，英特尔展示了 NTV 技术，把电源电压降到仅略高于阈值电压，从而显著提高了能效。 


在 AI 芯片设计中使用 NTV 的一个例子是韩国 KAIST 的研究人员在 ISSCC 2017 上展示的一款超低功耗、可以「永远在线」的人脸识别 AI 芯片  [234]  。该芯片包含 4×4 个处理单元，每个处理单元有 4 个卷积单元，每个单元具有 64 个 MAC 阵列。该芯片每个时钟周期可执行 1024 个 MAC 操作，而在 0.46 V 的近阈值电压的情况下，可以 5 MHz 的低时钟频率进行高吞吐量操作。在平均帧率为 1 f/s 时，该芯片的功耗仅为 0.62mW。 


但是，如果从 AI 系统的输入数据、算法级、网络架构级入手，从如何把神经网络的规模大大缩小，同时又不影响其精度和性能（或虽然降低了一些精度和性能，但不影响实际应用）的角度来考虑功耗问题，效果将大大好于电路级。这方面有很多问题值得思考，举例如下。 


（1）现在的监督式神经网络需要给输入数据加标签，并进行大量的数据分析，需要的数据量也非常大。如何仅仅使用少量数据即可达到效果？ 


（2）如果今后改成无监督或自监督的神经网络，神经网络的规模是不是可以缩小？ 


（3）把神经网络的架构从一层层的顺序运算，改为树形结构或图形运算，是否可以缩小网络的规模？ 


（4）如何把网络模型的规模做到 100 KB 以下，却仍然达到一定的精度并可以得到有效应用？ 


图 14.1 为深度学习 AI 芯片目前所能达到的性能与功耗的示意图。本章介绍的超低功耗 AI 芯片，是指芯片的功耗在 2mW 以下，甚至达到微瓦级（见图 14.1 中的绿色部分）。由于功耗极低，这种芯片的性能、精度、可编程性等都会受到一定影响，但是可以满足某些应用的特定需求。 


未来超低功耗 AI 芯片的目标是在超低功耗的运行状态下，把峰值性能提高到 1000 TOPS 的等级，同时不牺牲分类精度及可编程性（见图 14.1 中的箭头）。另外一个目标是把神经网络的规模尽量缩小，从而让芯片不需要达到很高的性能即可有效运算。这里的能效目标是 10,000 TOPS/W。 


![img](https://pic3.zhimg.com/v2-3e92c4cb3780c1f559bfe3119ea4e76c.webp)

超低功耗神经网络是一个很新的研究方向，主要研究如何把网络的规模做小，做成一个「袖珍式」网络模型。2019 年初，一个专门讨论这个问题的名为「TinyML」的兴趣小组在美国硅谷成立，并定期召开研讨会议。TinyML 期望的系统，其目标是功耗仅为几十纳瓦到一微瓦。 


现在已经有一些初创公司在朝着这个方向努力。例如，BabbleLabs 创建了约 20 层的中等复杂程度网络模型，这些模型可以适应不到 100 KB 的内存，以将语音命令带到电视和其他嵌入式设备中。以色列的初创公司 3DSignals 的深度学习 AI 芯片能够「听」出机器发出的异常声音，并发出警告，从而避免生产出低质量的产品。这款芯片也达到了极低的功耗。最近来自学术界和产业界的一些创新，确实已经将移动设备的硬件平台带入了 100mW 功率指标之内。 


实现超低功耗神经网络可以使用不少新的计算范式和新的方法，例如模拟计算、存内计算等，这些在本书前面几章已经作了探讨。采用存内计算的 AI 芯片的最新例子，是 2020 年 7 月由 IMEC 和格罗方德（Global Foundries）宣布的存内模拟计算（Analog in-Memory Compute，AiMC）深度学习加速芯片。它采用 22nm FD-SOI 技术，频率为 100 MHz，实现了创纪录的高达 2900 TOPS/W 的能效（见图 14.1）。在未来几年的探索中，很有可能会出现更有效的方法，通过硬件创新来缩小能效差距，尤其是对于功耗预算小于 1mW 的「永远在线」应用。 


除了研究新的计算范式和新的算法来降低功耗之外，其他研究则主要集中在使神经网络的待机功耗为零的可能性上。一般来说，芯片的功耗主要由动态功耗和静态功耗组成。动态功耗是芯片工作时所产生的，而静态功耗是由晶体管的漏电造成的，也就是说，一般的 CMOS 芯片即使处于待机状态，也仍然会消耗很多电能。相比之下，非易失性器件可在不考虑额外操作的情况下立即打开和关闭电源，从而理想地消除了静态功耗（见图 14.2）。这个意义上，CMOS 技术与自旋电子器件（如 MTJ 器件）的混合集成将可能成为实现高性能、超低功耗和高可靠 AI 处理单元的关键技术。 


![img](https://pic4.zhimg.com/v2-68e1b9d6c31e3c2d02c6ad2a9a31fdf6.webp)

用于零功耗待机应用的 AI 系统或能够与零电流待机配合使用的 AI 芯片可以延长设备的使用期限。然而，这类方法仍然依赖电池，其使用受到电池寿命的严格限制。 


备案号:YX01jbkWgwB9w7Lle

