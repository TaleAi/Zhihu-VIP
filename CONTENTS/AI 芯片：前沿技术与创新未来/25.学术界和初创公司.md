## 25.学术界和初创公司
全球各家半导体芯片供应商现在都对 AI 的芯片实现感兴趣，它们要么研发自己的 AI 芯片（单独的芯片或是集成到其他芯片上的 IP 核），要么收购那些较为成熟的初创 AI 芯片公司。 


美国市场调查公司 CB Insights 在 2019 年曾发表「AI100」报告，从 3000 多家公司中选出了前 100 家最有前途的 AI 初创公司，这些公司提供 AI 硬件和数据基础设施、机器学习工作流程优化，以及各行业的应用。 


上述 100 家公司中有 11 家公司是「独角兽」公司，也就是说，这些公司的估值均超过了 10 亿美元。从新兴初创公司到成熟的「独角兽」公司，各个不同阶段都需要不断的资金投入并进行产品商业化。募资资金充足，排名前 2 位的公司是中国的商汤（SenseTime）和旷视（Face++），专注于人脸识别技术；第 3 名是位于美国加利福尼亚州的 Zymergen，通过 AI 学习发现新材料，其重点领域之一是寻找塑料和石油产品的替代品。2019 年上榜的中国公司有 6 家：商汤、依图、第四范式、旷视、Momenta、地平线。以色列和英国各有 6 家公司上榜。 


这些公司有很大一部分是研发 AI 芯片的，有的打算针对 AI 某个领域的应用做出自己独有的半导体芯片。成百上千家 AI 芯片公司正在涌现。公开数据显示，截至 2019 年底，国内芯片设计公司的规模已经发展到近 2000 家，虽然受经济影响，增速大不如前两年，但是仍然有不少人想要分一块「蛋糕」。AI 产业如同海啸般的巨大规模，令人惊叹。 


此次 AI 热潮中涌现的 AI 芯片初创公司的数量，在半导体芯片行业中前所未有，也超过行业中任何其他领域。虽然它们现在大部分还在研发阶段，但仍然不时听到盈利的消息。第一批初创公司现在正在从架构创新向现实应用冲刺，以第一代芯片和工具链赢得客户。可以看出，尤其是在边缘侧应用上，初创公司的产品很有发展前景，那些芯片设计巨头的优势将会逐渐消失。 


第一批初创公司主要围绕深度学习加速器架构，进行不同程度的算法和架构协同优化，以期达到最低的功耗和最高的性能。大多数芯片开发的目标是赶上或超过英伟达。然而，大多数处理器都是针对现今的 AI 算法进行设计。对后来者来说，如果还是沿用原来的冯·诺依曼体系结构及 CMOS 数字电路，在原来的 AI 算法和架构基础上作出惊人的突破已经变得越来越难。 


2019 年以来，使用新兴技术（如模拟计算、量子启发计算、存内计算、光子计算、神经形态计算等）来实现 AI 芯片的初创公司已经浮出水面，而这些公司更容易获得关注并得到风险投资的青睐。但是，这些公司要真正实现产品化，还有很长的路要走。 


大学和研究机构也积极投入 AI 芯片的研究，学术界的研究在很多方面都具有独创性和前瞻性，占有领先地位。由于 AI 受到人们前所未有的重视和关注，现在学术成果被引入商业和产业的速度也非常惊人。有的是大学和产业界直接进行项目合作，新技术可以很快进入产品阶段，研究人员可以更快地获得回报，这可以大大促进创新。学术界往往先做成芯片样片，要使芯片真正产品化，实现解决方案，还需要产业界的巨大努力。 


#### 大学和研究机构的 AI 芯片


本节先介绍几种由学术界研发的 AI 芯片（这里先只介绍深度学习加速器芯片，类脑芯片等在后面几章中介绍），这些芯片的设计目标主要是开发最节能的芯片，重点是在全球边缘侧移动设备上的应用。这些芯片受到了业内较大程度的关注和借鉴。然后介绍几家十分有特色的 AI 芯片初创公司及其 AI 芯片。 


**1.Eyeriss**


Eyeriss（2016 年发布）和 Eyeriss v2（2018 年发布）  [61]  都是麻省理工学院研究团队研发的新一代 AI 芯片，是主要针对移动设备上低功耗的需求而设计的图像识别芯片。它通过可重构架构，针对各种 CNN 形式优化整个系统（包括加速器芯片和片外 DRAM）的能效。 


CNN 被广泛用于现代 AI 系统，但也为底层硬件带来了吞吐量和能效方面的挑战。这是因为其计算需要大量数据，片内、片外都有大量的数据移动，这比计算本身更耗能。因此，把任何数据移动能耗成本降到最低，是达到高吞吐量和能效的关键。而 CNN 中能耗最高的是卷积层，因而它是功耗降低的焦点所在。 


Eyeriss 通过使用独创的行固定处理数据流架构来实现这些目标。它具有 168 个 MAC 处理单元的空间架构。行固定数据流可以重构给定形状的计算映射，通过在本地寄存器文件级别最大限度地重用数据（权重、像素、部分和）来优化能效，以减少昂贵的数据移动（如对 DRAM 的访问）。 


压缩和数据门控也被用于进一步提高能效。Eyeriss 处理卷积层为每秒 35 帧，每次乘积累加约有 0.0029 次 DRAM 访问（对于 AlexNet），功率为 278mW，不到移动 GPU 的 1/10。芯片测试结果是，内核电压为 0.82 V 时功率为 94mW，内核电压为 1.17 V 时功率为 450mW。此外，Eyeriss 采用台积电 65nm 工艺，芯片面积也非常小（3.5 mm  2  ）。 


Eyeriss v2 在 Eyeriss 的基础上有了新的改进。为了应对各种各样的层的形式和大小，它引入了高度灵活的片上网络（NoC），称为分层网格网络，可以适应不同数据类型的数据重用和带宽需求，从而提高计算的资源利用率。此外，Eyeriss v2 可以直接在压缩域中处理稀疏数据，以处理权重和激活，因此能够利用稀疏模型提高处理速度和能效。总体而言，在稀疏的 MobileNet 中，采用 65nm CMOS 工艺的 Eyeriss v2 在批量大小为 1 时达到了 1470.6 推理/s 和 2560.3 推理/J 的吞吐量，比运行 MobileNet 的 Eyeriss 快 12.6 倍，耗能为 Eyeriss 的 1/4。 


不少芯片实现已经证明了降低数值精度或使用二值网络等可以大大减小计算量。MIT 在这些方面也作了很多研究，但专注于一些外界较少探索的方法，特别是考虑了如何设计紧凑 DNN 的各种卷积核，以及如何对稀疏 DNN 的压缩域中数据进行处理的方法。虽然紧凑和稀疏的 DNN 具有较少的操作和权重，但它们也为 DNN 加速硬件设计带来了新的挑战，主要表现在数据重用率方面。 


处理单元（PE）利用权重和激活的稀疏性来提高各种 DNN 层的吞吐量和能效。数据以压缩稀疏列（Compressed Sparse Column，CSC）格式保存，用于片上处理和片外访问，以降低存储和数据移动成本。总的来说，利用稀疏性可以在 MobileNet 上将吞吐量提高 1.2 倍，能效提高 1.3 倍。 


与运行 AlexNet（超过 724.4 亿个乘积累加操作）的 Eyeriss v1 相比，Eyeriss v2 的速度提高了 42.5 倍，并且使用稀疏 AlexNet 将能效提高了 11.3 倍。很明显，支持稀疏和紧凑的 DNN 对速度和能耗具有重要影响。 


图 4.10 为 Eyeriss  [62]  和 Eyeriss v2 架构的比较。与 Eyeriss 架构类似，Eyeriss v2 由一系列 PE 组成，每个 PE 包含用于计算乘积累加的逻辑和本地暂存区（Scratch Pad，SPad）内存逻辑，以利用数据重用及全局缓冲器（Global Buffer，GLB）。GLB 在 PE 和片外 DRAM 之间提供另一个级别的存储层。因此，Eyeriss 和 Eyeriss v2 都具有两级存储层，主要区别在于 Eyeriss v2 把 PE 和 GLB 合在一起分成多组，以支持以低成本将 GLB 连接到 PE 的灵活片上网络，而 Eyeriss 在 GLB 和 PE 之间使用了多播片上网络。 


![img](https://pic3.zhimg.com/v2-f32f7111ea98cb9eb42bef990239b908.webp)

**2.EIE**


虽然利用稀疏性等可以提高运算速度，但从 DRAM 提取权重比 PE 操作耗时高两个数量级，并且导致了绝大部分的能耗。因此，斯坦福大学提出了「深度压缩」方法，使大型 DNN（如 AlexNet 和 VGG）可以完全适用于片上 SRAM。这个方法主要通过修剪冗余连接并使多个连接共享相同的权重来实现压缩。 


斯坦福大学研发了「高能效推理引擎」（Efficient Inference Engine，EIE）  [63]  AI 芯片，它在图 4.11 所示的压缩网络模型上执行，并通过权重共享为稀疏矩阵矢量乘法加速。EIE 是可扩展的 PE 阵列，每个 PE 在 SRAM 中存储部分网络，并执行与该部分相关的计算。它利用了动态输入矢量稀疏性、静态权重稀疏性、相对索引、权重共享和位宽仅 4 位的权重。 


![img](https://pic2.zhimg.com/v2-cf0cdc3baea74517b1d69df24fdd0f66.webp)

EIE 把操作从原来的 DRAM 转到 SRAM，功耗降低为原来的 1/120，利用稀疏性可以在此基础上再降低 90%；权重共享可以再降低 7/8；而利用 ReLU 函数跳过零激活可进一步降低 2/3。根据 9 个 DNN 基准评估，在没有压缩的相同 DNN 情况下，EIE 比 CPU 快 189 倍，比 GPU 快 13 倍。 


EIE 的处理能力为 102 GOPS，直接在压缩网络上工作（在未压缩网络上处理能力为 3 TOPS）。在处理 AlexNet 全连接层并以每秒 1.88×10  4  帧运行时，功率仅为 600mW，是 CPU 的 1/24,000、GPU 的 1/3400。EIE 芯片主要用于自动翻译。 


**3.DNPU**


深度神经网络处理单元（Deep Neural Network Processing Unit，DNPU）芯片是由韩国科学技术院（Korea Advanced Institute of Science and Technology，KAIST）在 2017 年发布的  [64]  。DNPU 的异构架构由 CNN 处理器、MLP-RNN 处理器和高层 RISC 控制器组成。CNN 处理器旨在最大限度地利用卷积运算的可重用性，并具有大量 PE 以覆盖大规模 MAC 操作。另一方面，MLP-RNN 处理器的 PE 数量较少，但是它被优化以减少大量参数的片外访问。CNN 处理器由 16 个卷积核和一个聚合核组成。每 4 个卷积核串联连接，最后一个卷积核连接到聚合核。卷积运算的部分和结果被转移到下一个核并累加。 


该芯片由一个 CNN 卷积层处理器、一个 CNN 全连接层及 RNN/LSTM 处理器核组合而成，即不但用 CNN，还加上了 RNN 的功能。CNN 一般用于图像特征提取和识别，而 RNN 则用于序列数据识别和生成。CNN 先提取特征值，再作为 RNN 的输入，然后让 RNN 进行信息识别、字幕生成等操作，扩大了这款芯片的应用范围。CNN 需要进行烦琐的计算，因此用了 768 个 MAC 单元，而 RNN 只用了 8 个 MAC 单元。该芯片采用了量化查找表（LUT）来作乘法，由于权重使用 4 位精度，降低了外部存储器的带宽要求，降低了功耗。 


该芯片使用 65nm 8 金属层 CMOS 工艺制造。CNN 处理器具有容量为 280 KB 的片上 SRAM，MLP-RNN 处理器具有容量为 10 KB 的片上 SRAM，占用 16 mm  2  的芯片面积。该处理器可在 0.765～1.1 V 的电源电压下工作，时钟频率为 50～200 MHz；电源电压为 0.765 V 和 1.1 V 时的功耗分别为 34.6mW 和 279mW。这款芯片在 4 位位宽和 0.77 V 电源电压下，能效可达到 8.1 TOPS/W，主要用于移动图像识别及手势识别。 


**4.Envision**


比利时天主教鲁汶大学的研究人员在 2016 年和 2017 年分别发布了 Envision 芯片的 v1 和 v2  [65]  。这两块芯片集中利用了当时所有可以优化的技术，如压缩网络、降低数值精度、利用网络稀疏性等。它采用了二维单指令多数据 MAC 阵列的处理器架构。Envision v1 采用了动态电压精度缩放（Dynamic Voltage Accuracy Scaling，DVAS）技术，2017 年发布的 Envision v2 中则采用了动态电压精度频率缩放（Dynamic Voltage Accuracy Frequency Scaling，DVAFS）的硬件电路。这些都是基于近似计算原理在系统架构上实现的技术，在改变数值精度（如从 16 位变到 8 位或 4 位）时，将会同时改变电源电压和开关频率，这样就实现了功耗的动态变化。 


Envision 采用了 28nm FDSOI 工艺技术，FDSOI 技术独有的特点是体偏压（BodyBias）技术。通过调节体偏压，进一步提高了能效。这使得该设计在考虑计算精度的同时能够调整动态功耗与泄漏功耗的平衡。在高精度下，允许降低电源电压以降低动态功耗，同时保持速度，在有限的泄漏功耗代价下提高整体效率。而在低精度和开关频率降低时，提高晶体管阈值电压及电源电压以降低恒定速度下的泄漏能耗。这虽然增加了动态能耗，但降低了整体能耗。 


Envision 的芯片面积为 1.87 mm  2  ，与其他类似 AI 芯片相比，它的面积非常小，因此非常适合应用在物联网、带 AR 功能的微型可穿戴设备等场景。室温下，它在 1 V 电源电压下以 200 MHz 的频率运行。人脸识别的测试结果显示，在功耗平均为 6.5mW 时每帧图像识别耗能为 6.2μJ，而在 77mW 时则为 23,100μJ。通过一种能量可调节的分层处理，可以进行「永远在线」的人脸识别。Envision 能够最大限度地降低任何卷积层的能耗，在标称吞吐量下可节省 97.5% 的能耗，从而实现「永远在线」的人脸识别。这款芯片的性能达到了 76 GOPS。 


**5.QUEST**


在过去几年中，封装技术的进步为 3D 异质集成解决方案带来了福音。目前正在研究的裸片到裸片层间连线方案都涉及使用硅通孔（Through Silicon Via，TSV）。虽然 TSV 一直是直接在逻辑层上堆叠存储器的首选方法，但它并不是唯一的解决方案。TSV 的一种替代技术是 ThruChip 接口（ThruChip Interface，TCI），它是一种两层裸片之间的近场电感耦合无线通信技术，非常具有创意。目前，TCI 总体仍然处于研究阶段，尽管至少有一家公司正在商业产品中采用这种技术。 


在 2018 年的 IEEE 国际固态电路会议（IEEE International Solid-State Circuits Conference，ISSCC）上，日本北海道大学和庆应义塾大学的研究人员展示了 QUEST 神经处理器  [66]  。QUEST 是一种推理引擎，它使用堆叠式 SRAM 和 TCI 来集成大量 SRAM 缓存，拥有足够大的带宽以维持峰值性能。该芯片是日本北海道大学和庆应义塾大学的合作产品，而庆应义塾大学也是 TCI 的原创开发者。 


QUEST 是由 9 个裸片（即 8 个 SRAM 和一个逻辑裸片）组成的 3D 模块，采用台积电的 40nm CMOS 低功耗工艺制造，所有裸片的面积均为 121.55 mm2（14.3 mm×8.5 mm）。由于散热原因，实际的 QUEST 逻辑芯片包含 24 个内核，位于 SRAM 堆叠的顶部，如图 4.12 所示。 


![img](https://pic1.zhimg.com/v2-e193d6fb494e03b9a9c32f433371677f.webp)

QUEST 使用 TCI 来通信，不需要对基本 CMOS 工艺进行特殊改动。此外，TCI 可以通过磁场渗透金属层和有源器件，这意味着线圈可以放置在顶层上。对于电源连接，QUEST 则使用硅通孔，由于存在多个并联连接，因此可以在很大程度上消除开路接触故障之类的问题。值得注意的是，理想情况下，TCI 旨在配合高掺杂硅通孔（Highly Doped Silicon Via，HDSV），这是一种没有连线、通过电感耦合的功率传输技术。HDSV 使用深度杂质阱而不是实际的 TSV 或电线来跨越芯片。然而，该技术正处于早期研究阶段，尚未在任何实际的 TCI 原型芯片中得到展示（它还依赖最近晶圆减薄技术的进展）。 


QUEST 的逻辑芯片包含 24 个内核，每个内核都以 300 MHz 频率运行，并与一个位宽为 32 位的 4 MB SRAM 堆叠存储区（Vault）关联（见图 4.12）。Vault 由直接位于逻辑芯片下方的每个 SRAM 裸片的 512 KB 小块组成。各个 TCI 通道以 3.6 GHz 频率运行。每个内核有并行通道，由 7 个发送线圈和 5 个接收线圈组成，以便访问各个独立的堆叠 SRAM。读/写时延始终为 3 个时钟周期，这包括 TCI 断开时间，并且在所有 8 个堆叠 SRAM 裸片上保持一致。每个 Vault 的读写速度可达 9.6 GB/s，组合后的数据带宽达 28.8 GB/s。 


QUEST 芯片还有几个重要的特色（见图 4.13）：QUEST 使用相对简单的位串行架构，由 16 列和 32 行组成 32×16 PE 位串行架构阵列；PE 在单个周期内进行二值计算，并在 N 个周期（N<5）内进行 N 位对数式量化计算。对数量化方法在两个方面优于线性量化：允许更密集、更精细的方法来更好地表示权重和激活分布；通过对数位串行加法和线性累加在 PE 中计算点积，把消耗更多资源的乘法运算转换为加法。 


![img](https://pic3.zhimg.com/v2-a68f6643857faca71bf904797b69d7bd.webp)

该芯片的另一个特色是 24 个内核都采用 MIMD 并行处理引擎。每个核都以网格拓扑结构连接到其 4 个「邻居」——北、南、东和西，以及树状结构的全局网络。此外，每个核都通过 TCI 连接到相应的 SRAM 区。直接存储器访问由专用 DMAC 单元完成，该单元处理存储区 SRAM 存储器访问及 7.68 MB 的片上（内核）共享存储器。 


QUEST 支持 1～4 位精度，在 1.1 V 额定电压下功率为 3.3 W（包括堆叠的 SRAM），并且在位宽为 4 位时实现 1.96 TOPS 的峰值性能，位宽为 1 位（二值网络）时可达 7.49 TOPS 的峰值性能。 


**6.LNPU**


韩国 KAIST 在 2019 年公布了一个节能的片上学习加速器  [67]  ，称为 LNPU。它对数值精度加以优化，同时通过 fp8-fp16 的细粒度混合精度保持训练的准确性；部分使用较窄的位宽可减少外部存储器访问并提高吞吐量；此外，通道内及通道间累加都充分利用了稀疏性，使其具有更高的吞吐量及能效。此外，LNPU 集成了一个输入内部负载均衡器（ILB），用于在面对由不规则稀疏性引起的工作负载不平衡时提高 PE 的利用率。 


这个称为 LNPU 的深度学习 NPU 由 16 个稀疏深度学习核、1 个中心核（CC）、1 个 SIMD 核和 1 个高层 RISC 控制器组成。每个稀疏深度学习核都有一个 ILB 和 4 条 PE 线路，每条 PE 线路有 48 个 PE，每个 PE 都有 1 个 fp8-fp16 可配置 MAC 和一个 4×16 位本地寄存器文件。CC 在前馈（FF）和反向传播（BP）期间聚合每个核的部分和，并在权重梯度生成期间将激活梯度馈送到每个核。CC 将 fp8 数据转换为 fp16，反之亦然，并执行零压缩和解压缩。SIMD 核计算非线性函数和批量归一化。 


LNPU 可以在 0.78～1.1 V 电源电压下工作，最大时钟频率为 200 MHz。电源电压为 0.78 V 和 1.1 V 时的功率分别为 43.1mW 和 367mW，能效分别为 3.48 TFLOPS/W（fp8，稀疏度为 0）及 25.3 TFLOPS/W（fp8，稀疏度为 90%）。 


LNPU 不仅支持 DNN 推理，还支持各种 DNN 结构的训练。由于使用了混合精度和稀疏深度学习核，与密集 fp16 操作相比，LNPU 的能效提高了 2.08 倍，而对 ResNet18 的学习精度没有任何降低。LNPU 的能效比 NVIDIA V100 GPU 高 4.4 倍，峰值性能比之前的 DNPU 高 2.4 倍。 


LNPU 采用 65nm CMOS 工艺制造，占用 16 mm  2  的芯片面积，可处理稀疏性并提供用于学习的精细混合精度，并有着很高的峰值性能。 


表 4.2 列出了上述几款 AI 芯片在能效上的比较。 


![img](https://pic2.zhimg.com/v2-7f1704c74b9860221e26a831c5f12126.webp)

续表：


![img](https://pic1.zhimg.com/v2-e1c840d230a58191850bfb10d8e99ec1.webp)

表 4.2 中的 Bankman 是指丹尼尔·班克曼在 2019 年发布的由模数混合电路组成的 AI 芯片。这款芯片在本书 3.4.1 节已经作了介绍，它与其他绝大部分用数字电路实现的 AI 芯片不同，采用了由开关电容神经元组成的模拟 MAC。从表 4.2 可以看出，相对于其他芯片，含模拟 MAC 的芯片达到了最高的能效，而且它在可编程性方面也比较灵活。然而，采用模拟电路实现 MAC，会占用较大的芯片面积。 


#### 四家初创「独角兽」公司的芯片


2019 年，有 4 家「独角兽」公司分别展示了它们高度创新的深度学习 AI 芯片，引起了业界人士的高度关注。本节介绍这 4 款独特的芯片。 


**1.世界上最大的 AI 芯片**


2016 年在美国硅谷成立的初创公司 Cerebras，于 2019 年 8 月发布了一个晶圆级 AI 芯片。 


现代半导体芯片包含的晶体管数量巨大：目前英伟达最大的 GPU A100 的芯片面积为 826 mm  2  ，包含了 540 亿个晶体管；2019 年 8 月 AMD 发布的 7nm 霄龙（Epyc）系列 CPU（代号为「罗马」），包含多达 320 亿个晶体管；Graphcore 的 GC200 的芯片面积为 823 mm  2  ，包含了超过 594 亿个晶体管；而专注于 AI 的 Cerebras 设计了它的晶圆级引擎（Wafer Scale Engine，WSE），这是一个尺寸大约为 8 英寸 ×9 英寸（1 英寸 =2.54 厘米）的正方形硅片，包含大约 1.2 万亿个晶体管。Cerebras 发布了一张放在键盘旁边的 WSE 裸片照片（见图 4.14）。 


一家初创公司能将这种晶圆级产品迅速推向市场，非常令人印象深刻。晶圆级处理的想法最近引起关注，是因为它可以作为性能扩展问题的潜在解决方案。研究人员评估了在大部分或整个晶圆上构建巨大 GPU 的想法。他们发现该技术可以生产高性能处理器，还可以有效地扩展到更先进的节点尺寸。Cerebras WSE 绝对有资格被称为「世界上最大的 AI 芯片」。尽管它还不是完整尺寸的 300 mm 晶圆，但面积比 200 mm 晶圆要大。 


![img](https://pic3.zhimg.com/v2-61b88cc2f514baa347894b77e5b51691.webp)

Cerebras WSE 包含 40 万个根据 AI 需求优化了的处理单元，称为稀疏线性代数处理核（Sparse Linear Algebra，SLA），分布在一个晶圆上（见图 4.15）。它是一个灵活、可编程的计算核，针对支持所有神经网络计算的稀疏线性代数进行了优化。SLA 的可编程性确保内核可以在不断变化的深度学习领域中运行所有神经网络算法。 


SLA 配有 18GB 的超高速片上 SRAM 存储器，并具有前所未有的 9 PB/s 的存储器带宽。这些内核通过细粒度、全硬件的片上网格结构把通信网络连接在一起，可提供 100 Pbit/s 的互连速率。大量的处理核、大量本地内存和低时延大带宽结构共同构成了加速 AI 工作的最佳架构。 


![img](https://pic3.zhimg.com/v2-695393a2cd736d1cd346aa0be97e4b66.webp)

SLA 中包含了一个乘积和运算单元，称为融合乘法累加（Fused Multiple ACcumulate，FMAC）单元，它不仅可用于一维（标量）、二维和三维张量的乘积和运算，还具有算术逻辑运算、加载/存储、分路等一般 CPU 的功能。 


整块芯片基于台积电的 16nm FinFET 工艺制造。由于该芯片是由单个晶圆构建的，因此该公司已经实现了在片上坏核周围进行布线的方法，即使晶圆的一部分中具有坏核，也可以保持其阵列连接。该公司称它在芯片上实现了冗余内核，但尚未讨论具体细节。另外一个关键挑战是散热问题。WSE 使用位于硅片上方的大型冷板冷却，垂直安装水管直接冷却。由于没有足够大的传统封装适合该芯片，Cerebras 设计了自己的封装：将电路板和晶圆、连接两者的连接器和冷板结合在一起。 


这种芯片虽然不大可能被用于消费类设备，但人们一直有兴趣使用晶圆级处理器来改善云端的性能和功耗，有朝一日可能会看到 GPU 制造商利用晶圆级处理器来构建大型的云端 AI 系统。 


芯片尺寸对 AI 芯片来说非常重要，因为大芯片可以更快地处理信息，在更短的时间内产生结果；可以缩短学习（训练）时间，使研究人员能够测试更多想法、使用更多数据并解决新问题。今天 AI 的基本限制是模型训练需要很长时间。因此，缩短训练时间将消除整个行业进步的主要瓶颈。 


当然，芯片制造商通常不会制造这么大的芯片。在单个晶圆上制造芯片的过程通常会产生一些杂质。杂质会导致芯片发生故障，造成良率下降。如果晶圆上只有一块芯片，它有杂质的概率是 100%，会使芯片失效。但 Cerebras 设计的芯片是冗余的，因此一些杂质不会使整块芯片失效。 


WSE 专为 AI 功能设计，也引入了很多基础创新。通过解决数十年来限制芯片尺寸的技术挑战，如交叉掩膜连接、良率、电源输送、冷却、封装等，Cerebras 推动了最先进技术的发展，它的每个架构决策都是为了优化 AI 工作性能。努力的结果是，这家公司的 WSE 提供了比现有方案高出数百或数千倍的性能，而只需很小的功耗和空间，实现了巨大的技术飞跃。通过将各种学科的顶级工程师聚集在一起，该公司在短短几年时间内开发出了新技术并交付出了产品。 


由于处理单元针对神经网络计算进行了优化，因此 WSE 的性能据称可以超过 1000 个最高端的 GPU 所能达到的性能。此外，WSE 处理单元包含 Cerebras 发明的稀疏采集技术（Sparsity Harvesting Technology），以加速稀疏工作负载（包含零的工作负载）的计算性能，如深度学习。当 50%～98% 的数据为零时（如深度学习中的情况），大多数乘法运算都被浪费了。由于 Cerebras 稀疏线性代数核不会乘以零，所有零数据都会被滤除，并且可以在硬件中跳过，从而可以在其位置上完成有用的工作。另外，WSE 的片上内存比当时最先进的 GPU 大 3000 倍，内存带宽比后者大 10,000 倍。 


WSE 提供更多内核进行计算，有更多内存靠近内核，因此内核可以高效运行。由于大量内核和内存位于单个芯片上，因此所有通信都保留在芯片上。Cerebras 与台积电合作开发了一种跨划片线（晶圆通常沿着这些划片线切成芯片）建立互连的方法，以便每个小功能模块中的内核都可以通信。Swarm 通信结构是 WSE 上使用的处理器间通信结构，它仅以传统通信技术功耗的一小部分就实现了突破性的大带宽和低时延。Swarm 提供的低时延、大带宽 2D 网格，可连接 WSE 上所有的 400,000 个核，速率为 100 Pbit/s。该架构中的通信能耗成本远低于 1 J/bit，比 GPU 低近两个数量级。通过结合大带宽和极低的时延，Swarm 通信结构使 Cerebras WSE 能够比任何当前可用的解决方案更快地学习。 


**2.软件定义的 AI 芯片**


位于美国硅谷的初创企业 Groq 的成长速度非常快。2019 年 11 月，Groq 发布了张量流式处理器（Tensor Streaming Processor，TSP）架构和新的计算范式。Groq 采用全新的架构方法来加速神经网络，它没有创建一个小型的可编程内核，然后将其复制成数十个或数百个核，而是设计了一个具有数百个功能单元的巨大处理器。这种方法大大减少了指令解码的开销，使一块 TSP 芯片可以嵌入 220 MB 的 SRAM，可以在每个时钟周期进行超过 40 万个整数乘积累加运算。 


TSP 架构在单芯片实现中能够达到 POPS（即 1000 TOPS）级别的性能，相当于每秒执行 1000 万亿次运算。TSP 架构是世界上第一个达到此性能水平的架构，比英伟达当时最好的 GPU 快 4 倍。Groq 的体系结构还能够每秒进行多达 250 万亿次浮点运算（FLOPS），从而既可执行推理，也可执行训练。Groq 已成为第二家所发布深度学习芯片被应用于云端的初创公司（2019 年下半年，Graphcore 成为第一家所开发 AI 芯片被微软的 Azure 用于云端的初创公司）。 


就低时延和每秒推理速度而言，TSP 架构比其他任何架构都快。受软件优先思想的启发，TSP 架构提供了可实现计算灵活性和大规模并行性的新范例，无须传统 GPU 和 CPU 架构的同步开销。该体系结构既可以支持传统的机器学习模型，也可以支持新的机器学习模型，并且目前已在 x86 和非 x86 系统的客户站点上运行。 


Groq 的新体系结构比此前的体系结构更简单，是专门为满足计算机视觉、深度学习推理和其他 AI 相关工作负载的性能要求而设计的。该设计中执行计划放在软件中进行，从而释放了宝贵的硅片面积，而这些面积原本专用于动态指令执行。该体系结构提供的严格控制机制可以满足确定性处理的需求，这对于安全性和准确性需求极高的应用特别有价值。 


Groq 新架构的特色是先构建编译器原型（而不是硬件原型），然后围绕该编译器构建硬件体系结构。最终的 TSP 具有简化的硬件设计，所有执行计划都在软件中进行，即软件定义硬件（见图 4.16）。软件从本质上协调所有所需的数据流和时序，以确保计算不会发生停顿，从而使时延和性能可预测。编译器可以控制执行和电源配置文件，因此可以在编译时准确预测运行每个模型的代价，可以静态和动态地完全控制芯片。这样就省去了原来需用硬件设计的控制器、缓冲器等电路元素，释放了不少硅片面积。最主要的功能是消除了大多数体系结构所需的计算和结果传递之间的同步步骤。无开销的同步意味着可以大规模部署模型而不会产生时延，而这种时延是数据中心的主要问题。 


![img](https://pic3.zhimg.com/v2-ca7748170878993499596c42ca5b05f8.webp)

尽管软件定义硬件的概念类似于 FPGA，但 Groq 强调其 TSP 不是 FPGA，它没有查找表，但可以每个时钟周期更改（重构）芯片的功能。 


**3.速度极快的 AI 芯片**


在过去几年 AI 芯片初创公司涌现的大潮中，以色列初创公司 Habana Labs 脱颖而出。这家成立于 2016 年的公司在以色列特拉维夫和美国加利福尼亚州圣何塞市等地设有办事处，在全球拥有 120 多名员工。2019 年 12 月中旬，英特尔正式宣布以约 20 亿美元的价格收购了这家公司。 


Habana Labs 于 2018 年秋发布了 Goya HL-1000 芯片，用于处理推理，并以低功率达到了创纪录的性能，令许多人印象深刻。基于其 Goya HL-1000 处理器的 Habana PCIe 卡在 ResNet50 推理基准测试中达到每秒 15,000 帧图像的吞吐量，时延为 1.3 ms，而功率仅为 100 W。当时，该公司声称要推出第二款名为 Gaudi 的用于训练的芯片，宣称将是「世界上最快的 AI 芯片」，可以挑战英伟达在训练用 AI 芯片市场的独占地位。2019 年 7 月，该公司兑现了这一承诺，宣布了一款速度非常快的芯片。 


该公司的第一款芯片 Goya HL-1000 针对的是数据中心中相对简单的推理任务，而新的 Gaudi 芯片主要针对人工神经网络训练，这一市场现在由英伟达主导。Gaudi 使用 8 个张量处理器核，每个都有专用的片上存储器、通用矩阵乘法（General Matrix Multiply，GEMM）数学引擎、PCIe4.0 和 32 GB 大带宽存储器，如图 4.17 所示。此外，它还具有业界首个在 AI 芯片上实现以太网远程直接存储器访问（RDMA 和 RoCE）的片上实现，可提供 10×100 Gbit/s 或 20×50 Gbit/s 通信链路，可扩展到数千个加速器。 


![img](https://pic2.zhimg.com/v2-9aa68ba6dd4d6c6900083089843035df.webp)

上述方法体现了系统级思维：除了芯片的海量片上互连带宽、一系列系统构建模块，Habana Labs 还有称为 SynapseAI 的 AI 软件套件，其开发和执行平台还包括多串流执行环境、AI 库和 JIT 编译器，它们提供层融合和编译，以提高硬件利用率和效率。这款芯片基于行业标准，如 RoCE、PCIe 4.0 和开放式计算加速器模块，以及对流行的 AI 框架的支持，都旨在简化大规模生产环境中的采用和部署。 


Gaudi 芯片以相对小的批量（大小为 64）使用 ResNet50 基准来训练图像网络，可达到每秒 1650 帧图像的速度，功率为 140 W，大约是英伟达高性能 GPU 功率的 1/2。也许更重要的是，Gaudi 片上 RoCE 结构的可扩展性非常好，理论上可以容纳数千个节点，是英伟达 V100 芯片的 3～4 倍。 


作为第一家宣布推出高端训练芯片的半导体初创公司，Habana Labs 设计了令人印象深刻的平台、强大的 AI 软件套件和系统构建模块，所有这些都基于行业标准。 


**4.第一个采用存内计算的商用 AI 芯片**


数据在存储器和 CPU 之间来回移动需要功耗，现已证明从存储器调用数据可能比实际对其进行计算的功耗要高几百倍。这就是设置缓存的原因，但缓存也需要耗费 CPU 内大量管理操作。对于简单的操作，如位移或 AND 操作，可以将计算能力迁移到 DRAM 内，这样数据就不必来回穿梭了。这是一种存内计算（PIM）技术（见第 7 章）。 


半导体初创公司 UPMEM 在 2019 年 8 月宣布推出了存内计算加速解决方案，该解决方案能够将大数据和 AI 应用运行速度提高 20 倍，将时延降至原来的 1/100，能耗降至原来的 1/10。通过允许计算直接发生在存有数据的存储器芯片中，UPMEM 基于存储器的技术可以大大加速数据密集型应用。同时，UPMEM 也利用现有服务器架构和内存技术减少了数据移动。 


存内计算的思路是，当数据仍存储在 DRAM 中时，应该就近完成许多简单的整数或浮点运算，而无须将其推送到 CPU 操作再返回。如果数据可以保留在存储器并进行更新，则可以节省时间、降低功耗，而不会影响结果。或者，如果结果被发送回主存储器并且最终将 XOR 应用于存储器中的数据，则可以减少 CPU 上的计算，以释放主 CPU 核来执行其他与计算相关的操作。 


UPMEM 所做的是在 DRAM 工艺节点上开发内置于 DRAM 芯片的数据流处理单元（Dataflow Processing Unit，DPU）。每个 DPU 都在 64 MB 的 DRAM 中，这样 8 GB 的存储器模块中就有 128 个 DPU。它们在制造的时候内置于 DRAM 中，其逻辑并不像常规 ASIC 逻辑那样密集，因此制造良率很高。DPU 以 32 位指令集架构构建，具有大量优化功能，如基本逻辑指令、移位和旋转指令。编程模型基于 C 语言的库来处理所有常见问题，UPMEM 的预期是大多数应用程序只需要几百行代码，几个人的小团队也只需要 2～4 周就可以更新软件。 


最终的 DRAM 芯片主体仍然是 DRAM，相比之下 DPU 尺寸可以忽略不计。为此，UPMEM 创建了一个与 ASIC 类似的逻辑单元框架，包含 SRAM IP 核和实现流程。UPMEM 的最终目标是将这些 DPU 添加到其他未经修改的 DRAM 设计中。 


DPU 是一个 14 级交错流水线处理器，它使用 24 个硬件线程来实现更好的可扩展性。对于多线程代码，DPU 产生的指令吞吐量是每个时钟周期 1 条指令。每个 DPU 内部有 88 KB 的 SRAM，分为 64 KB 的 WRAM（数据缓存）和 24 KB 的 IRAM（指令缓存），用 DMA 指令在 DRAM 和 WRAM/IRAM 之间移动数据。这些 DMA 引擎是自治的，UPMEM 状态对流水线性能几乎没有影响。这里没有实际的硬件「缓存」，该公司声称实际缓存的线程太多，因此依靠高效的 DRAM 引擎和紧密耦合的 SRAM 组来完成这项工作。 


UPMEM 提出的是一种标准的 DDR4 RDIMM 模块产品，每 64 MB 的内存可以访问其中一个 DPU，DPU 内置于 DRAM 本身，使用内存工艺节点来制造。DIMM 模块上的 PIM-DRAM 采用了标准 DRAM 封装，实物形态如图 4.18 所示。UPMEM 声称正在制造 4 GB DDR4-2400 芯片，每 512 MB 嵌入 8 个 DPU，DPU 以 500 MHz 运行。UPMEM 计划把 16 块这种 4 GB 芯片放入 DDR4 模块中，这个模块就可提供 8 GB 内存，内置 128 个 DPU。该公司的目标是最终生产 128 GB 的模块，总共嵌入 2048 个 DPU。DPU 与其 64 MB 内存之间的有效带宽为 1 GB/s，这意味着 DPU 与内存之间有效带宽为 2 TB/s。 


![img](https://pic2.zhimg.com/v2-104c239607d322cd1f4a11b6d9b3df70.webp)

UPMEM 已获得 PIM 处理器和技术的专利，正在与 20nm 工艺的内存供应商合作。基于该技术添加 DPU 内核只增加非常小的裸片面积，并且可以在 2～3 个金属层内启用。与当时领先的 CPU 实现相比，DPU 可以实现 10 倍的总能效和可扩展性提升。表 4.3 是 UPMEM 发布的 PIM-DRAM 和普通 DRAM 的性能比较。 


![img](https://pic2.zhimg.com/v2-b7b2754b5e5501612e8a27f5677800a4.webp)

UPMEM 为客户提供软件模拟和硬件 FPGA 验证模拟器。实际的 PIM-DRAM 模块在 2020 年批量发货，该公司目标是用 PIM-DRAM 模块替换服务器中的 DRAM 模块。 


备案号:YX01jbkWgwB9w7Lle

