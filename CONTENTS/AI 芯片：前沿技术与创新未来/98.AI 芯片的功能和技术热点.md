## 98.AI 芯片的功能和技术热点
如果从芯片设计的角度来看，现在用于深度学习的 AI 芯片（包括 CPU、GPU、FPGA、ASIC）为了实现深度学习的庞大乘积累加运算，都包含大量乘积累加单元来进行并行计算。为了达到并行计算的高性能，芯片面积越做越大，由此带来了成本和散热等十分严重且难以解决的问题。另外，深度学习 AI 芯片软件编程的成熟度、芯片的安全性、神经网络的稳定性等问题也都未能得到很好的解决。 


因此，很多研究人员正在积极研究如何在原有基础上不断改进和完善这类 AI 芯片。本书前面几章已经介绍了一些改进深度学习的计算范式，如近似计算、模拟计算、随机计算、存内计算等；也探讨了使用光子而非传统的电子电路，甚至采用量子计算原理来实现深度学习算法。 


迄今为止，大多数 AI 专用芯片都针对视觉和语音应用而开发，主要用于图像识别、图像分类、语音识别等应用，这是因为卷积神经网络（CNN）技术在这两个领域应用的相对成熟。在自动驾驶汽车和机器人中，实现眼睛和耳朵的智能将对人类社会产生很大影响，但实际上它只是 AI 真正潜力的一小部分。 


CNN 运算属于硬件密集型操作。为了降低 CPU 的计算负担，CNN 目前大多使用专用电路来设计实现，即将作为 NPU 或深度学习处理单元（Deep Learning Processing Unit，DPU）的硅 IP 核集成到主 SoC 芯片里面。近年来，其他的神经网络算法，如循环神经网络（RNN）和 RBM 也已被实现为 AI 加速器芯片。由于 CNN 与 RNN 硬件具有不同的最优架构，因此需要分别为 CNN 和 RNN 设计构建异构加速器。 


随着基于 DNN 的 AI 技术开始出现明显变化，AI 的应用领域已开始迅速扩大。2015～2019 年，AI 技术主要集中在图像识别和语音识别领域，而从 2020 年开始，语言理解和语言抽象方面预计会得到快速发展。这方面的技术除了 RNN 的变种 LSTM 和 GRU 等之外，主要将依赖基于注意力机制的 Transformer 模型  [304]  。Transformer 在训练和参数数量上都具有很大的优势。结合 Transformer 的语言模型，可以理解句子的上下文，随着上下文的变化来理解同一个单词的含义。图像、语音识别与语言理解、抽象结合在一起，形成多模态运行，将把 AI 推到一个新的发展阶段（见图 17.3）。 


![img](https://pic1.zhimg.com/v2-58c8d3447c395a99ea31ae8b6e8aad2a.webp)

来自 Transformer 的双向编码器表征（Bidirectional Encoder Representations from Transformers，BERT）是谷歌「AI 语言」项目组研究人员的新成果。它将 Transformer 模型的双向训练应用于语言建模。结果表明，双向训练的语言模型比单向训练的语言模型对上下文有更深的理解。基于 Transformer 模型的运算现在基本都在 GPU 上运行，为此各家公司也在用 GPU 作为硬件平台的基础上，以 BERT 为基准，在运行 Transformer 模型的算法性能方面开始了竞争。2020 年 5 月，微软的 DeepSpeed 获得了最快的 BERT 训练纪录：在 1024 个 NVIDIA V100 GPU 上运行的速度为 44 分钟。他们采用了一种随机 Transformer 的算法。 


到目前为止，大多数使用多任务深度学习的方法通常在单一模态的情况下进行（例如，要么都是视觉任务，要么都是文本任务）。在未来，AI 芯片将集成多任务、多模态、元学习的功能，在多任务、多模态的情况下运行（见图 17.4）。它们可以用于与环境互动，如通过片上传感器和执行器用于 AIoT，并能够自行作出规划。而一个理想的 AI 芯片将可以与其他 AI 芯片进行高度协作（如内嵌多个 AI 芯片的机器人），并具有像人类一样的情境感知功能，这个时候的 AI 芯片已经具备自我思维能力。 


许多实际应用都需要使用多个 DNN，如 GAN 及自动驾驶汽车的应用。在自动驾驶中，需要不同的 DNN 进行对象跟踪、定位和分段。因此，在单片 AI 芯片中，需要实现同时处理多个 DNN 的功能。这方面最新的成果是韩国 KAIST 在 2020 年 ISSCC 上发布的 GANPU 芯片（见第 11 章）。但是，这款芯片仅实现了两个 DNN 的运算。如果要实现多个 DNN 运算，需要同时使用多组 AI 芯片，很可能将组成一个 AI 芯片阵列，以应对更智能、更先进的应用，尤其是机器人的应用。 


![img](https://pic2.zhimg.com/v2-457152485b1adabe5b17f4ef86e50183.webp)

神经网络的下一步是使观察和与现实世界互动产生的复杂时空数据变得有意义。为此，需要使用注意力机制来解析时空信息，以便能够理解复杂的指令及其与环境的关系。因此，注意力是善于理解具有序列数据的神经网络的最重要组成部分之一，无论处理对象是视频序列、现实生活中的动作序列还是输入的语音或文本序列。我们的大脑会在多个层面上实现注意力，以便仅选择要处理的重要信息，并消除手头任务不需要的大量背景信息。基于注意力机制的 Transformer 模型未来将会有很大的发展空间。 


一般来说，处理时间序列的模型并不是「硬件友好」的，算法的递归属性将导致很复杂的数据依赖性。尽管近年来有一些研究人员尝试做了芯片原型，要对 RNN、LSTM、GRU 等进行硬件加速（如专用于稀疏 LSTM 的加速器 ESE  [305]  、组合 CNN 和 RNN 的可重构 DNPU  [306]  、利用 RNN 增量网络更新方法的 DeltaRNN  [307]  等），但是从芯片架构角度来说，它们并不能与算法做到很好的匹配。然而，Transformer 所需的计算量更小，并且更适合现代机器学习硬件架构，从而可以将训练速度提高一个数量级。目前，已有一种基于 Transformer 的芯片实现方案采用了存内计算的范式，这也是未来几年的趋势。 


另外一个在深度学习领域很有前景的是被称为稀疏型专家混合体（Mixture of Experts，MoE）的模型  [308]  。它可以构建非常大容量的模型，对于任何给定的样例，模型中只有一部分被激活（如 2048 个「专家」中只有两三个「专家」被激活）。而目前大多数深度学习模型，对每个样例来说，都是整个模型被激活。现在，MoE 模型也已用于 GNN。 


备案号:YX01jbkWgwB9w7Lle

