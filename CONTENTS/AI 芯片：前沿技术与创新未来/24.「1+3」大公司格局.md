## 24.「1+3」大公司格局
处于「1+3」霸主地位的 4 家大公司，指持续领先的英伟达，以及谷歌、英特尔和微软这 3 家追赶的巨头公司。 


#### 英伟达


近 10 年来兴起的 AI 热潮，是由深度学习技术的出现和发展推动的。而正是有了 GPU 作为硬件基础，深度学习算法的优势才能得到展现。英伟达作为 GPU 的最早发明者和最大制造商，在 AI 芯片领域的霸主地位毋庸置疑。虽然英伟达也面临英特尔和高通等公司的竞争，但它仍在市场上领先一大截。 


英伟达最早的产品是 GPU，主要用于 PC 视频游戏和 Sega、Xbox 及 PS3 等视频游戏系统。近年来，英伟达开发了被称为「AI 芯片」的新一代 GPU 加速器，如使用台积电 12nm FFN 工艺技术制造的 Tesla V100 加速器，以及问世不久的使用 7nm 工艺制造的 A100。英伟达的目标是将每个新一代 GPU 引擎的应用吞吐量提高 10 倍，中长期目标是使吞吐量比 2017 年的水平提高 100 倍。 


英伟达还开发了大带宽接口技术 NVLink，其吞吐量为 300 GB/s。英伟达还基于 CUDA 开发了针对深度学习和人工智能生态系统的硬件平台。CUDA 是一个并行计算平台，可以让成千上万个第三方机构为深度学习开发算法，它具有基于 AI 的编程模型，可用于机器学习，并支持 cuDNN 神经网络库。英伟达的深度学习软件开发工具包（SDK）可以与多种框架替换，包括 TensorFlow、Caffe2、Microsoft Cognitive Toolkit、Theano、Chainer、DL4J、Keras、MatConvNet、Minerva、PaddlePaddle 等。 


与 20 世纪 90 年代的微软 Windows 系统相似，NVIDIA GPU Cloud 是各种软件和硬件的互连接口，并支持各种云上的 AI 训练，包括亚马逊弹性云（Amazon Elastic Compute Cloud，Amazon EC2）P3 实例、阿里云、百度云、谷歌云平台、腾讯云、微软 Azure 等，使 AI 环境中的各种新应用变得可行。迄今为止，已有 1200 多家公司开始使用英伟达的推理平台，包括亚马逊、微软、Facebook、谷歌、阿里巴巴、百度、京东、科大讯飞、腾讯（为微信）等。 


英伟达为边缘侧计算项目开发了处理器和应用解决方案，其中自动驾驶汽车是英伟达重点关注的领域之一。其他应用领域包括医疗和工业领域，其中医疗保健有很大的市场潜力。下面介绍英伟达的 3 款具有代表意义的 AI 芯片。 


**1.Volta 芯片**


Volta 是由英伟达开发的新 GPU 微体系结构，用来代替 Pascal。它于 2013 年 3 月首次在英伟达的 GPU 芯片路线图中亮相。然而，Volta 的第一个产品直到 2017 年 5 月才公布。这是英伟达首款采用张量核（Tensor Core）的芯片，这种专门设计的内核具有优于常规 CUDA 内核的深度学习性能，进一步提升了专业化程度，从根本上加速了神经网络的训练和推理。Volta 主要用于云端服务器，性能达 125 TFLOPS（fp16），功率为 300 W。 


Volta 的张量核专门进行神经网络计算，而传统的 GPU 内核用于图形计算。传统的 GPU 内核可以非常快速地执行常见的图形操作。而对于神经网络，基本模块是矩阵乘法和加法。英伟达的新张量核融合了乘法与加法，它将两个 4×4 fp16 矩阵相乘，然后将结果加到 4×4 fp16 或 fp32 矩阵中。因此，除了可以在并行运行的 V100 上获得 5120 个核的并行性之外，每个核本身也可以并行运行许多操作。结果是 Volta 与 Pascal 相比，训练的速度提高了 12 倍，推理的速度提高了 6 倍。 


Volta 的推出显然是为了与谷歌的 TPU 芯片竞争。但是英伟达还宣布了 TensorRT、TensorFlow 和 Caffe 的编译器，旨在优化 GPU 运行时的性能。该编译器不仅可以提高效率，还可以大大减小时延。英伟达还通过将其深度学习加速器设计和代码开源来更直接地响应来自定制推理芯片的竞争。 


Volta 架构含有 5120 个 CUDA 内核及 640 个新的张量核。张量核配有大容量（20 MB）寄存器文件，16 GB HBM2 RAM（900 GB/s）和 300 GB/s NVLink（I/O）。 


英伟达希望 Volta 能够在汽车和机器人行业发挥重要作用，希望基于 Volta 的处理器和电路板成为需要训练或推理技术的设备的核心，具体包括机器人（尤其是使用英伟达 Isaac 机器人仿真工具包模拟的机器人）及各种形状和大小的自动驾驶汽车。Volta 应用的其中一个案例，是用于空中客车公司（Airbus）一种可以垂直起飞、可搭载两名乘客飞行长达 112.7 km 的小型自动驾驶飞机项目。 


**2.Xavier 芯片**


英伟达还针对边缘侧的汽车应用推出了 Xavier 芯片。根据英伟达的公告，Xavier 已将 8 个 64 位 ARMv8-A 内核（可能是 Denver 的改进版本）和 512 核 Volta 架构 GPU 集成到一块芯片中。除了 CPU 内核外，还包含一个用于 AI 处理的 8 位整数矩阵运算电路，以及用于 AI 处理的专用深度学习加速器、全新计算机视觉加速器、全新 8K HDR 视频处理器、第 4 代双倍速率（DDR4）存储器等。使用台积电 12nm FFN 工艺，裸片面积为 350 mm  2  的 Xavier 芯片版图如图 4.3 所示。 


Xavier 拥有超过 90 亿个晶体管，凝聚着 2000 多名英伟达工程师 4 年时间的努力，研发投入高达 20 亿美元。Xavier 的技术细节非常复杂，但总体来说它可提供更高的处理能力，运行功率更低，每秒可运行 30 万亿次计算（30 TOPS），功耗却仅为 30 W，能效比上一代架构高出 15 倍。 


Xavier 是 NVIDIA DRIVE Pegasus AI 计算平台的重要组成部分。2017 年 10 月，英伟达发布了 Pegasus——全球首款致力于推进 L5 级全自动驾驶出租车的 AI 车载超级计算机，它的外形只有车牌大小，性能却相当于满满一后备箱的个人计算机。Pegasus 配备了两块 Xavier 系统级芯片和两块下一代英伟达 GPU，每秒可运行 320 万亿次计算，功耗为 500 W。 


目前，已有超过 25 家公司正在使用英伟达技术来开发全自动驾驶出租车，而 Pegasus 将为其量产提供支撑。 


![img](https://pic3.zhimg.com/v2-81bf258e3dbc0d17d396df0180585cfd.webp)

**3.A100 芯片**


2020 年 5 月，英伟达推出了新一代的 GPU，架构以 Ampere（安培）命名。这款 A100 GPU 芯片包含了超过 540 亿个晶体管，成为世界上晶体管容量最大的 7nm 处理器之一。由于采用了 7nm 工艺及新的架构，并且集成了几项最新的技术，A100 芯片在性能和能效方面都有了新的飞跃。无论是在 AI 的训练还是推理上，它的总体性能都比前一代产品（Volta）提高了 20 倍。 


为了满足其巨大的计算吞吐量，A100 GPU 拥有 40 GB 的高速 HBM2 内存及业界领先的 1.6 TB/s 的内存带宽，比 V100 快 1.7 倍。A100 上的 40 MB L2 缓存几乎是 Tesla V100 的 7 倍，并提供了 2 倍以上的 L2 缓存读取带宽。A100 中的流式处理器包含更大、更快的 L1 高速缓存和共享内存单元的组合，提供的容量是 Volta V100 GPU 的 1.5 倍。 


A100 配备了专用的硬件单元，包括通用算术核「CUDA 核」和可以更快进行矩阵运算的第 3 代张量核，更多的视频解码器单元、JPEG 解码器和光流加速器。各种 CUDA 库都使用所有这些来加速 AI 应用。 


A100 使用了一个新的技术，称为多实例 GPU（Multi-Instance GPU，MIG），可以把张量核安全地划分为多达 7 个用于 CUDA 应用程序的独立 GPU，从而为数据中心的多个用户提供独立的 GPU 资源，以满足不同的性能需求。A100 还引入了细粒度的稀疏结构，这是一种新颖的方法，对所允许的稀疏性模式施加了约束，可将 DNN 的计算吞吐量提高 1 倍。 


A100 支持两种用于深度学习的浮点算术格式：tf32（tensor float32）和 bf16（bfloat16）。tf32 的动态范围相当于 fp32（单精度浮点），精度相当于 fp16（半精度浮点）。使用 int8 进行推理时，处理性能通过 tf32 训练可达到最大 312 TFLOPS，推理可达到最大 1248 TOPS。 


此外，英伟达现在正在尝试模拟计算。数字计算将几乎所有信息（包括数字）存储为一系列 0 或 1；而模拟计算将允许直接编码各种值，如 0.2 或 0.7。这可以达成更高效的计算，因为数字可以被更简洁地表示。本书第 6 章将专门介绍模拟计算。 


#### 谷歌


谷歌一直在 AI 芯片方面紧跟英伟达，积极开发支持机器学习和 AI 的硬件功能。谷歌的 TensorFlow 已经成为深度学习的主流框架之一。 


由于第三方供应商提供的 AI 处理器不能满足谷歌在性能及功耗方面的要求，这迫使谷歌设计自己的处理器——TPU。TPU 是谷歌耗费 5 年的开发成果，目的是开发一种新的处理器架构以优化机器学习、AI 技术，从而提高谷歌语音搜索性能。TPU 可以解决云端的训练和推理问题，被视为 CPU 和 GPU 技术的强大挑战者，也为其他云服务提供商提供了开发自己 AI 加速器 ASIC 的样板。 


谷歌的第一代 AI 芯片 TPU 于 2016 年首次在「Google I/O」会议上发布。2017 年 4 月谷歌发表了一篇详细描述其性能和架构的论文。与使用通用 CPU 和 GPU 的神经网络计算相比，TPUv1 带来了 15～30 倍的性能提升（峰值运算速率为 92 TOPS）和 30～80 倍的能效提升。它以较低成本支持谷歌的许多服务。2016 年击败韩国顶级职业棋手李世石、2017 年击败围棋世界冠军柯洁的 AlphaGo，就采用了 TPUv1。图 4.4 是谷歌 TPUv1 的框图，图中百分比表示该模块占芯片总面积的比例。 


图 4.4 中巨大的 MAC 阵列引人瞩目，它可在一个周期内执行 64k 个运算，是 TPUv1 的核心单元。它包含 256×256（64k）个乘积累加运算器，可以执行 8 位乘法和有符号或无符号整数的加法。16 位结果（8 位乘法结果）被累加到矩阵乘法单元下面的 4 MB 32 位累加器中。这是一个包含 256 个元素的 32 位累加器矩阵单元（或更确切地说是矢量单元）。 


TPUv1 的每个时钟周期生成 256 个元素的一个部分和。由于时钟工作在 700 MHz，所以该芯片的性能为 2 次操作（乘积和累加）×65,536（64k）×700 MHz= 91,750 GOPS，即约 92 TOPS。 


![img](https://pic3.zhimg.com/v2-4365c93d579b600b80ffce5c81675f8f.webp)

谷歌在 MAC 中采用脉动阵列结构，通过数据流动、加载权重来计算乘积累加。最初的 TPUv1 芯片产品工作在 700 MHz，功耗为 40 W，采用 28nm 工艺，用于推理。谷歌的第二代处理器（TPUv2）用在 Google Cloud 中，被称为「云 TPU」，用于加速大量的机器学习和人工智能工作负载，包括训练和推理。一个 TPUv2 内核具有 16 GB HBM，600 GB/s 内存带宽，可进行 32 位浮点运算（标量单元和混合乘法单元），性能达到 45 TFLOPS。 


四核 TPUv2 配置有 64 GB HBM，2400 GB/s 内存带宽，性能高达 180 TFLOPS，可以支持 TensorFlow。一个 TPU 群集（称为 TPU pod）拥有 64 个 TPUv2 四核配置，可提供高达 11.5 PFLOPS 的性能和高达 4 TB 的 HBM。根据谷歌公布的数据，如果用 GPU 来训练用于大规模翻译的算法，需要用 32 块高性能 GPU 花一整天的时间，而完成同样的训练，只需要 1/8 个 TPU 群集（8 个 TPUv2 四核配置）运行一下午就可以了。 


TPUv2 与 TPUv1 之间的主要区别在于矩阵乘积的计算从 8 位整数（int8）变为 16 位半精度浮点（fp16）运算。因此，TPUv2 不仅可用于推理，还可用于训练。int8 的准确性可用于推理，但训练需要 fp16 的准确性。TPUv1 在训练阶段使用 GPU。 


自 2016 年首次发布 TPUv1、2017 年发布 TPUv2 之后，谷歌又在 2018 年 3 月的 Google I/O 大会上推出了 TPUv3。TPUv3 每个群集的机架数量是 TPUv2 的 2 倍；每个机架的云 TPU 数量是原来的 2 倍。据官方数据，TPUv3 群集的性能可达 TPUv2 的 8 倍，高达 100 PFLOPS。表 4.1 给出了 TPU 三个版本的指标对照。 


![img](https://pic4.zhimg.com/v2-5bfe418efb3a412c0d12c12aeb1dfecf.webp)

解决功耗达到 200 W 的 AI 芯片的散热问题，一直是很大的挑战。引人瞩目的是，谷歌的 TPUv3 采用了最新的液体冷却技术（见图 4.5）。 


![img](https://pic2.zhimg.com/v2-b4a808037cb7735aec93d9d8a262df02.webp)

谷歌的第四代 TPU ASIC（TPUv4）提供的矩阵乘法 TFLOP 比 TPUv3 高 2.7 倍，显著增加了内存带宽，这受益于更先进的互连技术。TPUv4 是在最新举办的业界标准 MLPerf 基准测试竞赛中亮相的。MLPerf 是一个由 70 多家公司和学术机构组成的联盟，它提供的套件用于 AI 性能基准测试。 


这次测试结果表明，谷歌打造了世界上最快的深度学习训练超级计算机。在使用 ImageNet 数据集对 ResNet50 v1.5 进行至少 75.90% 分类精度的训练中，256 个 TPUv4 在 1.82 分钟内就完成了训练任务。这几乎与将 768 个 NVIDIA A100 图形卡和 192 个 AMD Epyc 7742 CPU 核组合在一起的速度（用时为 1.06 分钟）一样快。这个结果显示，TPUv4 群集在目标检测、图像分类、自然语言处理、机器翻译和推荐基准方面超过了 TPUv3 的性能。 


TensorFlow 也有专门用于移动设备的框架 TensorFlow Lite，支持开发者使用手机等移动设备的 GPU 来提高模型推理速度。2018 年夏天，谷歌也推出了相应的 ASIC 芯片 Edge TPU，专门用于在边缘侧运行 TensorFlow Lite 的深度学习模型。Edge TPU 是一块小型 ASIC，功耗非常低，适合在移动设备或 IoT 等设备中进行深度学习推理。Edge TPU 现已装载到谷歌的「Coral 开发板」上供开发者使用。 


#### 英特尔


在英特尔公布 2019 年后半年推出的产品——神经网络处理器（Neural Network Processor，NNP）细节的大约两年前，英伟达就已宣布推出 NVIDIA Tesla V100。尽管如此，英特尔一直紧追不舍。英特尔、Facebook 和英伟达这 3 家公司的总部虽然彼此距离不到 30 分钟车程，但它们之间一直在进行激烈的竞争。 


英特尔曾获得了数据中心 CPU 设计大奖，它占有市场份额高的原因包括 Xeon 处理器系列的优势，再加上 x86 指令集在数据中心中得到了广泛使用。Xeon Phi 处理器采用英特尔的集成多核架构。最初，英特尔使用其 Xeon Phi 架构在 AI 芯片市场上与英伟达竞争，该架构使用数十个 Atom 核来加速深度学习任务。然而，英特尔意识到仅有 Xeon Phi 无法赶上英伟达，而英伟达似乎每年都会在性能方面取得重大飞跃。 


因此，英特尔开始寻找其他选择，具体行动包括收购 Altera 将 FPGA 引入其产品阵容，收购 Movidius 用于其嵌入式视觉处理器，收购 Mobileye 用于其自动驾驶芯片，并在 2016 年以大约 4 亿美元的价格收购了 Nervana Systems 用于其专用神经网络处理器。2018 年 8 月，英特尔又收购了 Vertex.ai，这是一家开发平台无关 AI 模型套件的初创公司。2019 年 12 月，英特尔以 20 亿美元收购了以色列 AI 芯片初创公司 Habana Labs。此外，英特尔已经开始研发自己的专用 GPU，正在与 Facebook 合作开发高度优化的 AI 推理芯片。英特尔还开发了一种新的微架构，将取代 Xeon Phi 的 Exascale 计算应用，并增强了英特尔支持的特有 AI 芯片架构。英特尔同时还致力于研发基于神经形态计算的类脑芯片（研发了 Loihi，见第 5 章）和量子计算芯片。 


英特尔将所有这些布局称为人工智能的「整体性方法」。然而，该公司可能还希望避免再次将所有选项押在单一架构上，以避免像 Xeon Phi 那样在 AI 芯片市场中落后于英伟达。另一方面，这种分散策略也可能使开发人员感到困惑，因为他们不会知道英特尔将长期支持哪种技术。 


从 2018 年开始，英特尔相继推出了两个深度学习加速器专用芯片系列。在 2018 年 5 月第一次 AI 开发者大会上，英特尔发布了 Nervana NNP-T，用于云端训练，代号为「Spring Crest」。这是英特尔收购 Nervana 之后推出的第一个 NNP。相比理论峰值性能，该芯片更优先考虑内存带宽和计算利用率。在 2019 年消费电子展（CES）上，英特尔又发布了 Nervana NNP-I（Spring Hill）用于 AI 推理。该芯片是使用硅中介层的 2.5D 芯片（见图 4.6）。 


![img](https://pic1.zhimg.com/v2-fbd804cb01b2e176fc8e44915243c869.webp)

英特尔推出的 Nervana 系列芯片针对图像识别进行了优化。它的架构与其他芯片截然不同：没有标准的缓存层次结构，而片上存储器则由软件直接管理。此外，由于其高速的片上和片外互连，它能够在多块芯片上分配神经网络参数，实现非常高的并行性。 


然而，在收购了 Habana Labs 之后仅两个月，英特尔根据一些大客户的反馈意见，停止了其现有的 Nervana 神经网络处理器产品线的开发（将继续支持客户对 NNP-I 推理芯片的承诺，但 NNP-T 训练芯片将停产）。相反，从 2020 年开始，英特尔专注于 Habana Labs 的技术，其推理芯片（Goya）和训练芯片（Gaudi）已投放市场并获得了关注。Habana Labs 产品线为推理和训练提供了统一的、高度可编程的体系结构，具有强大的战略优势。 


在 2017 年，英特尔收购了 Mobileye，开发专用于自动驾驶汽车的 AI 芯片，占领了 L1/L2 高级驾驶辅助系统（Advanced Driver Assistance System，ADAS）的主要市场。Mobileye 的 EyeQ4 已经量产，性能为 2 TOPS，功率为 6 W，而 2020 年量产的 EyeQ5 性能达 12 TOPS，功率为 5 W。汽车领域的竞争尤为激烈，许多初创公司都专注于这个领域。Mobileye 采用基于道路体验管理（Road Experience Management，REM）和即时定位与地图构建（Simultaneous Localization And Mapping，SLAM）技术的数据分析能力，其商业模式可用于从数据中获取价值。然而，潜在的利益冲突在于谁控制由车辆产生的数据。汽车公司及 Uber、滴滴打车等车辆服务运营商都希望利用 AI 从数据中产生价值。这些公司对与英特尔、苹果、谷歌或其他数据聚合公司共享数据的兴趣很低。 


英特尔也在 FPGA 方面做了很多创新工作，以更好地用于 AI。例如，FPGA 加上芯粒（Chiplet）后封装在一块芯片里，可以成为很多应用的 AI 引擎。英特尔现在热衷于研发芯粒技术（见第 15 章），这是因为英特尔自己有很先进的工艺生产线可以研究、试验和实现。 


图 4.7 为英特尔在 FPGA 方面创新工作的一个例子，其中的 FPGA 是 Intel Stratix 10 系列 FPGA 芯片，周围用嵌入式多芯片互连桥接（Embedded Multi-Die Interconnect Bridge，EMIB）技术组合了多个 AI 芯粒。英特尔在很早之前就已经开始布局「胶接封装」的 EMIB 技术，这种技术可以将不同工艺的芯片以 2.5D 方式集成并封装到一起，从而降低生产成本和芯片设计难度。例如，CPU、GPU 核、AI 芯片、FPGA 采用 10nm 工艺，而 I/O 单元、通信单元则是 14nm 工艺，内存使用 22nm 工艺，英特尔可以将这 3 种不同工艺集成到一个产品上。EMIB 技术已在业界得到高度评价。 


![img](https://pic1.zhimg.com/v2-744f40d5f1fe06c504ad3ae4001eacb9.webp)

#### 微软


微软有很强的研究团队，在 AI 方面有很多创新。微软的 Brainwave 平台项目通过云提供 AI 处理加速服务。该平台的硬件没有选用 ASIC，而是选用 FPGA 来实现灵活的 NPU，可以加速 DNN 推理。由于基于 FPGA，Brainwave 可以快速开发并在每次改进后重新映射到 FPGA，与新发现保持同步，并与快速变化的 AI 算法的要求保持同步。 


Brainwave 通过使用可编程芯片组成的互连可配置计算层来增强 CPU，从而改变云计算。通过在大型循环神经网络（RNN）上应用最先进的 GPU，实现了时延和吞吐量超过一个数量级的改进。Brainwave 无须批量处理即可提供实时 AI 和超低时延，可降低软件开销和复杂性。 


高性能、可对精度自适应的 FPGA 软处理器是该系统的核心，它可以实现高达 39.5 TFLOPS 的有效性能。使用 FPGA 意味着它可以灵活地进行持续的创新和改进，使基础设施面向未来。Brainwave 项目在数据中心规模的计算结构上利用 FPGA，因此可以将单个 DNN 模型部署为可扩展的硬件微服务，利用多个 FPGA 创建可实时处理大量数据的 Web 级服务。 


作为旨在加速实时 AI 计算的硬件架构，Brainwave 能够部署在 Azure 云端及边缘侧设备上，帮助用户实现低成本的实时 AI 计算。 


#### 其他一些著名公司的 AI 芯片


**1.特斯拉**


AI 芯片的一个重要应用领域是自动驾驶汽车。谷歌和 Waymo 已经于 2018 年 12 月开始在美国提供使用自动驾驶汽车的付费出租车服务。有许多厂商瞄准了这个领域，包括以其自动刹车图像处理芯片闻名的以色列 Mobileye（已被英特尔收购）。此外，AI 芯片还将广泛应用于内置监控摄像头和制造设备故障诊断等应用。 


自动驾驶汽车是预计在几年内就能成为现实的最有前途、最具颠覆性的 AI 创新之一。特斯拉（Tesla）及许多其他市场参与者正在大力投资支持自动驾驶汽车的 AI 技术。特斯拉并不是一家专门做半导体芯片的公司，但是它后来自己建立了研发团队，决定自己开发高度专业化的芯片。 


特斯拉的全自动驾驶计算机（Full Self-Driving Computer，FSD）芯片于 2019 年 4 月推出（见图 4.8），包括内部开发的两个神经网络加速器（Neural Network Accelerator，NNA），还集成了第三方 IP 核，包括 GPU 和基于 ARM 的处理器子系统。FSD 使用整数算术，加法为 32 位，乘法为 8 位。每个 FSD 芯片的 NNA 都包括 96×96 乘法和加法硬件单元的阵列，并且可以每秒执行 72 万亿次运算（72 TOPS），而不会耗尽汽车的电池电量。 


![img](https://pic2.zhimg.com/v2-fbae2eaa10ae03a4287961bf04f6b595.webp)

特斯拉声称其基于包含 FSD 芯片的电路板，比之前基于英伟达 GPU 的解决方案快 21 倍，同时开发成本也降低了 20%。根据特斯拉的说法，一旦软件赶上，FSD 芯片将能够支持自动驾驶，安装进特斯拉生产线上的每一台电动车中。 


特斯拉自动驾驶芯片的总设计师彼得·班农（Pete Bannon）曾参与领导了苹果从 A5 到 A9 的 iPhone 芯片的开发。他为特斯拉带来了软硬件深度结合的「苹果作风」。而在 2018 年 4 月之前，特斯拉的芯片开发由来自 AMD 的顶尖芯片架构师吉姆·凯勒（Jim Keller）领导。 


这款自研芯片于 2017 年秋季前设计完成。从公布的一系列参数来看，无疑是相当强大的一款芯片：性能达 144 TOPS（完胜竞争对手英伟达目前最先进的 Xavier 芯片 21 TOPS 的表现）。另一个亮点是，其神经网络加速达到令人难以置信的 2300 f/s，即能够处理 8 个摄像头同时工作产生的每秒 2300 帧的图像输入，相当于每秒 25 亿像素，而之前采用英伟达的硬件只能处理 110 f/s，性能提升了 21 倍左右。FSD 芯片由三星代工生产，采用 14nm FinFET（Fin Field-Effect Transistor，鳍式场效应晶体管）工艺，裸片面积为 260 mm  2  ，晶体管达 60 亿个。在不影响车辆能耗和续航的前提下，FSD 能将安全性和自动化水平提升到新的等级。总体来看，这款 14nm 芯片的设计特别针对神经网络进行了架构优化以降低能耗和成本，尤其针对大量图像和视频的处理。 


FSD 发布后，特斯拉陆续放弃了英伟达提供的图像处理解决方案。特斯拉 CEO 埃隆·马斯克（Elon Musk）表示，特斯拉的芯片是世界上最好的（自动驾驶）芯片，远超其他竞争对手。与此同时，特斯拉也已经将下一代芯片的工作进行了一半，并表示下一代芯片可能比现有版本性能提高 3 倍，且有望在两年内推出。 


特斯拉非常看重在自动驾驶汽车中使用视觉传感器，而不看好目前很流行的激光雷达（Light Detection and Ranging，LiDar）技术。特斯拉的 AI 软件能够处理来自视觉传感器的车道线、交通信号、行人等信息，将收到的视觉信息进行 3D 渲染，将视频输入也纳入深度感知范围，将这些信号与已知的物体进行匹配再最终作出决策。然而，业界还是更多采用 LiDar 方案或 LiDar+ 计算机视觉的方案，因为目前的纯计算机视觉方案在安全性保障上仍有一定风险。 


**2.Facebook**


Facebook 原来从多家半导体制造商那里采购用于推理的 AI 芯片，但现在正在推动专门用于 NLP 的 AI 芯片的内部开发。Facebook 的深度学习推理处理量非常大，每天执行超过 200 万亿次预测和 60 亿次语言翻译。如果 CPU 和 GPU 处理如此大量的推理工作，则功耗非常高。因此，Facebook 采用了一种能够以低功耗进行推理的 AI 芯片，这款芯片的能效超过 5 TOPS/W，仅支持 8 位整数运算和 fp16（半精度）浮点运算，类似于采用谷歌的 TPU 芯片进行推理。 


Facebook 正与 4 家公司合作，即世界语科技（Esperanto Technologies）、英特尔、迈威科技集团（Marvell Technology Group）和高通，开发用于推理的 ASIC。 


Facebook 研发了全景特征金字塔网络（Panoptic Feature Pyramid Network）视觉识别系统，可以通过多层路径提取图像特征，由多层路径特征生成输出图像，其中包含图像中的全部实例并输出分类结果。这些分类不仅是目标本身的分类，还包含背景、材质等分类，如草地、沙地、树林等。这种分类对自动驾驶会很有用。在翻译应用上，该系统采用了许多网络架构上的创新，如注意力机制、轻量卷积、动态卷积等，实现基于语境的动态卷积网络内核。Facebook 在 ICML2019 上展示了其最新卷积模型。 


2019 年 7 月，Facebook 又发布了一个新的 AI 神经网络架构：在神经网络中加入一个结构化存储层，这个存储层能够处理超大规模的语言建模任务，在不增加计算成本的基础上，通过扩充网络容量、增加参数数量，有效提升了性能，特别适用于自然语言处理任务。这些新的网络架构思路与出任 Facebook 副总裁及 AI 首席科学家的 DNN 主要奠基人 Yann LeCun 分不开。他一定会给 Facebook 的自然语言处理 AI 芯片带来很多创新思路和新的功能。 


**3.苹果**


当前，边缘侧设备中嵌入式 AI 芯片的市场扩张已经全面展开。智能手机已经越来越多地使用 SoC，包括用于 AI 处理的专用电路。第一家切入该领域的公司可能就是苹果。2017 年秋季苹果发布的 iPhone X，其应用处理器 A11 Bionic 仿生芯片配备了 AI 芯片「神经引擎」（Neural Engine）。 


A12 Bionic 芯片于 2018 年 9 月 13 日发布，是苹果公司设计的 64 位 SoC。它是全球第一款量产出货的 7nm 工艺芯片，首先被搭载于 iPhone XS、iPhone XS Max 和 iPhone XR 中，其内部模块如图 4.9 所示。 


![img](https://pic2.zhimg.com/v2-167959d5af8b35ddd8195345ff683d52.webp)

这款芯片采用了第二代八核 AI 神经引擎，采用 2 个高性能核 Vortex 与 4 个高能效核 Tempest 搭配的六核 CPU 设计。根据 Geekbench4 提供的数据，A12 Bionic 的 CPU 单核与多核性能较上一代（A11 Bionic）均提升了约 15%，与官方宣传数据相符，节能最高达 50%。此外，这款芯片除了神经引擎之外，还有单独的图像信号处理（Image Signal Processing，ISP）模块，这意味着可以使用 AI 来完成图像处理的一些功能。A12 芯片的神经引擎能够以每秒 5 万亿次（5 TOPS）的速度执行实时处理，以 8 位操作；其芯片面积为 83.27 mm  2  ，与 A11 大致相同。 


2019 年 9 月，苹果又发布了 A13 Bionic。这款芯片为 64 位架构，采用 7nm 工艺，内含 85 亿个晶体管，面积为 98.5 mm  2  （比 A12 的面积约增加了 20%）；其神经引擎拥有 8 个核，速度最高提升 20%，能耗最多可降低 15%，为三摄系统、人脸 ID、增强现实类 App 和更多功能提供驱动力。这款芯片还新增了两个深度学习加速器，能以最高达过去 6 倍的速度执行矩阵运算，CPU 每秒可进行 1 万亿次运算。为发挥 A13 Bionic 的机器学习能力，开发者可以利用苹果的机器学习框架 Core ML3 与其机器学习控制器配合，自动为 CPU、GPU 或神经引擎分配任务。苹果当时宣称 A13 Bionic 芯片是智能手机上最快的芯片，远远领先安卓生态中的竞争对手。 


苹果的下一代 AI 芯片 A14 Bionic 将采用台积电的 5nm 工艺，将成为更快、更省电的芯片。 


**4.亚马逊**


2018 年 11 月，亚马逊旗下负责云业务的亚马逊网络服务公司发布了其 AI 芯片 AWS Inferentia。它是一个使用学习型 AI 的推理处理芯片，于 2019 年末在亚马逊的云服务上「服役」。与通常用于加速 AI 的 GPU 相比，其目标是将运营成本降低一个数量级。 


亚马逊 AWS Inferentia 是针对 AWS 定制设计的深度学习推理芯片，旨在以极低成本提供高吞吐量、低时延推理性能。AWS Inferentia 支持 TensorFlow、Apache MXNet 和 PyTorch 深度学习框架及使用 ONNX 格式的模型。 


AWS Inferentia 可以对复杂模型进行快速预测，可提供数百 TOPS 的推理吞吐量。如果组合多个该芯片，吞吐量则能达到数千 TOPS。AWS Inferentia 可以与 Amazon SageMaker、Amazon EC2 和 Amazon Elastic Inference 一起使用。 


自 2015 年初以来，亚马逊已经拥有一个专注于 AWS 的定制 ASIC 团队，在此之前，亚马逊与合作伙伴一起构建了专门的解决方案。在 AWS re:Invent 2016 大会上，亚马逊展示了多年来已安装在所有 AWS 服务器上的 AWS 定制 ASIC。 


AWS Inferentia 在 int8 数据类型上提供 32～512 TOPS 的可扩展性能，专注于成本敏感的机器学习推理部署中常见的大规模部署。它支持近线性横向扩展，基于 DRAM 等而不是 HBM 等高容量技术；采用 int8 以获得最佳性能，同时还支持混合精度 fp16 和 bf16 以实现兼容性，因此客户不必费力地量化他们在 fp16、fp32 或 bf16 中训练过的神经网络，也不需要费力决定使用哪种数据类型用于特定工作负载。与 Amazon Go、Alexa、Rekognition 和 SageMaker 等更广泛的亚马逊和 AWS 机器学习服务团队协同，AWS Inferetia 硬件和软件可以满足广泛的推理用例和先进的神经网络。 


#### 三位世界级 AI 科学家


这里我们需要提一下三位赫赫有名的世界级 AI 专家。2019 年 3 月 27 日，美国计算机学会（Association for Computing Machinery，ACM）宣布 Yoshua Bengio、Geoffrey Hinton 和 Yann LeCun 获得当年度图灵奖，获奖理由是这三人推动了神经网络的关键突破。 


图灵奖被誉为「计算机界的诺贝尔奖」，是计算机科学领域的最高奖，由 ACM 于 1966 年设置，设立目的之一是纪念著名的计算机科学先驱艾伦·图灵（Alan M. Turing）。历届获奖者均在计算机领域作出了持久、重大的先进技术贡献。 


该届图灵奖的三位获奖者中，Bengio 是加拿大蒙特利尔大学教授、加拿大魁北克省人工智能研究所（Mila）的科学主任，并创立了 Element AI；Hinton 任谷歌副总裁、工程研究员，加拿大矢量学院首席科学顾问及多伦多大学名誉教授；Yann LeCun 则是纽约大学的教授，也是 Facebook 的副总裁兼 AI 首席科学家。 


深度学习领域的一个关键性事件发生在 2012 年，当时在加拿大多伦多大学的 Hinton 和两名研究生首次参加 ImageNet 图像识别比赛，通过其 CNN（AlexNet）一举夺冠，准确率高达 85%，比第二名（SVM 分类方法）高了超过 10%，使得 CNN 受到众多研究者瞩目。2013 年，谷歌收购了 Hinton 和这两名研究生组成的初创公司。此后，Hinton 一直为谷歌工作，帮助谷歌设计 TPU 芯片。Hinton 和他的团队提出了胶囊网络（Capsule Network），以更好地制定物体的等级表示和关系。由于对图像有着更好的上下文理解，胶囊网络已经显示出在抵御对抗性攻击方面的能力。 


Facebook 2013 年聘请 Yann LeCun 出任副总裁及 AI 研究部主管（后转任 AI 首席科学家）。Bengio 尚未加入任何科技巨头公司，不过他是微软的顾问，并与初创公司合作，将深度学习应用于药物发现等领域。也就是说，前面提到的三家巨头公司谷歌、微软和 Facebook，分别有这三位科学家的身影。这三位图灵奖得主走向了不同的方向，但仍然是合作者和朋友。 


备案号:YX01jbkWgwB9w7Lle

