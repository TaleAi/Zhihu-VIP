## 4.AI 芯片的历史
从工业革命开始，机器已经逐步取代了人类的重复性手工劳动和繁重的体力劳动。如今，人类的部分脑力劳动和知识性工作也逐渐可以被具有人工智能的机器所取代。 


这种带有 AI 芯片的「智能机器」具有强大的计算处理和学习能力，可以自主操作。智能机器不但可以仿照人类肌肉执行任务，而且将成为大脑功能的替代者。这种智能机器将会越来越普及，性能也将会不断提高，它将被广泛地用于人脸识别、汽车驾驶、艺术作品创作、新材料合成、新药开发、医学诊断、机器人及人们的日常生活中。可以这样说，在未来的 25 年内，AI 将无处不在，它将可以胜任大部分人类正在从事的工作。因此，在未来，现在人类大部分的工作岗位都将被这种智能机器「抢走」，也就算不上是危言耸听了。 


如前所述，AI 芯片的发展主要依赖两个领域的创新和演进：一个是模仿人脑建立起来的数学模型和算法，这与脑生物神经学和计算机科学相关；另一个是半导体集成电路，简称芯片。 


芯片是上述智能机器的「核心」，是运行这种机器最关键的「引擎」。从 1957 年第一块芯片发明以来，芯片技术得到了极为迅速的发展。从一开始几个晶体管的集成，到今天已经可以在一块很小的硅基芯片上集成几十亿甚至几百亿个晶体管，这是非常了不起的人类智慧的结晶，也是人类历史上的奇迹。海量晶体管的集成使大量运算得以进行，从而大大提高了芯片的运算能力。今天用于深度学习的图形处理器（Graphics Processing Unit，GPU）芯片，已经可以达到 100 TFLOPS（每秒 100 万亿次浮点运算）以上的运算速度，是 20 世纪 90 年代初超级计算机 Cray-3 运算速度（16 GFLOPS，即每秒 160 亿次浮点运算）的 6000 多倍。 


世界上第一块芯片是由美国德州仪器（TI）公司的杰克·基尔比（Jack Kilby）发明的。现已退休的台积电（TSMC）创始人张忠谋在他的自传里，记述了当时他和芯片发明人杰克·基尔比一起并肩工作的情景。「我入职 TI 不久，结识了一位和我几乎同时加入的同事，他有一个令人印象深刻的外表，高得出奇（超过两米）、瘦削，最显眼的是巨大的头颅。那时他 30 多岁，……正想把好几个晶体管、二极管，加上电阻器，组成一个线路放在同一粒硅晶片上。……老实说，那时要我做一个晶体管都有困难，把好几个晶体管再加别的电子元器件放在同一粒硅晶片上，还要它们同时起作用，简直是匪夷所思。」但杰克后来成功了。就是这块芯片，奠定了后来信息革命的基础。 


AI 想法的产生，以及后来神经网络数学模型和算法的发展，一路上伴随着半导体芯片的演进过程。虽然在二十世纪三四十年代就有人在研究人类的脑功能，并试图建立一种数学模型，但没有产生较大影响。直到 1957 年，模拟人脑的感知器（Perceptron）的发明被看作是第一个「人工神经网络」方面的突破。感知器是当时就职于康奈尔航空实验室的法兰克·罗森布拉特（Frank Rosenblatt）发明的。作为最简单的前向人工神经网络形式，感知器虽然结构简单，但它拥有学习能力，能不断进化从而解决更为复杂的问题。 


到了 20 世纪 80 年代，已经有人研究构建模仿大脑运行的硬件（不是从逻辑学的角度来看，而是根据感知器的模型），这是构建人工神经元和神经网络的初步尝试。AI 走出实验室，走向商品化，形成了巨大的投资热潮。当时人们预测，如果使用感知器构建深度神经网络（Deep Neural Network，DNN），可以在计算机上构建类人的推理和学习机制。不少公司不但使用数字电路，还尝试使用模拟电路来制作神经网络的芯片。但是，当时硬件的计算能力非常低，无法使这类网络模型得到有效应用。从 20 世纪 90 年代初开始，AI 科技泡沫逐渐破灭。 


现在不少人认为当时的 AI 泡沫破灭是因为没有像如今最红火的「深度学习」（即深度神经网络）算法这样的好算法。其实，最关键的还是当时半导体芯片的运算能力没有跟上。「深度学习」这样的模型和算法，其实早在 20 世纪 80 年代就有了。1986 年，杰弗里·辛顿（Geoffrey E. Hinton）与同事们一起，探索了如何显著改善多层神经网络（即深度神经网络）的性能，使用被称为反向传播（误差反向传播方法）的算法，并发表了他们的划时代论文。1989 年，当时还在贝尔实验室的杨立昆（Yann LeCun）和其他研究人员一起开发了可以通过训练来识别手写邮政编码的神经网络，证明了能够在现实世界中应用这一新技术。但在那个时期，他们训练一个深度学习卷积神经网络（Convolutional Neural Network，CNN）需要 3 天的时间，因此无法投入实际应用。 


这也说明了算法再好，如果没有足够的计算能力，也就是高性能的芯片，AI 就无法得到实际应用，只能在实验室里被束之高阁。 


2009 年以来，AI 又一次受到人们的关注，飞速发展，这是由 GPU 芯片带动的。虽然英伟达（NVIDIA）公司在 1999 年就发明了 GPU，但从来没有人把它用于深度学习。一直到 2009 年，斯坦福大学的拉亚特·莱纳（Rajat Raina）、阿南德·马德哈文（Anand Madhavan）及吴恩达（Andrew Y. Ng）共同发表了一篇突破性的论文，介绍了如何利用现代 GPU 远超过多核中央处理器（Central Processing Unit，CPU）的计算能力（超过 70 倍），把 AI 训练时间从几周缩短到了几小时。 


2012 年，一切都发生了变化。一系列极具影响力的论文发表，如亚历克斯·克里泽夫斯基（Alex Krizhevsky）、伊利·萨茨凯（Ilye Sutskever）和辛顿的《具有深度卷积神经网络的 ImageNet 分类》一文，就展示了他们在 ImageNet 图像识别挑战赛上取得的成果。其他很多实验室也已经在从事类似工作。在这一年结束之前，深度学习已成为美国《纽约时报》的头版，并且迅速成为人工智能中最知名的技术。 


之后，深度学习在图像识别、语音识别方面的实验结果逐年得到改善，直到超过人类的识别率，引起人们的极大关注，再次掀起了 AI 热潮。由此可见，AI 与半导体芯片的发展是紧密联系在一起的，没有 GPU 等半导体芯片近年来的迅猛发展，AI 就不会像今天这样炙手可热。从图 1.1 中可以看到按时间顺序列出的 AI 和半导体芯片的演进历程对照。 


虽然感知器和第一块芯片都是在 1957 年发明的，但 AI 和半导体芯片这两条路发展到现在，还不能说很匹配。芯片的运算能力还远远无法满足算法的运算需求。非营利组织 OpenAI 的资深研究员最近指出，芯片性能需要每年提高 10 倍，才能满足训练 DNN 的需求。这个需求是巨大的，但目前看来还难以满足。 


![img](https://pic3.zhimg.com/v2-f378e147a57181a82ed2f31b9def112a.webp)

  



备案号:YX01jbkWgwB9w7Lle

