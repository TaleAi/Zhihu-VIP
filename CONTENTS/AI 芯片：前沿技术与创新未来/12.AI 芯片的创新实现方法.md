## 12.AI 芯片的创新实现方法
要做出高水准的 AI 芯片，不但要有 AI 模型和算法，更重要的是采用先进的电路设计及工艺技术。下面介绍一些目前正在尝试的新的设计和实现方法。 


（1）脉动式电路（见图 2.13）。谷歌的 TPU 在关键矩阵乘法单元使用了脉动式设计，使得运算过程中的数据像流水线一样「流过」各个处理器，使这些数据可被重复使用而不用每次都返回存储器，从而大大降低了功耗（每个乘积累加单元的功耗可以降低到原来的 1/10～1/5）。让数据模仿人体心脏中血液的脉动式流动（心脏相当于存储器，血管相当于处理器阵列及连接），这种技术曾经在二十世纪七八十年代流行过一段时间，现在有了用武之地。现在微软和一些初创公司纷纷采用这个技术，有的研究人员在此基础上还作了进一步创新。 


（2）异步电路。异步电路没有固定时钟，而是由事件驱动的，对于芯片电路设计者来说，这毫无疑问是非常具有吸引力的方法，因为可以大大提高芯片性能并降低功耗。但是，没有时钟来同步，也会在某些场合造成混乱，增加了电路设计难度，需要有高超的电路设计技巧。另外，现在还没有很好的异步设计 EDA 工具。因此，目前比较好的办法是采用折中的方式：在模块中仍采用时钟，即还是同步电路，但是各个模块的时钟可以不一样，在系统集成的总体上是异步的，称为全局异步局部同步（Globally Asynchronous and Locally Synchronous，GALS）技术，图 2.14 即为 GALS 的架构，也有人把它称为自定时（Self-Timing）技术。现在，已经至少有一家 AI 芯片初创公司成功使用了这种技术，做出了高性能、低功耗的 AI 芯片。 


（3）新的散热方式。例如，谷歌最近发布的 TPUv3 采用了水冷散热的方法。这需要非常高的工艺水平。 


![img](https://pic1.zhimg.com/v2-3d4eb5d26334455f94f53edbf435e729.webp)

![img](https://pic3.zhimg.com/v2-76f58d1b012cfae9842a68f3ec934f5d.webp)

（4）大芯片。芯片集成很多个（如 32～64 个）大处理器核，或集成海量的小处理器核。有家日本 AI 公司已在试验由几十个大核组成、面积超过 8 cm×8 cm 的芯片，里面封装了 4 片裸片（即未经封装的半导体芯片），而每片裸片的面积达到了 32.2 mm×23.5 mm。而英伟达用于 AI 加速的 GV100 GPU 集成了 211 亿个晶体管，内核面积达 815 mm  2  。这些芯片几乎已经达到了目前半导体芯片制造工艺所能达到的极限（注：目前所能制造的裸片最大面积为 8 cm  2  ，这是因为受到 12 英寸晶圆掩模版区域的限制）。芯片面积大，就可以容纳很多处理单元和片上存储器，而不用把信号传递到片外 DRAM 来回存取，可以大大降低时延和功耗。但是，大面积的芯片对于制造来说是个巨大的挑战，在芯片上跨越长距离需要跨越芯片的长布线，产生时序收敛问题；同时，成品率会大大降低；另外，热量耗散也是个大问题。 


（5）晶圆级集成。这个比上述大芯片还要大得多，把 AI 系统做在整个晶圆上，而不是做在从晶圆切割下来的芯片上。当芯片的特征尺寸已经很难做到 3nm 以下时，晶圆级集成和大芯片一样是一种反向思维：既然不能把晶体管再做小，那就把整片面积做到晶圆这么大，以覆盖复杂度极高的 AI 系统。这种方法以前在欧洲由政府资助的研究机构尝试过，其「类脑」研究项目中集成了海量神经元和突触，以尽量向人脑的结构靠拢。时隔很多年之后，2019 年 8 月，初创公司 Cerebras 使用晶圆级集成技术实现了深度学习加速器，从而大大加快了神经网络的训练速度。晶圆级集成的面积，要比 CPU、GPU 等常见的芯片面积至少大 50 倍。 


（6）芯粒（Chiplet）。这种芯片美国硅谷有家公司已经做了很多年，开始的应用是射频识别货物标签。2018 年以来英特尔、超威（AMD）、英伟达等都加入了研发这种芯片的行列。因为面积非常小（常见的为 6 mm  2  左右，有的甚至小于 1 mm  2  ），耗电极低（甚至可做成自供电），芯粒很适合在物联网中应用，或者进行模块化设计，即把多个芯粒组装成一个较大的高性能芯片。英特尔的应用是把 CPU、存储器、片上 FPGA 等做成芯粒，然后组装在较大的硅片上，从而可以缩短连线、降低成本并提高芯片开发速度。 


（7）新一代存储器（如 RRAM 等）及存内计算和近数据处理（Near Data Processing，NDP）等新技术，距离大批量商用已经不远。一些新的 AI 芯片初创公司也已经采用了这些方法。因此，对芯片设计者来说，需要为相应的电路设计作好准备。 


（8）自供电电路。当把电路的功耗做到非常小的时候（如毫瓦级甚至微瓦级），可以考虑采用外部绿色能源，如太阳能、人走路的动能、散射无线电波的能量等来给芯片供电，使该芯片基本上不用电池或普通电源。 


（9）模拟电路。未来模拟计算计算范式的兴起，需要大量模拟电路或数模混合电路的设计。使用模拟电路可以大大提高速度，但是系统就不能带有其他数字电路，因为如果需要一直进行模数转换（Analog-to-Digital Converter，ADC），就会得不偿失。近年来，一些大公司（如英伟达）已经组织团队开始研究模拟计算，谷歌也已经对此表示了强烈兴趣，而 IBM 的一个项目已经做出模拟计算的芯片原型。 


（10）亚阈值电路。是使晶体管工作在低电压（未达到其开启电压）的一种低功耗方法。英特尔及一些研究所都曾做出亚阈值电路的初步样片，它的工作频率不高，但对于一些神经网络应用已经绰绰有余。 


（11）细胞神经网络。最初的想法是在 20 世纪 50 年代被提出的细胞自动机，用于模拟生物系统中细胞间的自组织现象；20 世纪 80 年代，随着人工神经网络的兴起，美国加利福尼亚大学伯克利分校的蔡少棠教授提出了细胞神经网络模型。30 多年后，当年他的博士生在美国硅谷成立了一家初创公司 Gyrfalcon，按照细胞神经网络的原理做出了一款 AI 芯片，据称这款 AI 芯片的能效远远高于英特尔的传统 CPU 和英伟达的 GPU，可用于训练和推理。 


（12）多值逻辑电路。即运算机制不再是通常数字电路的二进制，可以有 3 个值（如 +1、0、-1）或更多的值（有人提议多达 8 个值）来组成逻辑运算。这个思路虽然已经提出多年，但是一直没有得到真正应用。忆阻器及 RRAM 等阻变存储器的出现，为这种方法打开了应用之门，因为忆阻器本身就可以进行多值逻辑的运算。 


（13）单片 3D 芯片。三星已经把 3D 存储器做得相当成熟了。下一步要实现的是把逻辑电路和存储器用 3D 方式集成在一起，目前较成熟的是采用所谓的 2.5D 方法，即还不是真正的 3D，而是通过一块中介层来集成（见图 2.15）。未来的目标是把模拟电路（包括射频电路）、数字逻辑电路、存储器、传感器等全部堆积起来，成为一块单片 3D 芯片。这种工艺技术也为存内计算和近数据处理的有效实现开辟了道路。 


（14）硅光芯片。很有前景的光子计算的核心就是硅光芯片。英特尔已经把硅光技术做得相当成熟，开始时用于光通信模块。2017 年，麻省理工学院（MIT）的研究人员提出了光神经网络，即让矩阵乘法在光域完成。AI 芯片如果用硅光来实现，在性能上至少会提升 2 个数量级。目前，全球至少有 5 家初创公司或研究团队在研发 AI 硅光芯片。 


![img](https://pic4.zhimg.com/v2-24d96386312b02179d368a0fca039a66.webp)

（15）量子芯片。这种芯片主要用于量子计算，现在有很多公司都在研究、开发并做出了样片：有的使用硅光技术；有的使用普通的 CMOS 数字芯片，在常温下进行量子计算；更多的是设计成在超低温环境下工作的超导芯片。使用量子芯片作为量子神经网络的 AI 加速器，是一个很有前景的想法。最近几年以来，已经有不少研究人员在研究如何用量子芯片加速线性代数运算，或作为图形模型中训练和推理的采样器等；也有研究人员利用伊辛模型（Ising Model），做成了量子神经元和量子突触等。 


图 2.16 展示了一些 AI 芯片的创新实现方法，分别以高性能或低功耗为重点。 


![img](https://pic4.zhimg.com/v2-5b0b56cdce2feeb9ad3cc489b875b2cd.webp)

  



备案号:YX01jbkWgwB9w7Lle

