## 11.AI 芯片的创新计算范式
新的深度学习模型和算法是指在 DNN 这个基本模型的基础上，为了进一步提高性能而提出的创新思路和算法。例如，开发出 AlphaGo 程序击败了顶尖围棋棋手的 DeepMind 公司，一直在不断改进原有的深度学习模型。该公司组建了脑神经科学研究小组，试图进一步提高人类对于大脑机制的认识程度，开发更详细的模仿人脑的脑神经模型和算法。 


如果要用芯片实现 AI 算法，首先要判别这些算法是否「硬件友好」，即这些算法是否容易转换成硬件架构，因为有的算法可以用软件非常好地进行运算，而它的机制却很难做成芯片。例如，强化学习算法需要不断与环境状况互动，对环境适应一次次试错，用电路来实现时，就存在试错时间太长造成的信号时延问题，不易用硬件架构实现。因此，直至近年才出现了基于深度强化学习的 AI 芯片（见第 12 章）。同样，需要经过大量迭代运算才会收敛，或本身基于时间序列处理的算法（如 RNN、LSTM 等），也需要设计专门的电路以减少计算时间。因而到目前为止，只有很少人专注于 RNN 的硬件加速并做成了芯片。 


最近出现的一些深度学习算法在计算和精度之间作了折中，以提高计算的能效，如使用二值权重以执行高效计算的方法。当神经网络的尺寸很小并且冗余连接被修剪了很多时（尤其是去除接近零的权重），通常可以实现较高能效。修剪以迭代方式完成，即在修剪之后重新训练权重，然后再次修剪低权重的边。在每次迭代中，来自前一阶段的训练权重被用于下一阶段。结果是密集网络可以被稀疏化，变成具有连接数少得多的网络。 


有的新算法把编码和压缩量化相结合。量化的目标是减少表示每个连接的比特数。这种方法将所需的存储空间减小到原来的网络模型的几十分之一，同时不会降低精度，这样就降低了功耗，提高了边缘计算的能力。最近还有人尝试使用原来用于视频压缩的 MPEG 标准的压缩方法来对神经网络进行压缩。 


前文提过的类脑芯片，是基于一种新的计算范式——神经形态计算而实现的。一个很有意义的方向是深入研究大脑神经的运作机制，将其映射为一个细致的数学模型，开发直接针对神经网络定制的芯片。值得注意的是，人类大脑并没有把软件和硬件分开，而现在的计算机科学学科把硬件和软件分得非常清楚，虽然这种区别从计算机维护的角度来看是有好处的，但这也是计算机效率无法与人类大脑相比拟的根源。现在，硬件和软件紧密集成在受大脑启发的计算模型中，推动类脑计算领域取得了很大进展。 


除了模仿人脑功能的神经形态计算之外，模拟计算、近似计算、随机计算、存内计算、可逆计算、光子计算和储备池计算等计算范式在 AI 算法中已得到较多应用，尤其是深度学习算法的硬件架构实现上，这些范式已经发挥出独特的优点，可以在很大程度上降低电路的复杂性、减少计算时间，从而减小芯片面积并降低功耗。 


自然计算、仿生计算是受到大自然物理、化学、生物启发所诞生的新的计算范式。而量子计算又采用了完全不一样的计算方式，有望在未来几年内有更大的进展。 


图 2.12 中列出了 AI 芯片的一些创新计算范式。 


![img](https://pic4.zhimg.com/v2-66efbf6fdd0ced4b3d237eeed84bb65a.webp)

  



备案号:YX01jbkWgwB9w7Lle

