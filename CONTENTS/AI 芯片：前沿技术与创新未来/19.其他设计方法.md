## 19.其他设计方法
随着深度学习技术的深入发展，过去几年中业界出现了大量的改进算法，同时也已经提出了许多硬件架构  [51]  。 


#### 卷积分解方法


谷歌研究人员发表的 MobileNet 模型  [52]  基于深度可分离卷积，是一种分解卷积的形式，它将标准卷积分解为深度卷积和 1×1 卷积，称为逐点卷积。对于 MobileNet，深度卷积将单个卷积核应用于每个输入通道。然后，逐点卷积应用 1×1 卷积来组合输出深度卷积。标准卷积既进行过滤，也一步将输入组合到一组新的输出中。深度可分离的卷积将其分为两层，一层用于过滤，另一层用于组合。这种分解具有极大减少计算量和减小模型大小的效果。 


MobileNet 后来又有了新的更新，现在已经有了 v3（第 3 版）。2019 年发表的这个新一代 MobileNet 中  [53]  ，组合了互补搜索技术，即通过硬件感知的神经架构搜索（NAS）与 NetAdapt 算法相结合的方式，适应手机 CPU，然后通过新颖的架构进行改进。 


#### 提前终止方法


这种方法根据输入将结果输出到网络中间，而不再进行进一步处理（提前终止）。它可以根据输入动态更改网络结构（动态计算图），大大减少了平均处理时间。 


DNN 正在变得越来越深，中间网络层数的增加可以进一步提高精度，但却使得运行时延增加、功耗快速升高。在某些需要实时应用的场合（如移动通信），时延是一个重要因素。为了降低这些增加的成本，哈佛大学学者提出了 BranchyNet  [54]  。在这种神经网络架构中，边侧分支被添加到主神经网络分支，以允许某些测试样本提前退出。这种新颖的架构利用了这样的观察：通常情况下，在 DNN 的早期阶段学习的特征可以正确地推理出数据总体中的大部分，通过在早期阶段利用预测退出这些样本，避免对所有层进行逐层处理，BranchyNet 大大减少了大多数样本的推理运行时间和能耗。BranchyNet 可以与先前的工作结合使用，如网络修剪和网络压缩。 


#### 知识蒸馏方法


知识蒸馏方法是由 Hinton 及其团队最早提出的  [55]  。复杂网络结构模型是由若干个单独模型组成的集合，或是在一些很强的约束条件下训练得到的很大的网络模型。因此，可以通过使用较大神经网络产生的输出预测来训练较小神经网络。一旦复杂网络模型训练完成，便可以用另一种训练方法——蒸馏（即精炼的意思），从功能强大的复杂模型中提取出需要配置在应用端的、缩小的模型。大型模型集合所获得的知识可以迁移到一个小型模型中。 


蒸馏模型采用的是迁移学习，通过采用预先训练好的复杂模型（教师模型）的输出作为监督信号去训练另外一个简单的网络。这个简单的网络被称为学生模型。知识蒸馏方法能令更深的模型变浅从而显著降低计算成本，但也有一些缺点，如只能用于具有 Softmax 损失函数的分类任务，这阻碍了其应用。另一个缺点是模型的假设有时太严格，其性能有时比不上其他方法。 


#### 经验测量方法


2018 年，麻省理工学院和谷歌的研究人员提出了新的推理算法——NetAdapt  [56]  ，它可以把一个预先训练过的 DNN 自动适应到一个移动终端平台，达到事先设定的功耗需求或时延需求。虽然许多算法通过减少 MAC 或权重的数量来简化网络，但这些度量还是间接的，有时不一定会直接降低功耗或减少时延。NetAdapt 使用所谓的经验测量方法，直接把功耗和时延指标的评估用于指导网络的优化过程，自动逐步简化预先训练的网络，直到满足资源预算，同时最大限度地提高准确性。 


#### 哈希算法取代矩阵乘法


美国莱斯大学的研究人员在 2020 年 3 月展示了被称为 SLIDE 的深度学习算法，可以在训练中不使用最耗时和耗能的矩阵乘法，而利用自适应稀疏性，改用哈希算法来完成  [57]  。在每次梯度更新过程中根据神经元的激活，选择性地稀疏大多数神经元，从而准确地训练神经网络，这就变成了一个搜索问题。哈希算法是 20 世纪 90 年代为互联网搜索发明的一种数据索引方法，一种近似最近邻的搜索技术，大大缩短了检索时间，也节约了内存占用空间。 


由于不需要矩阵乘法，这种算法只需使用一般 CPU，而不需要专业级的加速硬件或 GPU。演示结果表明，在 44 核 CPU 上运行直接用 C++ 编写的算法，对一个超过 1 亿个参数的神经网络进行训练，只用了 1 小时；而使用 TensorFlow，在 8 个 NVIDIA V100 GPU 上运行却需要 3.5 小时。但是，目前这个演示仅仅基于全连接网络，还没有涉及 CNN。 


#### 神经架构搜索


神经架构搜索（NAS）是近年来在 AI 算法和 AI 芯片开发者中被热烈讨论的新技术之一。DNN 是靠分层提取特征、以端到端用数据进行学习的方式来完成的。通过架构修改可以明显提高深度学习方法的性能，但搜索合适的架构本身就是一项耗时、艰难且容易出错的任务。对于复杂程度越来越高的网络架构，研发人员人工设计已经力不从心。2018 年至今，研究人员一直在进行自动化搜索过程的研究工作。NAS 是一种把架构设计自动化的方法，因此被看作是深度学习技术的下一步发展方向。如果要把 NAS 归类，NAS 可以被视为 AutoML 的一个子类别，与超参数优化和元学习都有相当类似的地方，也许可以相互借鉴。 


使用 NAS 设计的基准网络模型的一个最新例子是称为 EfficientNet 的模型系列，该模型系列与以前的 CNN 相比，参数要少几个数量级，具有更高的准确性和效率  [58]  。 


总体来看，这种设计自动化被视为定义神经网络不同组件的一组决策的搜索问题。这些决策的一些可行解决方案隐含地定义了搜索空间，搜索算法由最优化工具来定义（见图 3.18）。但是，AI 芯片开发者还需要考虑最佳的硬件配置。如果要同时有效地找到最佳的神经网络架构和硬件配置，这样的搜索空间就会非常大。研究人员使用了自然算法、仿生算法（尤其是进化算法）、强化学习算法等来发现优良的架构及硬件配置。但是，这些搜索方法需要很长的计算时间。因此，许多后续工作集中在寻找减少计算负担的方法上。自然算法和仿生算法对于 NAS 的应用，将在第 9 章里讲述。 


![img](https://pic2.zhimg.com/v2-4e7fa04d1cd0483bfb165c0d330b0f3b.webp)

  



备案号:YX01jbkWgwB9w7Lle

