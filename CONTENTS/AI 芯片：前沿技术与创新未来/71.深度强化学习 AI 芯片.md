## 71.深度强化学习 AI 芯片
强化学习算法的原理与图 12.3 中的自主层原理十分相似，同样是通过外部环境的交互作用和反馈机制产生奖励和惩罚，不断试错和纠错。因此，近年来已经有人把强化学习的机制引入有机计算中。 


虽然常规的 DNN 除了可以用于图像识别，也可用于动作控制，但是它的实时性很差，因为它需要通过网络与云端服务器连接进行远程学习。因此，DNN 很少被人用于诸如机器人之类的自主系统。 


与图像识别任务不同，实时操作在动作控制中非常重要，这就需要使用强化学习那样的新的学习技术，以在本地确定和选择正确的机器人动作。深度强化学习（DRL）是强化学习（RL）与 DNN 的组合，可以说是 AI 领域现在最热门的方向之一。它之所以声名大振，与 DeepMind 团队用它在 AlphaGo 和 AlphaZero 上大获成功是分不开的。但是在当时，DRL 都是使用 CPU、GPU 及 FPGA 实现的，还没有基于 DRL 的专用 AI 芯片。 


2019 年，Kim 等人在 ISSCC 上展示了一款适用于移动设备的 DRL 专用芯片  [221]  ，这是一款带有自主性的 AI 芯片。 


图 12.6 为使用 DRL 在环境中连续学习的自动驾驶智能体，把状态作为 DNN 的输入，而 DNN 的输出是动作，即带动汽车发动机的运行。它会反复采集运行经验并学会驾驶。DRL 的处理过程包含两个步骤：样本采集（SC）和策略更新（PU），用于动作的连续控制。首先，在 SC 步骤中，通过 DNN 推理来确定动作。输入状态、输出动作和相应的奖励组成采集经验，存储到存储器里。一旦采集到足够的经验样本，PU 步骤就开始计算损失，然后更新 DNN 的权重。计算损失的目的是把奖励最大化。 


DRL 的芯片实现存在不少挑战，其中最大的挑战是需要大量的存储器访问，如需要存储 10,000 个「经验」及其他中间数据，需要很大的存储带宽。图 12.6 所示的这款芯片中设计了多个内核，而每个内核包含了经验压缩器、可换位处理单元阵列、控制器及存储器等。该芯片使用 65nm CMOS 工艺制成，裸片面积为 16 mm  2  ，性能为 204 GFLOPS（权重精度为 16 位）。 


![img](https://pic2.zhimg.com/v2-4bc5d0d9a487aca9d7cfbd08f4a05617.webp)

  



备案号:YX01jbkWgwB9w7Lle

