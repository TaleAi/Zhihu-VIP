## 51.超参数及神经架构搜索
深度学习模型的设计是一项具有挑战性的任务，需要在输出的分类精度与运行时间、能量消耗情况之间进行不同的权衡。虽然使用更简单的 DNN 设计可以缩短整体运行时间，但在给定的底层硬件平台的情况下，这可能会显著降低利用率。 


发现高性能神经网络架构需要通过反复试验，并进行多年的广泛研究。就图像分类任务而言，最先进的 CNN 超越了深度链式结构布局，变成了越来越复杂的图拓扑结构，设计空间的这种组合爆炸使得架构的人工设计不仅费时，而且性能也可能不是最理想的。 


近年来，人们越来越关注使用算法来进行超参数优化，使架构设计过程自动化。目标可以描述为在给定搜索空间中找到最佳架构，以便在给定任务上最大化验证其精度。例如，最初致力于基于三维存储器设计的硬件加速器  [164]  ，已经利用以最小功耗为目标的 Eyeriss 模型进行设计空间探索。超参数优化工作可以针对多个设计目标进行优化，因此这是一个组合优化问题。 


超参数定义神经网络的架构。在大多数支持 CNN 的 DNN 框架中，需要为卷积层指定的超参数有卷积核数量，卷积核大小，激活函数的类型，隐藏层的数量，卷积层、池化层和正则化的顺序，隐藏神经元的数量，卷积层和池化层的结构等。 


在网络训练过程中，正则化可以帮助防止过度拟合，如使用 Dropout 的方法，按一定的概率将一部分中间层的单元暂时丢弃。与训练有关的超参数包含学习率、迭代次数等。 


上述这些超参数在哪种情况下到底取哪个值，这没有简单的答案。如果存在确定这些设置的简单方法，程序员就会构建神经网络框架，自动设置这些超参数。但神经网络的训练和成功仍然受到对超参数经验选择的敏感性（如模型架构、损失函数和优化算法等）的影响。在没有扎实的数学理论支撑的情况下，最常见的是把这些超参数通过在不同数据集上反复试验来进行调整。 


构建神经网络需要经历模型选择过程。模型的选择是非常耗时的，可能需要几分钟、几小时甚至几天来训练神经网络，并确定给定的一组超参数训练神经网络所能达到的程度。通常，建模的成功与在模型选择上花费的时间密切相关。模型选择是一个非常活跃的研究领域，涌现出了许多创新方法。如果将超参数视为一系列值组合的一个矢量，并将那些超参数的最佳神经网络得分作为目标函数，就可以考虑使用自然仿生算法来解决这些超参数的优化问题。 


在构建神经网络时，学习率可能是最重要的超参数。在每次模型权重更新时，学习率都会根据估计的误差来控制模型更改的程度。学习率的选择很具挑战性，因为学习率的值太小可能会导致很长的训练过程而陷入类似死机的状态；而值太大可能会导致学习的权重太快进入次优值，或导致训练过程不稳定。 


迄今为止，已经有不少研究人员应用自然仿生算法解决 DNN 中的参数优化、最佳神经架构搜索及网络架构进化等问题，使其通过不断「进化」而最终收敛到给定应用的最佳超参数集，从而使 DNN 的架构、算法和模型进一步完善，提高性能，而无须人工干预。这些自然仿生算法包括粒子群优化（PSO）、遗传编程、进化算法、模拟退火、蚁群优化、布谷鸟优化及强化学习方法等。下面介绍近年来的几个应用例子。 


#### 粒子群优化的应用


Fei Ye 研究了 PSO 全局和局部探索能力的优势，以找到 DNN 的最佳网络结构，即无须人工干预，并且具有更好的超参数配置，可用于网络的最终训练  [165]  。Fei Ye 提出了一种新的自动超参数选择方法，使用 PSO 结合最陡梯度下降算法来确定 DNN 的最优网络配置（网络结构和超参数）。在他所提出的方法中，网络配置被编码为一组实数 m 维矢量，作为搜索过程中 PSO 算法的个体。在搜索过程中，PSO 算法用于通过在有限搜索空间中移动的粒子，搜索最佳网络配置，并且在 PSO 的群体评估期间，使用最陡梯度下降算法来训练 DNN 分类器（以找到局部最优解决方案）。在优化方案之后，使用更多时段和 PSO 算法的最终解来执行最陡梯度下降算法，以分别训练最终的集合模型和各个 DNN 分类器。利用最陡梯度下降算法的局部搜索能力和 PSO 算法的全局搜索能力来确定接近全局最优的最优解。研究人员构建了手写字符和生物活动预测数据集，进行了一些实验，证明由 PSO 算法最终完成的网络配置所训练的 DNN 分类器，在总体性能上优于随机方法。因此，他所提出的方法可以被视为一种用于 DNN 的神经架构和参数选择的工具。 


#### 强化学习方法的应用


在强化学习方法中，智能体像人类一样，会为自己学习成功的策略，从而带来最大的长期回报。而深度强化学习（Deep Reinforcement Learning，DRL）又更进了一步，智能体直接从原始输入（如视觉）中构建和学习自己的知识，而无须任何人工设计的功能或启发式方法。这是通过 DNN 的组合来实现的。深度强化学习是几年前由 DeepMind 最早提出的，开始的时候学习并不稳定，但其研究人员通过大量实验解决了这个问题，以在许多具有挑战性的领域实现人类水平的效能。 


强化学习在开始的时候主要应用于下棋和一些计算机游戏上。然而，深度强化学习的应用范围已经大大扩展了，不但已有效应用于金融领域的股票交易和股价预测，而且也应用于自然语言处理、聊天机器人等领域。基于深度强化学习的 AI 芯片也已开发出来（见第 12 章）。 


DeepMind 的研究人员在 2017 年提出了基于群体的训练（Population Based Training，PBT）  [166]  ，这是一种简单的异步优化算法，它有效地利用既定的计算资源，对一组模型和超参数（一个群体）进行优化，以达到性能最大化。重要的是，PBT 可以发现超参数设置的时间进度（学习率延伸曲线），而不是遵循通常的优化策略，后者只是找到用于整个训练过程的单个固定的数据集。通过对典型的分布式超参数训练框架进行小规模修改，这种方法可以对模型进行稳健可靠的训练。这种方法结合了深度强化学习技术来优化超参数。 


相同的方法可以应用于机器翻译的监督学习，并且还可用于生成对抗网络（Generative Adversarial Network，GAN）的训练，可使生成图像的评价指标达到最高。在所有情况下，PBT 都会自动发现超参数时间进度和选择模型，从而实现稳定的训练和更好的最终性能。 


#### 进化算法的应用


进化算法已经有 50 多年的历史，其背后的基本思想是：问题的候选解决方案可以通过突变（随机改变一种解决方案）和交叉（混合两种解决方案以形成一个新的解决方案）的组合来逐渐使其变得更优，从而随着进化的发展而不断改善。 


尽管进化算法已经被反复应用于神经网络架构，但由此建立的图像分类器仍然不够理想，不能优于人工设计。直到谷歌的「谷歌大脑」项目组开发了一个图像分类器 AmoebaNet-A  [167]  ，它首次超越了人工设计。AmoebaNet-A 与使用更复杂的架构搜索方法发现的先进的 ImageNet 模型具有相当的精度。这种分类器基于改进了的进化算法，加入了新的「老化演变」及变异规则。在与强化学习算法的对照比较中，证明了进化可以使用相同的硬件更快地获得结果，尤其是在搜索的早期阶段，当因计算资源限制而无法长时间运行实验时非常重要。因此，进化是一种有效发现高质量架构的简单方法。 


DeepMind 的研究人员探索了有效的神经架构搜索方法  [168]  ，揭示出一个简单而强大的进化算法可以发现具有出色性能的新架构。他们的方法结合了一种新颖的分层遗传表示方案，它模仿了人工方法通常采用的模块化设计模式，以及一个支持复杂拓扑的富有表现力的搜索空间。这种基于进化算法的方法有效地发现了性能超出大量人工设计的图像分类模型架构，与现有的最佳神经架构搜索方法相比具有明显竞争力。 


洛伦佐（Pablo Ribalta Lorenzo）和雅各布·纳莱帕（Jakub Nalepa)使用模因算法（Memetic Algorithm）以数据驱动的方式进化 DNN 的拓扑结构  [169]  ，以便根据所提供的输入数据进行调整。他们使用了从高斯过程回归得到的变异方案。进化的 DNN 主要用于脑 MRI 图像中的肿瘤分割。在模因算法中，DNN 架构以有向非循环图的形式，通过使用相关的邻接矩阵来编码。 


美国得克萨斯大学奥斯汀分校的研究人员将用于深度学习的自动机器学习（AutoML）系统又向前推进了一步  [170]  。他们设计了一个名为 LEAF 的进化 AutoML 框架，不仅可以优化超参数，还可以优化网络架构和网络规模。LEAF 使用进化算法和分布式计算框架。医学图像分类和自然语言分析的实验结果表明，该框架可实现最先进的性能。LEAF 表明架构优化可以使超参数优化效果得以明显提升，并且可以同时缩小网络规模，而性能几乎没有下降。 


#### 其他自然仿生算法的应用


普里扬卡·辛格（Priyanka Singh）和布拉格·德维韦迪（Pragya Dwivedi）提出了一种新的领头羊算法  [171]  ，该算法基于羊群内移动行为的概念，称为跟随领导者（Follow The Leader，FTL）。为了评估该算法的性能，该项研究工作使用了 24 个基准函数。实验结果表明，与其他传统方法相比，该方法提出模型所产生的预测误差最小。 


安格斯·肯尼（Angus Kenny）和李晓东提出了一种称为半非相交扩展网络（Semi-disjoint Expanded Network，SdEN）的网络架构  [172]  ，以使 PSO 能够通过克服可扩展性约束来优化权重。哈桑·巴德姆（Hasan Badem）等人将人工蜂群（Artificial Bee Colony，ABC）算法与限制记忆算法相结合  [173]  ，以对由级联到 Softmax 分类层的自动编码器层组成的 DNN 结构进行优化。阿南·班哈恩斯库恩（Anan Banharnsakun）试图通过基于 ABC 算法生成的解决方案来初始化 CNN 分类器的权重，从而使错误分类降到最低  [174]  。科林斯·莱凯（Collins Leke）等人使用布谷鸟搜索优化的 DNN 模型来估计高维数据集中的缺失特征  [175]  。2020 年，为了弥补 NAS 计算成本太高的问题，王斌等人提出了一种有效的粒子群优化方法，以开发基于迁移学习思想的 CNN 架构。这种方法通过将搜索空间最小化到单个块，并在进化过程中利用训练数据的一小部分子集来评估 CNN，从而降低了计算成本  [176]  。 


深度学习方法可以从自然仿生算法获得帮助和优化，但这类算法太多，大部分仍然未被探索。例如，进化算法在深度学习中似乎很有用处，因为在 20 世纪 90 年代就已有人将其应用于人工神经网络并取得良好成果，但迄今为止只进行了有限的探索。而其他算法，如烟花算法，还没有人将其应用于深度学习领域。 


深度学习技术主要被认为是大数据分析的有效技术。也就是说，深度学习技术是通过大量未标记的训练数据才取得了巨大成功。然而，如果只有有限数量的训练数据可用，需要更强大的神经网络模型以实现增强的学习能力。因此，重要的是在未来考虑深层模型的设计，以便利用少量训练数据集中学习。这时可能就会需要使用自然仿生算法。 


到目前为止，基于自然仿生算法的超参数优化及神经架构搜索方法得到的架构，在某些任务中的表现已经优于人工设计，如在图像分类、物体检测或语义分割等领域。研究人员现在正在将其应用于改进 DNN 架构，如权重分享、修剪搜索、分类精度测量等，并应用到前人较少探索的领域，如图像恢复、语义分割、迁移学习、机器翻译、GAN、循环神经网络等。 


更有意义的研究工作或许是如何利用自然仿生算法来开发与 GAN 相关的、更前沿和更具挑战性的深层架构。自然仿生算法在这类架构上的应用，将会改善这类架构的学习机制，改善学习算法的性能，并使每个数据源形成根据所需运算进行优化的表述，进而减少训练时间。而这方面的工作目前几乎尚未开展。 


下一代的深度学习技术将更倾向于半监督和无监督学习，从而可以降低对大量标记数据的需求。由于自然仿生算法在模式聚类方面有很长的成功历史，未来的研究可能会寻求将基于深度学习的特征与进化聚类技术相结合。 


尽管学术界已经开展了不少研究工作，试图把自然仿生算法与深度学习技术集成在一起，但产业界在这方面的研究还不是很多。在接下来的几年中，随着研究工作的开展，预计自然仿生算法与深度学习的集成将会完善深度学习的架构、提高其性能并加快训练过程。为了做到这点，开发相应的 AI 芯片是非常重要的一个前提。因为要使用自然仿生算法来进行神经架构搜索和超参数优化，需要非常强的硬件处理能力。 


备案号:YX01jbkWgwB9w7Lle

