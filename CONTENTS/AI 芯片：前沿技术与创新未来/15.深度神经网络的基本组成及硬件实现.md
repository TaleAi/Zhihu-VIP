## 15.深度神经网络的基本组成及硬件实现
绝大多数 AI 芯片设计团队正在利用传统的数字电路设计技术，寻求提供硬件加速，以实现高能效、高吞吐量和低时延的 DNN 计算，同时避免牺牲神经网络精度。不管在系统级还是在芯片级，他们都大量采用异质架构，如 CPU 加 GPU、CPU 加 FPGA，或在 FPGA 里再增加「AI 引擎」（赛灵思公司）等。对 AI 芯片研发人员而言，了解并考虑这些数字加速器不断取得的进展至关重要。 


DNN 将神经元排列成一层，这样排列后的网络层多达数十层、几百层甚至更多，逐层推理输入数据的更抽象的表示，最终达成其结果，如实现翻译文本或识别图像。每层都被设计用于检测不同级别的特征，将一个级别的表示（可能是图像、文本或声音的输入数据）转换为一个更抽象级别的表示。例如，在图像识别中，输入最初以像素的形式出现；第一层检测低级特征，如边缘和曲线；第一层的输出变为第二层的输入，产生更高级别的特征，如半圆形和正方形；后一层将前一层的输出组合为一部分熟悉的对象，后续层检测对象。随着经过更多的网络层，网络会生成一个代表越来越复杂特征的激活映射。网络越深，卷积核所能响应的像素空间中的区域就越大。 


在各种神经网络类型中，卷积神经网络（CNN）被认为是计算机视觉领域最具潜力的创新之一，在分类和各种计算机视觉任务方面准确性更高。因此，现在的主流深度学习都以 CNN 作为最主要的部分，这也是现在很大一部分深度学习应用在图像识别、图像分类等领域的原因。包含 CNN 在内，目前比较流行的 DNN 及其主要特征如下。 


（1）卷积神经网络（CNN）：前馈型，权重共享，稀疏连接； 


（2）全连接（Fully Connected，FC）神经网络：前馈型，又称多层感知器（MLP）； 


（3）循环神经网络（RNN）：反馈型； 


（4）长短期记忆网络（LSTM）：反馈型，具备存储功能。 


目前大部分 DNN 的基本组成，主要是 CNN 卷积层（CONV 层）加上少量全连接层（FC 层）及池化层等，如图 3.1 所示。 


![img](https://pic4.zhimg.com/v2-f85302e7444720887064dc081d199db4.webp)

在卷积层和卷积层之间，或卷积层和全连接层之间，往往还加有起到降低特征维度等作用的池化层和归一化层。卷积层一般多达几十层，也有的达到了上千层。由于卷积层占了计算量的 90% 以上，时延和功耗都是在卷积层的计算上产生的，后面介绍的很多新的 DNN 算法和架构，都是针对卷积层进行各种优化和改进。 


如前文所述，DNN 的一个主要特点是很高的并行性，需要充分加以利用。例如在某个网络中，一个全连接层就有 1600 万个互相独立的乘法运算，在训练的时候，可以并行训练多个样例，可以把模型分到多个处理器来处理。 


典型的 CNN 是具有管道状结构的多层前馈神经网络。具体而言，每一层对前一层的输出执行计算，以生成下一层的输入。一般来说，CNN 有两种类型的输入：要测试或分类的数据（又称特征图）和权重。一方面，图像、音频文件和录制的视频都可以是使用 CNN 分类的输入数据；另一方面，网络权重通过在包含与被测试输入相似的输入的数据集上训练 CNN 而生成。 


网络层的数量、层内和层之间的特定互连、权重的精确值及阈值行为相结合，给出了整个网络对输入的响应，通常需要多达数千万的权重来指定神经元之间的广泛互连。在实践中，人们会使用反向传播过程离线训练 CNN。然后，离线训练出的 CNN 用于前馈过程执行识别任务。前馈过程的速度是很重要的。当网络最终用于推理时，权重在系统中作为新输入时通常保持不变。层中的每个神经元执行独立计算（将每个输入乘以相关权重，再把这些乘积加起来，进行非线性计算来确定输出）。大部分计算都可以使用矩阵乘法处理，从而让许多步骤并行完成。 


#### AI 芯片的设计流程


研发人员一直在不断改进现有的 DNN 模型和算法，使用越来越多样化的网络进行训练和测试，以实现更好的分类或更高的识别准确度，在某些情况下已经能够达到或超出人类的识别准确度。虽然神经网络的规模正在增加，变得更强大并且能提供更高的分类准确度，但是其需要的存储容量和计算成本正在呈指数增长。另外，这些具有大量网络参数的神经网络的大规模实现，并不适用于低功率应用，如智能手机、物联网设备、无人机、各种医疗设备等。 


很多硬件平台及其使用的神经网络对于「永远开启」、低时延处理而言不够节能。因此需要有创新的想法，对模型和算法作新的改进，协同设计芯片和算法，以使能耗达到最小，并在芯片的架构级和电路级都有新的实现方法。图 3.2 为 AI 芯片的设计流程。 


![img](https://pic4.zhimg.com/v2-e025e056ad612e26dcac567a2dd27502.webp)

（1）基本思路。AI 技术的基本思路大致有两种，一种是基于西方哲学的逻辑思考，使用符号规则来进行符号表征，起到推理的作用；另一种是基于生物中神经网络的连接现象，找出学习的规则。从人工智能的发展历史来看，一开始都是基于符号规则来进行推理，但后来发展到以连接为主导的神经网络架构和算法。近年来，有人提出了把这两种类型结合在一起进行推理的新想法。 


（2）数据集。对监督学习来说，训练数据就是「输入」和「标记」的组合，而「标记」代表了正确答案。数据的数量取决于所需的精度，也与网络架构和算法有关。无监督学习则只有训练数据的输入，没有「标记」。 


（3）神经网络架构/算法。根据准备好的数据集，确定合适的网络架构和算法，并确立一组训练参数，称为超参数（详见第 9 章）。可尽量采用近年来优化的方法，如二值网络、三值网络、降低数值精度（量化）、稀疏性（ReLU、网络剪枝）、可重用性、近似计算、专用 DNN 架构、张量分解（可分离卷积核）、循环穿孔等。 


（4）芯片架构。主要把网络架构和算法映射到硬件架构，包括乘积累加器/处理单元的优化、处理单元阵列及网络拓扑的优化、充分利用稀疏性及数据可重用性、高效存储器分层、权重固定（Weight Stationary）及行固定（Row Stationary）数据流等。 


（5）电路实现。如近阈值电路、异步电路、基于查找表（Look-Up-Table，LUT）的乘法运算、定制片上 SRAM 单元电路等。长远来看，用 NVM（非易失性存储器）来取代 SRAM 将可以大大提高能效。 


#### 计算引擎和存储系统


**计算引擎**


深度学习 AI 芯片是基于 DNN 实现的，而 DNN 里最主要的组成部分就是卷积层。 


卷积层里的计算包含对 3D 卷积核（权重）、输入特征图（激活）、输出特征图（激活）、这些特征图的数量［称为批量（batch）］等的计算，达到 7 个维度的计算空间  [7]  （见图 3.3）。在图 3.3 中，R 和 S 是一个卷积核的高和宽；C 是一个卷积核的通道数或输入特征图的通道数；X 和 Y、X′和 Y′分别是输入及输出特征图的宽和高；K 为卷积核数量或输出特征图的通道数；N 表示批量的大小，用作训练的神经网络的 N 很大，而用作推理的 N 相对较小。 


![img](https://pic3.zhimg.com/v2-9b6a937a5056138dc15168344eca2a61.webp)

卷积可以看作是矩阵乘法的扩展版，它增加了局部连接性和平移不变性（即图像经过了平移，图像样本的标记仍然保持不变）。与矩阵乘法相比，每个输入单元被替换成一个 2D 特征图，而每个权重单元被替换成一个卷积核，然后用滑动窗来计算。例如，从输入特征图的左上角开始，卷积核向右滑动，到了最右端后又移回最左端并下移一行。 


由于卷积层需要进行 7 个维度的计算，再加上为提供平移不变性而需要的卷积核数据重用等，卷积层的计算比一般矩阵乘法复杂很多。 


在 CPU 和 GPU 中，计算引擎通常是算术逻辑单元（Arithmetic and Logic Unit，ALU）；而在 FPGA 和 ASIC 中，计算引擎则是复杂的处理单元（PE），它可支持多种数据流模式。PE 通过片上网络（Network on Chip，NoC）互连，以实现所需的数据移动方案。 


卷积计算需要 PE 阵列来完成。这些 PE 是 DNN 的关键组件，也就是 MAC，如图 3.4 所示。图 3.4a 中，MAC 接收 3 个数据，即输入数据 X  i  、权重数据 W  i  j  和到此为止得到的部分和 Y  j  -  1，并输出新的部分和 Y  j  。MAC 中的乘法器大部分都是用数字电路实现的，但也有用模拟电路实现的（见图 3.4b、c）。 


AI 芯片常常以 TOPS 为单位来衡量其性能，它主要是可实现峰值吞吐量的度量，但不是实际吞吐量的度量。大多数操作是 MAC，因此 TOPS=MAC 单元数 × MAC 操作频率 ×2。为了充分利用性能，芯片需要一个能够让 MAC 在大多数时间保持忙碌（高 MAC 利用率）的存储架构，这是实现高实际吞吐量的关键。 


![img](https://pic3.zhimg.com/v2-372a3f3c4ff7d3bcb93492d89961f90d.webp)

简单地说，卷积其实就是对数据加权求和，这也正是 MAC 发挥作用的地方。CNN 能够执行一些不同类型的数据处理操作，但数量不相同，最常使用的操作是卷积。卷积层的计算工作量可能涉及深度嵌套的循环。卷积和反卷积基于算术乘法和加法。虽然 CNN 的概念很专业，但它们执行的基本操作中有 99% 以上是最基本的乘法和加法。表 3.1 为某种 AI 芯片的计算量实现结果（MOPS 指每秒百万次操作）。由表可见，该 AI 芯片执行的基本操作中，有 99.7% 是乘法和加法。 


![img](https://pic2.zhimg.com/v2-83f860a65fb53f26954d5fa1e67e1b2a.webp)

\*数据来源于特斯拉公司。 


在 GPU 实现中，MAC 操作被转换为矩阵与矩阵或张量与张量的乘积累加运算  [8]  。这允许同时计算批量样例，其中每层 MAC 在现代 GPU 内的许多 SIMD 处理器上并行操作。在宽高比大致相同的大矩阵相乘时，GPU 特别有效，因此可以选择批量的大小以便充分利用 GPU 的计算或存储器资源。批量选得越大，处理速度越快；选得越小，时延就越低。因此这需要折中考虑。 


此外，还可以从数学角度优化计算。研究人员提出应用 Strassen 算法来减小矩阵乘法的计算工作量  [9]  。该算法以增加矩阵加法为代价减少乘法次数，某些层可以降低高达 47% 的计算负荷。然而，与朴素矩阵矢量乘法相比，这需要更多的逻辑控制和存储器。在过去的半个世纪中，各种类似 Strassen 算法的快速矩阵乘法（Fast Matrix Multiply，FMM）算法使很多计算机科学家着迷，已经推动了许多理论上的改进。Strassen 算法不仅能在 ASIC 中实现，而且在 CPU 和 GPU 中都可得到有效实现。另外，为了加速矩阵乘法运算，还有人提出使用快速傅里叶变换及 Winograd 算法等。 


MAC 处理单元看起来比较简单，但是如何有效地把大量的处理单元连接、组织起来，就有很多种不同的创新方案了。而对于 MAC 处理单元本身，如何更好、更快地实现乘积累加，也有不少研究人员从纯数学角度进行了深入研究，提出了新的运算方法。 


CPU、GPU 及 DSP 都包含宽矢量 SIMD 寄存器文件及处理单元。SIMD 可以减少指令解码的开销，但是每次计算都得从寄存器文件中读出操作数并把结果写回去，又造成数据来回移动和存储的新开销，这成为 SIMD 的一个瓶颈。 


因此，有研究人员想到了使用在 20 世纪 80 年代比较热门的脉动式阵列，将复杂的脉动数据流精心组织到 MAC 处理单元之中  [7,10]  。在 AI 芯片中，脉动式阵列方法于 2015 年首次应用于谷歌的 TPU 中，目前已经广泛应用于基于深度学习的 AI 芯片中。在这种架构中，计算结果从一个 MAC 直接传输到另一个 MAC，没有寄存器的读写过程，并自动计算乘积累加。这样，每个 MAC 的功耗可以降低至原来的 1/10～1/5。而且这种架构很容易扩展，谷歌就使用了一个 256×256 的阵列，覆盖了 65,536 个 MAC。 


TPU 由以同步序列激励的 2D 脉动式 MAC 阵列组成，每个 MAC 执行操作 ab+c，其中 a、b 和 c 是存储在寄存器中的值。图 3.5 展示了由脉动式 MAC 阵列组成的 DNN 架构。在谷歌的 TPU 中，MAC 阵列通常被配置成：表示神经网络的一个网络层（或一个网络层的一块）的权重被加载到阵列中，并且矢量数据（图像等）按照时钟周期流过阵列。在计算矩阵-矩阵乘积时，需要将许多不同的数据矢量乘以相同的权重值。因此，将权重存储在 MAC 阵列中有很大的好处，不必从内存中重复加载它们。这被称为「权重固定」方案，并且通常被视为深度学习推理任务中的有效方法，因为存储器访问既缓慢又耗能——尤其是当存储器不含在该芯片内时。 


为了让开发人员更方便地进行开发，使矩阵大小与存储器更加匹配，现在一些主流的芯片架构都把许多 MAC 先划出分区（Tile）。例如，一个边缘侧推理芯片的 MAC 计算阵列常由 4～16 个分区组成，而云端用的 AI 芯片包含 64 个或更多的分区。 


![img](https://pic2.zhimg.com/v2-86421c2c3f4cf012853c20a7a81b7e34.webp)

**存储系统**


合理设计 DNN 中的存储系统也是一个关注重点，包括激活值存储器、权重存储器及所有可能的组合和层次结构。不同的网络具有截然不同的存储特征，其中一些是激活值约束的，一些是权重约束的。权重存储器或激活值存储器的需求可以变化 2～3 个数量级。同样，不同内存配置的峰值带宽需求可能相差几万倍。由于这种差异很大，想要让一种 AI 芯片在各种 DNN 基准测试中都得到良好的性能和能效，将会十分困难。 


AI 芯片的存储架构需要满足下列要求： 


（1）为神经网络模型提供存储权重的能力，用于存储执行神经网络模型的代码，并用于存储初始输入/图像和中间激活值； 


（2）以足够大的带宽向 MAC 提供权重和中间激活值，以跟上 MAC 的执行速度； 


（3）将中间激活输出写回到存储器，其带宽应足够大，不会在写入未完成时停止操作。 


存储架构分 3 个层次：MAC 中的寄存器文件，用于存储 MAC 间移动或累加的数据；全局缓冲器，用于存储足够的值以供 MAC 使用；片外内存，通常是 DRAM（见图 3.5）。 


片外 DRAM 通常保存当前层的所有网络权重和激活。这些数据的一部分会定期移到较低级别以靠近 MAC 处理单元。中间层即全局缓冲器，由 3 个 SRAM 缓冲区组成：一个用于输入激活，一个用于权重，一个用于输出激活。通常，激活缓冲区和权重缓冲区是分开的，因为可以使用几种不同的数值位宽。存储架构的最低级由位于 MAC 中的本地内存寄存器的单元组成，它们负责根据数据流策略所选择的数据重用，并组织与数据流相关的循环。此外，根据特定的设计和数据流，我们还可以插入其他存储器件。 


由于利用了存储层次结构，加速器与 CPU 之间没有直接通信。CPU 将数据加载到 DRAM 中，并对寄存器文件进行编程。寄存器文件的每个位置对应于 DNN 层的特定参数，即输入大小、输出大小、卷积核数量、卷积核大小。 


存储器的类型有 3 种选择，大多数芯片以不同的比例从中选取 2～3 种进行组合。 


（1）分布式本地 SRAM：这种方法是把 MAC 分成多个分区，然后用本地化 SRAM 来分配这些分区。这是英特尔等公司经常采用的一种方法。这种方法面积效率稍低，但保持 SRAM 接近处理单元会减少时延、降低功耗并增加带宽。 


（2）集中式单个 SRAM：这种方法有更高的面积效率，但跨芯片移动数据会增加功耗和时延并使单个 SRAM 成为性能瓶颈。 


（3）DRAM：选用 DRAM，每比特的成本要便宜得多，但存储容量可能比实际所需要存储权重和代码的量大得多；功耗明显高于 SRAM 存取，并且以大带宽访问 DRAM 的控制器成本非常高。 


理想的 AI 芯片能通过使用 1 片或 2 片 DRAM 和最少的片上 SRAM 实现高 MAC 利用率，从而在实现高吞吐量的同时降低成本和功耗。另外，一些新的深度学习 AI 芯片架构采用了近年来出现的新的 2.5D 和 3D 存储器作为存储单元，如混合存储立方体（Hybrid Memory Cube，HMC）和高带宽存储器（High Bandwidth Memory，HBM）等。 


以上介绍的存储架构是基于冯·诺依曼计算模型的架构，这也是至今绝大多数商用 AI 芯片所采用的架构。然而，随着新型 NVM 的日益成熟，基于以存储器为中心的存内计算的 AI 芯片已经崭露头角（见第 7 章）。对存内计算来说，存储器本身就可以进行计算（存算一体化），存储架构要简单得多，从而可大大降低时延和功耗。除了采用新型 NVM 来实现存内计算外，有人还在 HMC 的基础上加了扩展功能，如意大利博洛尼亚大学设计了智能存储立方体（Smart Memory Cube，SMC），相当于一个多核存内计算芯片。每个 SMC 由存储器加上 16 个群集组成，每个群集包含了 8 个协处理器和 4 个 RSIC-V 处理单元。 


DNN 的基本要求是训练和推理时间尽量短、模型参数尽量少、模型规模尽量小及 MAC 处理单元的数量尽可能少等，前提是保持相同的模型精度。这需要采用一些方法来压缩模型、降低数值位宽，以提高运算速度。 


作为深度学习加速器的 AI 芯片，其性能取决于算法、架构和电路这 3 个方面的优化。下面将介绍这几年来所积累的一些较典型的设计及优化方法。 


备案号:YX01jbkWgwB9w7Lle

