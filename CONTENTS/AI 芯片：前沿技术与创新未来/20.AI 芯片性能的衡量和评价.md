## 20.AI 芯片性能的衡量和评价
现在，不管在产业界还是在学术界，都已经涌现出使用各种各样架构和算法的 AI 芯片。如何衡量和评价这些芯片的性能，已经成为一个亟待解决的问题。 


如前所述，衡量一个 AI 芯片性能最基本的指标是芯片每秒的操作数（常用单位为 TOPS），表示完成任务的速度。这些芯片的性能也常常以 MAC/s 表示。MAC 算作两个运算：乘法和加法（尽管两者之间需要不同的时延和能量）。考虑到 MAC 由乘法和加法两个运算组成，OPS 与 MAC/s 之比为 2︰1。如果要在边缘侧执行图像识别的话，需要大约 10 TOPS 的处理能力。 


但是从这个指标还看不出能效，因此常使用单位功率下芯片的每秒操作数（常用单位为 TOPS/W）。例如，10  9  次运算/10 ms/0.1mW=1000 TOPS/W。图 3.19 中的对角红线展示了各种不同的 AI 芯片的能效，箭头所指为发展方向。当前，微控制器和嵌入式 GPU 的能效仅限于几十至几百 GOPS/W，而如果要实现边缘侧设备「永远在线」推理，AI 芯片的系统级能效需要远超过 10 TOPS/W。 


![img](https://pic3.zhimg.com/v2-bc93a6e55a847d558935f6a460f9488d.webp)

一些论文常常使用每秒操作数或单位功率下的每秒操作数来描述芯片的性能。然而，如果仅仅用这两个指标来描述，并不能全面反映一个 AI 芯片真正的性能优势或劣势。 


AI 芯片的性能衡量指标应该包含以下几个方面。 


（1）时延：这个指标对于某些应用来说特别重要（见图 2.9）。有的初创公司设计了所谓的「实时 AI 芯片」，就是针对这个指标作了特别的努力。时延直接与所使用的批量的大小有关。 


（2）功耗：不但包含了芯片中计算单元的功率消耗，也必须包括片上和片外存储器的功耗。 


（3）芯片成本/面积：这个指标在边缘侧应用特别重要。裸片的面积（包含存储器面积）对成本有直接影响，它取决于所用的工艺技术节点（如 5nm）及片上存储器的大小，也有的以 TOPS/mm  2  为单位表示。大多数 AI 芯片面积的很大一部分被高速缓存、控制电路等占据，而 10%～30% 被神经元和突触占据。 


（4）精度：识别或分类精度，体现了这个 AI 芯片的输出质量。要对困难的任务或在某些效用不足的数据集上实现高精度，通常需要更复杂的 DNN 模型。 


（5）吞吐量：表示单位时间能够有效处理的数据量，尤其在处理大量视频数据（25 f/s 或 30 f/s）时，大吞吐量可以保证画面的连续性。吞吐量除了用每秒操作数来定义外，也有的定义为每秒完成多少个完整的卷积，或者每秒完成多少个完整的推理。 


提高 MAC 的时钟频率及增加处理单元的数量，是提高吞吐量的有效方法。另外，可达到的吞吐量还取决于这些 MAC 的实际利用率。对于推理来说，吞吐量还与所用的 DNN 模型及输入数据有关。 


了解一个深度学习加速器的性能极限可以使用著名的屋顶线模型（Roofline Model），作为 DNN 模型和加速器设计特征的函数，把平均带宽需求和峰值计算能力与吞吐量联系在一起。屋顶线模型是 2009 年用于计算机多核架构的，现在已经有许多适用深度学习的扩展模型。 


（6）热管理：随着单位面积内的晶体管数量不断增加，芯片工作时的温度急剧升高，需要有「暗硅」设计（见第 1 章）和考虑周全的芯片热管理方案。为了达到足够的散热效果，有的 AI 芯片已经使用液体冷却方法。 


（7）可扩展性：如果可以通过扩展处理单元及存储器来提高计算性能的话，那么这种架构具有很好的可扩展性。但是并不是所有架构都具备这样的特性。可扩展性决定了是否可以用相同的设计方案部署在多个领域（如在云端和边缘侧），以及系统是否可以有效使用不同大小的 DNN 模型。 


（8）灵活性/适用性：有的 AI 芯片所用的架构和算法可以适用于很多不同的深度学习模型及任务，而有的适用性则非常局限。 


综合以上各项指标来看，要达到这些指标并在市场上保持竞争力的最关键条件是采用最先进的芯片制造工艺（当前是 5nm）。具有领先工艺节点的 AI 芯片对于 AI 算法经济高效和快速的训练和推理越来越重要。这是因为它们显示出能效和速度上的明显提高。因此，AI 开发人员和用户必须拥有最先进的 AI 芯片才能在 AI 研发和部署中保持竞争力。 


从 AI 芯片设计角度来看，目前还不可能在架构级、算法级、电路级，及各种工作负载时都能实现最佳的性能和能效。比较好的设计方法，是跨越这 3 个层级进行「跨层」设计，这样可以对各种参数和指标进行总体权衡。 


收集大量带标签的训练数据对深度学习工作至关重要。开源数据集在语音识别、自然语言翻译和计算机视觉方面已经非常丰富。计算机视觉中的常用数据集包括 MNIST（发布于 1990 年）、CIFAR-10（发布于 2009 年）、ImageNet（发布于 2009 年）、AlexNet（发布于 2012 年）等。现在有不少论文介绍的实验结果仍然使用这些数据集作为测试标准。然而，MNIST 已经有 30 年的历史，最初是给大学生编程序用的；CIFAR-10 用于图像识别，因为太简单，早已被 ImageNet 代替；AlexNet 曾经红极一时，但现在早该更新了，因为该网络一共只有 8 层。另外，在语音处理和计算机视觉领域之外，到目前为止还无法收集足够的带标签数据。这是深度学习技术无法应用到其他常见领域的主要障碍。 


对边缘侧 AI 来说，特别重要的是在各种边缘侧设备硬件上进行比较，包括简单设备，如树莓派（Raspberry Pi）、智能手机、可穿戴设备、家庭网关和边缘侧服务器等。当前的许多工作都集中在功能强大的服务器或智能手机上，但是随着深度学习和边缘侧计算的盛行，需要对异质硬件上的深度学习性能进行比较理解。由于缺乏对 AI 芯片特性方面一一对应的比较，因此难以选择正确的 DNN 模型，所比较的模型子集还是由研究人员自行决定的。此外，随着新的 DNN 模型的出现，单独的测量文件可能很快会过时。 


为了能够对各种 AI 芯片的性能作出系统、全面的评价，近年来麻省理工学院及英伟达的研究人员开发了专门的软件评价工具，如 Accelergy  [59]  、Timeloop  [60]  等。Accelergy 主要用于评估架构级的能耗，如基于处理单元的数量、存储器容量、片上连接网络的连接数量及长度等参数进行评估。而 Timeloop 是一个 DNN 的映射工具及性能仿真器，根据输入的架构描述，评估出这个 AI 芯片的运算执行情况。这样可以在不同架构之间进行公平的比较，并使 AI 芯片设计更加系统化。 


对各种深度学习工作负载进行基准测试（Benchmarking），并根据合理的指标评估其性能是进行公平比较的前提。近年来一个新的联盟——MLPerf 已经形成，它提供了内容广泛的基准套件，用于衡量深度学习软件框架、AI 芯片（硬件加速器）和云平台的性能，主要贡献者包括谷歌、英伟达、英特尔、AMD 和其他公司，以及哈佛大学、斯坦福大学和加利福尼亚大学伯克利分校等大学。MLPerf 的初始版本 v0.5 仅包含用于 AI 训练的基准，2019 年 11 月又推出 AI 推理基准。这些基准提供了各领域的工作负载的实施参考，这些领域包括视觉、语言、产品推荐，以及深度学习模型已证明成功且数据集公开可用的其他关键领域。 


目前，还没有成熟的通用评估系统来测试 AI 芯片性能，以便在不同层级上的不同方法之间进行比较。MLPerf 已经起到了领头作用，但还会遇到很多挑战。 


备案号:YX01jbkWgwB9w7Lle

