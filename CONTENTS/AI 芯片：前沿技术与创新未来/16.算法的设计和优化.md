## 16.算法的设计和优化
如前所述，深度学习的成功是依靠大量深度神经网络层取得的（主要是达到了很高的准确度）。这些最先进的神经网络使用非常深的模型（主要是 CNN），在训练期间会达到几十亿次计算，模型和数据的存储量达到几十兆字节到几百兆字节。例如，ResNet50 的运行有 77 亿次计算，权重和激活的存储量分别为 25.5 MB 和 10.1 MB，数据格式为 int8。这种复杂性给 AI 的广泛部署带来了巨大挑战，特别是在资源受限的边缘侧环境中。这就需要压缩模型中的大量搜索计算，最大限度地减少内存占用和计算，同时尽可能地保持模型精度。 


算法优化旨在通过采用数值的位宽缩减、模型压缩和增加稀疏性来降低计算成本。这方面的进展可以分为两个方面，其中一方面以二值网络为代表，以蒙特利尔大学的约书亚·本吉奥（Yoshua Bengio）教授为核心。2015～2017 年间，人们对二值（1 位）网络和三值（2 位）网络的研究取得了明显的进展。另一方面是研究如何把基于服务器的高精度模型引入移动和可穿戴设备。例如在 ImageNet 中，目标误差率的提高被抑制到小于 1%，功耗降至 100mW 或 10mW 甚至更低，以实现「永远开启」的操作。 


现在研究人员已经提出了很多方法来降低 DNN 的能耗。大多数工作要么是设计更有效的算法，要么是设计优化的硬件。专用硬件平台针对典型数据流进行了优化，并利用了网络稀疏性及大多数神经网络的固有错误恢复能力。人们尽可能在硬件实现中利用 DNN 的高度并行性，使用群集训练和被训练了的修剪来缩减模型规模，并提出针对其压缩方案进行优化的硬件加速器  [11]  。其他最近的硬件实现提出了利用稀疏性的解决方案，要么在稀疏操作期间加速，要么提高能效。一些工作通过使用降低数值精度的算子来利用 DNN 对噪声和变化的原有容错度进行扩展，这降低了乘积累加之类算术运算的功耗并压缩了模型的内存占用，代价是可能损失输出精度。 


设计高效的深度学习算法可以有很多种方法，这些方法有的已经作为成熟的产品被实现并得到了大量应用，有的还停留在实验室样片阶段。虽然这些方法在理论上减小了操作规模、减少了操作数量、降低了存储成本，但是通常需要专门的硬件来将这些理论上的好处转化为能效和处理速度方面可实测的改进。下面介绍一些这类方法。 


#### 降低数值精度的量化技术


用于存储和计算模型权重参数的数值精度（位宽）会影响网络训练和推理的性能和效率。通常，较高的数值精度表示（尤其是浮点表示）用于训练，而较低的数值精度表示（包括整数表示）可用于推理。一些芯片实现已经可以有效地将数值位宽从 16 位降至 4～8 位，甚至 1 位，且可使用整数值，这同时节省了能耗并提高了处理速度。 


事实上，深度学习的一个特征是它不需要很高的计算精度。许多软件使用以高精度表示的数字（如 32～64 位），而在深度学习中，8～16 位计算就足够了。位宽越小，参与运算的电路越少，就可以在同一芯片区域上集成更多电路以提高性能，或减小芯片面积以降低功耗和成本。 


降低 CNN 关键数据（即权重和激活值）的位宽，可以显著降低存储要求和计算复杂性，因此受到越来越多的关注。权重量化技术  [12,13]  就是降低权重数值的位宽。还有研究人员提出将权重量化方案直接扩展到激活值量化，但是它在如 ImageNet 这样的大规模图像分类任务中引起了明显的精度降级  [14]  ，即最终牺牲了模型精度。 


AI 在训练期间需要保持极高的输出精度。一般来说，对于一个 AI 推荐系统，需要有误差仅为 0.02% 的输出精度；而对于一个计算机视觉模型来说，也需要达到误差仅 0.5% 的精度。为了避免降低数值位宽之后出现 AI 系统输出精度损失，近年来已经有很多研究人员（包括 IBM 和高通的团队）对此作了大量研究，提出了许多改进方案  [15,16]  。也有些研究人员提出使用「多种精度混合」的自适应方法，根据不同的需求对位宽进行灵活变换，这样既满足了减少计算量的要求，也能保持训练精度不变。 


当 2012 年深度学习刚刚兴起的时候，大多数 AI 训练使用 32 位浮点（fp32）对训练过程中的权重进行更新，后来采用了 16 位浮点（fp16），把计算效率提高了一倍。谷歌发明的 16 位脑浮点（Brain Floating Point，bf16 或 bfloat16）格式与 fp16 相比增加了动态范围，与标准 fp32 相同。大多数情况下，用户在进行神经网络计算时，bf16 格式与 fp32 一样准确，但是能以一半的位宽完成任务。因此，与 fp32 相比，采用 bf16 可以使吞吐量翻倍、内存需求减半。 


fp32 格式包括 1 个代表符号（+ 或-）的符号位，其后依次为 8 个指数位及 23 个尾数位，共 32 位数字；fp16 包括 1 个符号位、5 个指数位和 10 个尾数位，共 16 位数字；而 bf16 格式将 fp32 数字的尾数缩减到 7 位，以稍降低精度。因此，bf16 格式依次包括 1 个符号位、8 个指数位和 7 个尾数位，共 16 位数字。 


具有 M 个尾数位的浮点格式的乘法器电路，是一个（M+1）×（M+1）全加器阵列。这需要将两个输入数字的尾数部分相乘，从而构成了乘法器中的主要面积和功率成本（可以认为乘法器的实体尺寸会随着尾数长度的二次方而增加）。fp32、fp16 和 bf16 格式分别需要 576 个、121 个和 64 个全加器。因为 bf16 格式的乘法器需要的电路少得多，所以有可能在相同的芯片面积和功率预算中放置更多的乘法器，从而意味着采用这种格式的深度学习加速器可以具有更高的性能和更低的功耗和成本。 


自 2015 年以来，bf16 格式一直是第二代和第三代 TPU 的主力浮点格式。目前，英特尔已把 bf16 整合到它的一些 AI 处理器中，ARM 及一些初创公司的产品也都采用了这种数据格式。 


由于数据格式对数字电路实现的效率有直接影响，产业界和学术界都有人在研究新的格式。现已被英特尔收购的 Nervana 曾使用过一种新的数字格式——Flexpoint，它允许将神经网络推理的标量计算用整数乘法和加法来实现，从而在提高功效的同时实现并行性的更大提升。然而，目前这个格式至少在其产品中已经销声匿迹。另一种方法是学术界正在研究的，即使用 Posit Number 来替代标准浮点格式。Posit Number 的主要优点是能够从给定的位宽中获得更高的精度或动态范围。这个格式由约翰·古斯塔夫森（John L. Gustafson）提出，具有突破性意义，现在已经用到神经网络的训练和推理场景  [17]  。 


然而，还有一些 AI 推理采用了整数格式。对于同样的位宽来说，整数格式可以节省 50% 的功耗和面积。研究表明，在同样的输出精度下，8 位整数的能效可以达到 32 位浮点的 4 倍。 


表 3.2 为不同数值精度对应的能耗（基于台积电 45nm 工艺的 ASIC）  [18]  。 


![img](https://pic2.zhimg.com/v2-03592843e63e46457ccc173aad25eebf.webp)

将 32 位全精度（fp32）模型量化为 8 位整数（int8）会在权重和激活值上引入量化噪声，这通常会导致模型性能降低。这种性能下降可能非常微小，但也可能导致灾难性后果。为了最小化量化噪声以减轻其影响，研究人员已经公布了各种不同的方法。这些方法通常依赖量化感知的微调或从头开始的训练  [19,20]  。实际量化方法的主要缺点是它们依赖数据和微调，最好将 fp32 模型直接转换为 int8，而不需要运行传统量化方法所需的专有技术。 


高通研究人员介绍了一种量化方法  [16]  ，称为无数据量化（Data-Free Quantization，DFQ）技术。这种量化技术不需要数据、微调或超参数调整，从而改进了量化性能，提高了系统精度，在将 fp32 模型量化为 int8 时仍能实现接近原始的模型性能。这是通过调整预训练模型的权重张量使得它们更易于量化，并通过校正在量化模型时引入的误差的偏差来实现的。 


比利时研究人员还提出了最小能量量化神经网络（Quantized Neural Network，QNN）  [21]  ，通过量化训练提供对任意数量的位宽和任何网络拓扑的网络量化的明确控制，并将其链接到推理能量模型。这可以既优化所使用的算法，也优化硬件架构，适用于「永远在线」「永远开启」的嵌入式 AI 应用。 


具体地说，这是一种二值神经网络（+1，-1）的广义化。研究人员将二值网络训练从 1 位扩展到 Q 位（intQ），其中 Q 可以是任意值。然后通过将网络复杂性和大小与完整的系统能量模型联系起来，评估 QNN 推理的能量和精度，以计算精度权衡。最后得出结论，能耗取决于所需的精度、计算精度和可用的片上存储器。int4 实现通常被认为是最小能量解决方案。 


这种方法通过引入 QNN 及用于网络拓扑选择的硬件能量模型来使嵌入式神经网络的能耗最小化。为此，二值训练设置可以为从 1～Q 位的广义化。该方法允许找到最小能量并导出若干趋势。首先，能耗根据使用位宽有数量级的变化。对于所有基准测试，相等精度的最小能量点在 1～4 位之间变化，具体取决于可用的片上存储器和所需的精度。通常，int4 网络的性能优于 int8 最多可达 6 倍。这表明，在既支持低功耗始终开启，也支持高性能计算的应用中，传统的 fp32、fp16、int8 应该使用 int4 进行扩展，以实现最小能耗的推理。图 3.6 为基于 QNN 的 AI 芯片框图。 


![img](https://pic3.zhimg.com/v2-3f768a6ba37c672deca439c4e5cbe8b9.webp)

上述 QNN 可以以低数值精度来训练，最低可以低至 1 位（单比特）。这样，可以把模型扩展至更多的权重和更多的操作，达到很高的能效。 


除了上述量化方法外，近年来也在不断出现其他方法，如随机舍入、混合精度、缩短数据流及新的量化格式等。 


很显然，不管对于神经网络的训练还是推理，1 位的数值精度可能是今后的发展目标（见图 3.7）。图 3.7 中的两条线分别表示训练和推理的系统精度不变时，功耗的下降趋势，有人把它称为一种「新摩尔定律」  [22]  。当前，用 ASIC 方式实现 AI 芯片成为热点时，已经可以把精度降到 4 位（推理）及以下，功耗也可降低至原来的 1/10 以内。最新的例子是初创公司 LeapMind 于 2020 年 4 月发布的边缘侧 CNN 加速器，权重和激活的乘法分别使用了 1 位和 2 位，从而大大提高了能效，达到了 27.7 TOPS/W，比其他类似加速器芯片的能效［如 ARM 的神经处理单元（Neural Processing Unit，NPU）核的最大单个能效为 5 TOPS/W，谷歌用于边缘推理的 Edge TPU 加速器为 2 TOPS/W］高得多。 


而接下来几年中，一旦存内计算技术得到大量应用，可以进一步把功耗降低两个数量级。 


![img](https://pic2.zhimg.com/v2-6dc480284ec73644cb233258b148051f.webp)

降低精度确实带来了很大的好处，它可以降低时延、节省存储空间、减少存储器带宽，以及降低存储器和处理单元的功耗。但是，降低精度的方法都要保证系统网络一定的输出精度。换句话说，这是一种与输出质量的折中，总要牺牲一些输出精度。对于图像识别、语音识别等应用，稍微有点误差，并不影响整体运算的质量；但是对于某些对精度要求极高的应用，如自动驾驶汽车的 AI 芯片，往往仍旧采用 32 位整数作加法及 16 位整数作乘法。 


一些商用芯片如英伟达的 Pascal（2016 年推出），谷歌的 TPUv1（2016 年推出）、TPUv2 及 TPUv3（2019 年推出），英特尔的 NNP-L（2019 年推出）等也都采用了量化技术：8 位整数用于推理，16 位浮点用于训练。一般来说，吞吐量与芯片面积成正比，而与数值精度的位数成反比。因此，云端 AI 芯片和边缘侧 AI 芯片都将会继续遵循量化「新摩尔定律」往前发展。 


#### 压缩网络规模、「修剪」网络


在 DNN 计算中，有许多值是 0、接近 0 的值或重复的值。将这些值输入一个 MAC 进行乘积累加运算没有意义，且浪费资源，这就需要用到压缩。有些芯片设计了某种功能，阻止 MAC 作这类没有意义的运算，因此可以节省能源。 


类似的方法是「修剪」网络，在训练的最后阶段消除被认为不重要的神经元（称为剪枝，见图 3.8）。如果可以使用稀疏矩阵技术有效地存储矩阵并将其传递到加速器，则甚至可以去除不重要的单个权重。或者，可以将不能移除的不重要的权重设置为零，并且简单地指示电路跳过这些权重，从而消除不必要的计算。压缩技术可以减小片上带宽，但与解压缩相关的计算会有所增加。所有这些方法都有助于减少必须带到芯片上加载至 MAC 的数据量。 


![img](https://pic3.zhimg.com/v2-75d9135cc8c46cd234222dc39cdd0024.webp)

修剪的类型包括：修剪层数（这通常带来最大的好处）、修剪连接数、修剪神经元的数量、修剪权重数量。 


在数据量化之后往往还采用霍夫曼编码（Huffman Coding），以进一步降低网络存储量。霍夫曼编码通常用于无损数据压缩，最早由范·鲁文（Van Leeuwen）在 1976 年提出。它使用可变长度码字来对源符号进行编码，从每个符号的发生概率中推导出一个表，常见的符号（如在神经网络中反复使用的相同的权重）用更少的比特表示。实验已经表明，对非均匀分布值进行霍夫曼编码可以节省 20% 的网络存储容量。经过算法和架构的优化，就可以根据这个优化的算法和架构设计芯片，最好在设计时把算法、网络架构与芯片的硬件架构一起考虑，形成协同设计（Co-Design），并采用比较成熟的电路来实现。 


修剪神经网络的方法最早由 Yann LeCun 在 1990 年发表的文章里提出。2016 年，斯坦福大学的博士生韩松、清华大学毛慧子等人进行了一项关于深度压缩和修剪的简单研究，训练量化和霍夫曼编码，并给出了一些令人印象深刻的研究结论，说明如果调用适当的修剪和压缩方法，可能显著缩小神经网络，论文名为《深度压缩：用修剪压缩深度神经网络》  [23]  。 


深度压缩可以大大减小存储器带宽，用片上 SRAM 就可以容纳所需的存储容量，并可把一个 1 GB 规模的网络缩减到只有 20～30 MB，从而可用于移动应用（移动应用 ＜100 MB）。 


除了使用霍夫曼编码来对神经网络进行压缩外，还可以使用更多的理论方法来解决这个问题，如使用 Vapnik Chervonenkis 维数  [24]  和 Kolmogorov 复杂度  [25]  之类的思想来分析神经网络，并定义出神经网络压缩的极限值。 


#### 二值和三值神经网络


把乘法精度降低，或通过丢弃连接而大大减少乘法量，可以大大降低计算成本  [26]  。有些研究论文还介绍了二值神经网络（Binary Neural Network，BNN）和三值神经网络（Ternary Neural Network，TNN）。通常，通过实数值激活的实数值权重的相乘（在前向传播中）和梯度计算（在后向传播中）是 DNN 的主要操作。BNN 是通过将前向传播中使用的权重二值化来消除乘法运算的技术，即仅约束为两个值（0 和 1，或-1 和 1）。结果，乘法运算可以通过简单的加法（和减法）来执行，这使得训练过程更快。有两种方法可以将实数值转换为相应的二值：确定方法和随机方法。确定方法是直接把阈值技术应用于权重，它可以用下式表示： 


![img](https://pic3.zhimg.com/v2-9908c2ff7dbd265726a3595d9b91a468.webp)

而随机方法是基于使用硬 S 形函数的概率将矩阵转换为二值网络，因为它在计算上是简便的。实验结果表明它具有良好的分类准确度。BNN 有以下几个优点： 


（1）GPU 上运行二值乘法比 CPU 上运行传统矩阵乘法快 7 倍； 


（2）在前向传播中，BNN 大大减少了存储量和访问量，并且通过逐位操作取代了大多数算术运算，从而大大提高了能效； 


（3）二值处理单元用于 CNN 时，可降低约 60% 的硬件复杂性； 


（4）与算术运算相比，存储器访问通常消耗更多能量，并且存储器访问成本随着存储器容量的增加而提高，而 BNN 在两个方面都有改进。 


过去几年中也有其他技术被提出  [27-30]  。最值得注意的是，如果将网络权重或权重和激活值两者都限制为 +1 和-1，从硬件角度来看，这尤其令人感兴趣，因为这种二值网络拓扑结构允许用节能的同或门（XNOR）操作替换所有昂贵的乘法运算。在基于 XNOR 的 DNN 实现中，卷积核和卷积层的输入都是二值的，这使得卷积运算速度提高了 58 倍，存储量降至原来的 1/32。这样在 CPU 上就可以实现最先进 DNN 的实时使用，而无须用 GPU。二值神经网络在 ImageNet 数据集上进行了测试，与全精度 AlexNet 相比，分类精度仅降低了 2.9%，且功耗更小、计算时间更短。这使得专门的硬件实现加速 DNN 的训练过程成为可能  [31,32]  。研究人员最近成立了一家公司 XNOR.ai（已于 2020 年 1 月被苹果收购）来进一步探索这种算法和处理工具，旨在于边缘侧装置中部署 AI。 


也有研究人员提出了三值网络  [33]  。这种方法可以将神经网络中的权重精度降低到三值（二位权重）。这种方法几乎没有降低精度，甚至可以提高 CIFAR-10 和 ImageNet 上 AlexNet 某些模型的准确性。这种三值网络也可以被视为稀疏型二值权重网络，可以通过定制电路加速。 


#### 可变精度和迁移精度


对于一个神经网络来说，所需的数值精度可能因应用的不同阶段而异，不是所有的应用都需要同样的精度。例如，有的通过使用较少的位宽在中间层实现 DNN 的最佳性能与输出精度之间的权衡，有的使用强化学习方法来发现一个具有每层不同量化的有效量化神经网络。 


而每一层里面的数值精度要求也都会不同，矩阵相乘可能是 8 位和 8 位相乘，也可能是 2 位与 4 位相乘。同样，权重与激活值也可能具有不同的数值精度。因此，较好的解决方案是把芯片做成「可变精度」。 


一般来说，硬件都是固定精度的结构，而软件的好处是可以做到可变精度。在芯片工作时，数值精度如果没有得到很好的匹配，会造成芯片面积浪费、性能下降及功耗提高。 


虽然硬件是固化了的结构，但还是有办法来做到「可变」。图 3.9 展示了各种用硬件实现可变精度的方法：全覆盖架构是把各种精度的电路全部做在芯片里，通过选择器来选择所需精度的电路，这种架构利用率很低；动态重构架构可以动态地把电路改成所需精度的电路；第三种是「位串行（Bit-Serial）」架构。 


![img](https://pic3.zhimg.com/v2-c897e8d51f741041af3e674109fa1654.webp)

近年来，位串行架构受到了研究人员的重视，并已在一些先进的 AI 芯片（如 QUEST，见第 4 章）中得到应用。1951 年之前的几乎所有数字计算机，以及大多数早期的大规模并行处理机都使用位串行架构。人们在 20 世纪 60～80 年代开发了用于数字信号处理的位串行架构。位串行的好处是一次只处理一位数字，数值精度可以按需求实时调节。通常，N 个串行处理器将比单个 N 位并行处理器占用更小的面积，并具有更高的总体性能。 


在使用位串行计算时，整数矩阵乘法表示为二值矩阵乘法的加权和  [34]  。位串行方案提供了使用一个有效的二值矩阵乘法加速器来计算任何精度矩阵乘法的可能性。研究人员提出了一种由软件可编程加权二值矩阵乘法引擎和相关硬件组成的方案，用于获取数据和存储结果。 


这个方案基于 FPGA 实现，硬件架构可配置，并带有成本模型，用于估计给定参数集的资源使用情况。它的软件可编程性使其能够以任何矩阵大小和任何定点（整数）精度运行。该方案还引入了一种新的并行到串行（Parallel-to-Serial，P2S）加速器，采用传统的位并行矩阵并产生等效的位串行矩阵。 


如前所述，如果系统从头到尾以同样的精度运行，将是非常浪费资源的。迁移精度方法  [34]  可以根据需求对各种不同的精度进行自适应，即按需变化精度，从而降低功耗或提高性能。它从大自然中获得灵感，来定义计算架构。这些架构在处于较宽且平滑的范围内的精度与成本之间进行权衡，如图 3.10 所示。 


近似计算已经在 AI 芯片的设计中得到应用（见第 8 章）。但是，传统近似计算的主要障碍是缺乏从应用到硬件来管理精度而不影响应用质量的框架。更准确地说，缺乏输出精度保证和严格的误差控制是主要的问题。使用迁移精度的计算框架中，可通过细粒度硬件对精度进行分布式控制，使用可扩展的、基于反馈的运行方式，并以一种可在线跟踪误差的编程模型来调整操作参数。这在满足应用级不同质量要求的同时，大大降低了功耗。 


![img](https://pic2.zhimg.com/v2-93a70ce3bcefbb2270d432a85418d967.webp)

按照自适应迁移精度的思路，也有研究人员提出类似的精度混合方法，即根据运行需求，混合使用二值、三值、16 位浮点、8 位整数等各种精度。 


然而，不管是可变精度、迁移精度还是精度混合的方法，硬件实现都需要增加不少额外电路来支持，这就需要在由其得到的好处与硬件成本之间作出权衡。 


#### 简化卷积层


对卷积层使用低维卷积核，可以减少网络结构的内部操作和参数  [36,37]  。这种方法有很多好处：首先，改进的卷积操作使得运行过程更加清晰；其次，这种方法大大减少了计算参数的数量。例如，如果一个层具有 5×5 卷积核，可以用两个 3×3 卷积核替换（中间没有池化层）以便更好地进行特征学习；3 个 3×3 卷积核可用作 7×7 卷积核的替代等。使用低维卷积核的好处是假设当前的卷积层都具有 C 个通道，对于 3×3 卷积核的 3 个层，参数的总数等于权重，即 3×（3×3×C×C）= 27C  2  个权重；而在卷积核的尺寸为 7×7 时，参数总数为（7×7×C×C）= 49C  2  ，与 3 个 3×3 卷积核参数相比几乎多一倍。 


#### 增加和利用网络稀疏性


增加和利用网络稀疏性是一种重要设计方法。权重和激活矩阵中存在着大量的零，非零元素分散分布。任何数值与零相乘还是等于零，因此需要避免执行这类不必要的 MAC 运算。 


稀疏性包括权重的稀疏性和激活值的稀疏性。要获得激活值的稀疏性，可以使用激活函数 ReLU 或者最大池化（用于反向传播）；而对于权重的稀疏性，则有各种各样的选择，可以省略整行、整列、卷积核、通道或内核等来得到，如图 3.11 所示。粒度越粗，稀疏性的结构越明显，也就越容易硬件实现。对于权重和激活矩阵来说，非零值的数量可以分别减少到 20%～80% 和 50%～70%。 


稀疏性如果得到很好的利用，将会大大提高性能。不过，增加稀疏性会牺牲一定的输出精度，需要掌握平衡。另外，在硬件实现时，常常需要额外的逻辑电路来找出非零值并进行其他处理，这将会增加硬件成本。 


![img](https://pic3.zhimg.com/v2-81865a81415880d6930d3e6e719dbd8a.webp)

事实上，现代数据中心的工作负载很大且非常稀疏，其中大多数内容为零。因此，以大型稀疏矩阵为目标的矩阵乘法算法的关注度在不断提高。这种乘法称为稀疏矩阵-矩阵乘法（Sparse Matrix Multiplication，SpMM），是目前流行的多种算法的重要组成部分。朴东贤（Dong-Hyeon Park）等人专门为此设计了一种算法和架构  [39]  ，把矩阵-矩阵乘法最常见的内积实现方法，改成了外积算法来最大限度地减少冗余内存访问。这种 SpMM 加速器芯片由 48 个异质核组成，并与交叉开关和可重构内存紧密耦合在一起。 


备案号:YX01jbkWgwB9w7Lle

