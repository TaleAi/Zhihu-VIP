## 18.电路的设计和优化
电路设计是 AI 芯片最后实现的关键阶段。如果把 AI 芯片设计分成架构级、算法级和电路级的话，电路级是最后测量这块芯片的关键性能指标（如 TOPS/W）的层级。目前，大部分 AI 芯片都基于数字电路，然后用一些创新思路来实现。例如，使用 XNOR 来实现 1 位数字 MAC、多精度（2～16 位）数字 MAC 等。另外，很多用于普通 ASIC 或 SoC 的低功耗设计技术（如电源门控、时钟门控等）在 AI 芯片设计中也得到了广泛应用。 


#### 用模数混合电路设计的 MAC


考虑边缘侧应用中苛刻的能效要求，如何进一步降低 AI 芯片的功耗，一直是研究人员探讨的问题。为此，研究人员提出不仅要从网络架构级设法提高能效，而且要从电路级改变思路。2019 年，斯坦福大学的丹尼尔·班克曼（Daniel Bankman）提出一种使用模拟电路的模数混合信号二值 CNN 处理器  [49]  ，就是针对用于「永远在线」的 AI 推理的应用所开发的。这种处理器每次分类任务只消耗 3.8μJ 的能量。 


该设计针对中等复杂度的推理任务，如 CIFAR-10 图像分类数据集。一般来说，能量和可编程性之间，以及能量和分类精度之间往往不容易得到很好的权衡。而这项工作的设计目标是尽量降低能耗，允许牺牲一些可编程性和分类精度。尽管如此，这种设计仍然是半可编程的，并与其他最近的硬件实现相比具有更高的精度。这种「模拟 MAC」实现了每次分类的最低能耗，与数字 MAC 相比功耗要小得多，但缺点是芯片面积要比数字 MAC 大。 


这个模拟 MAC 使用了二值神经网络，权重和激活值都约束为 +1 和-1，把乘法简化为 XNOR，并允许在片上集成所有存储器。使用具有输入重用功能的权重固定、数据并行架构，可以在许多计算中分摊内存访问的能量成本。剩下的能量瓶颈是矢量求和，该设计采用高能效的开关电容（Switched Capacitor，SC）神经元来应对这一挑战，采用 1024 位温度计编码（即一元编码）的电容式数模转换器（Capacitive Digital-to-Analog Converter，CDAC）对 CNN 卷积核权重和激活值的逐个点积求和，并采用一个 9 位二值加权部分，用于添加卷积核偏置。SC 神经元通过重新分布充电电荷来执行矢量求和，通过一个比较决策电路产生 1 位神经元输出  [49]  ，如图 3.16 所示。该设计是占用 6 mm  2  的 28nm CMOS，包含 328 KB 片上 SRAM，工作速率为每秒 237 帧（237 f/s），工作电压为 0.6 V/0.8 V，功率为 0.9mW。相应的每个分类任务所需能量（3.8μJ）约为之前 CIFAR-10 低能耗基准的 1/40，部分原因是牺牲了一些可编程性。SC 神经元阵列的能效比一个自动综合的数字神经元阵列实现高 12.9 倍，整体系统能效提高了 4 倍。 


![img](https://pic3.zhimg.com/v2-3201311a5fa8165f79a9fdcccac33521.webp)

#### FPGA 及其 Overlay 技术


硬件加速需要很仔细地探索如何达到高并行性。并行性水平可以分为粗粒度、中粒度、细粒度等。FPGA 在细粒度和粗粒度重构能力方面表现优异，其分级存储结构和调度机制可以灵活优化。灵活的分层存储架构可以支持 DNN 的复杂数据访问模式，它通常用于提高片上存储器的效率并降低能耗。 


神经网络硬件架构和数值精度（位宽）必须在细粒度和粗粒度水平上可以重新配置，以实现更多功能和更高的能效。如果要做到精度随应用不同阶段的需要而不断变化，最好的选择还是 FPGA。FPGA 的可编程、可重构、自适应、可定制等高度灵活的特点，非常适合目前的 AI 算法发展阶段，因为 AI 算法现在还一直在改进和进化中，这些改进思路包括多种精度组合、多种网络拓扑组合、多个神经网络组合、多种应用组合等，以及正在受到研究人员关注的 FPGA Overlay 技术（见图 3.17），这是一种软件定义硬件技术。 


Overlay 是在 FPGA 硬件层之上加设的一层虚拟层，连接到顶层应用。这种虚拟可编程架构，又称为 FPGA 虚拟化。顶层用户不必太多关心 FPGA 的硬件逻辑结构细节，就可以在 FPGA 的不同运行时间，对资源做出最佳的调度和利用。加载程序到 Overlay，只需微秒级的时间。这成为软件定义硬件（Software Defined Hardware，SDH）的一种方式，正成为工业界和学术界的研究热点（硬件重构时间的目标是 300～1000 ns）。EDA 厂商也在进行新的创新，研究如何让 FPGA 开发者轻松地利用 Overlay 进行软件编程。 


![img](https://pic4.zhimg.com/v2-3aa8456b792e14819123a2927db07027.webp)

另外，作为机器学习的一个子领域，在国际机器学习大会（International Conference on Machine Learning，ICML）上每年讨论的自动机器学习（AutoML），是指自动生成定制的神经网络，这样的机制也只有在 FPGA 上才可以得到灵活的实现和硬件验证。 


与 GPU 相比，目前普通的 FPGA 吞吐量为每秒几十 GFLOPS，并且内存访问受限。另外，它本身不支持浮点数，但比 GPU 更有效（但如果与 ASIC 比较能效，FPGA 只能算是一种「低效率」的 ASIC）。由于其有限的存储器访问，许多方法集中于加速神经网络的推理时间，因为与训练过程相比，推理过程需要较少的存储器访问。 


大型神经网络加速需要对外部存储器进行优化。不同的模型需要不同的硬件优化，即使对于相同的模型，不同设计加速性能也会有相当大的差别。FPGA 可以重新配置，更容易开发硬件、框架和软件。特别是对于各种神经网络模型，其灵活性缩短了设计周期，降低了成本。 


埃里科·努维塔迪（Eriko Nurvitadhi）等最近在新一代 GPU（NVIDIA Titan X Pascal）和 FPGA（Intel Arria 10 GX 1150 和 Intel Stratix 10 2800）上评估了新的 DNN 算法  [50]  。实验结果表明，DNN 的当前趋势有利于 FPGA 平台，因为它们提供更高的能效。与基于 GPU 的加速器相比，FPGA 和 ASIC 硬件加速器具有相对有限的内存、I/O 带宽和计算资源，但它们可以以更低的功耗实现至少中等水平的性能。 


通过定制存储器层次结构和分配专用资源，我们可以提高 ASIC 设计的吞吐量。然而，在开发周期、成本和灵活性方面，基于 ASIC 的深度学习 AI 芯片不太令人满意。作为替代方案，目前使用基于 FPGA 的加速器以合理的价格提供高吞吐量，具有低功耗和可重构性。FPGA 厂商可使用 C 或 C++ 高级综合工具，这降低了编程难度，缩短了基于 FPGA 的硬件加速器的开发时间。 


因此，多家大公司正在积极研发新的 FPGA。赛灵思是 FPGA 的发明者和最大的 FPGA 生产商，针对 AI 应用，赛灵思在 2018 年 3 月推出了新的自适应计算加速平台（Adaptive Compute Acceleration Platform，ACAP）可重构芯片系列。其中 Versal「AI Core」系列包含自适应引擎、标量引擎及智能引擎 3 大部分，使用台积电 7nm 工艺，速度据称比最快的 FPGA 快 20 倍，比服务器级别的 CPU 快 100 倍以上，而时延低于 2 ms。ACAP 可以视为赛灵思的可重构 AI 芯片。 


英特尔则把该公司的 FPGA 与芯粒（Chiplet）技术相结合，封装在一块芯片内，实现了高性能、高灵活性的 AI 引擎。 


这类新器件及 Overlay 之类技术的成功开发，将有助于实现 SDH 的目标，即以极低的成本开发和运行数据密集型的 AI 算法，用非常快速的实时硬件重新配置速度和动态编译，同时在能效上也高于 ASIC，从而能够广泛适用于大数据解决方案。 


备案号:YX01jbkWgwB9w7Lle

